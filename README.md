# CVPR-2022-Papers
![5533b620402406dba74eb9a452e32d4](https://user-images.githubusercontent.com/62801906/150053890-e667997b-62c8-40a8-b561-ccc99ecd89f6.png)

官网链接：https://cvpr2022.thecvf.com/

开会时间：2022年6月19日-6月24日

### ❣❣❣近日，[CVPR 2022 接收论文公布！ 总计2067篇！](https://mp.weixin.qq.com/s/WfzbGK34z3gIk1E9su8moA)，全部论文已发布，本文档也将持续收录分类ing，多多关注!!

### ❣❣❣另外打包下载所有论文，可在[【我爱计算机视觉】微信公众号](https://user-images.githubusercontent.com/62801906/163739684-175f0b8a-871e-4a41-b310-b549625fdcb1.png)后台回复“paper”。

## 历年综述论文分类汇总戳这里↘️[CV-Surveys](https://github.com/52CV/CV-Surveys)施工中~~~~~~~~~~

## 2022 年论文分类汇总戳这里
↘️[CVPR-2022-Papers](https://github.com/52CV/CVPR-2022-Papers)
↘️[WACV-2022-Papers](https://github.com/52CV/WACV-2022-Papers)

## 2021年论文分类汇总戳这里
↘️[ICCV-2021-Papers](https://github.com/52CV/ICCV-2021-Papers)
↘️[CVPR-2021-Papers](https://github.com/52CV/CVPR-2021-Papers)

## 2020 年论文分类汇总戳这里
↘️[CVPR-2020-Papers](https://github.com/52CV/CVPR-2020-Papers)
↘️[ECCV-2020-Papers](https://github.com/52CV/ECCV-2020-Papers)

### ❗❗❗ 7月 1 日 61 篇
* 文本修复
  * [Fourier Document Restoration for Robust Document Dewarping and Recognition](https://arxiv.org/abs/2203.09910)<br>:house:[project](https://sg-vilab.github.io/event/warpdoc/)
* 对比学习
  * [Consistent Explanations by Contrastive Learning](https://arxiv.org/abs/2110.00527)<br>:star:[code](https://github.com/UCDvision/CGC)
* 视频字幕
  * [Hierarchical Modular Network for Video Captioning](https://arxiv.org/abs/2111.12476)<br>:star:[code](https://github.com/MarcusNerva/HMN)
  * [SwinBERT: End-to-End Transformers With Sparse Attention for Video Captioning](https://arxiv.org/abs/2111.13196)<br>:star:[code](https://github.com/microsoft/SwinBERT)
* 深度估计
  * [Depth Estimation by Combining Binocular Stereo and Monocular Structured-Light](https://arxiv.org/abs/2203.10493)<br>:star:[code](https://github.com/YuhuaXu/MonoStereoFusion)
* 重识别
  * [Salient-to-Broad Transition for Video Person Re-Identification](https://openaccess.thecvf.com/content/CVPR2022/papers/Bai_Salient-to-Broad_Transition_for_Video_Person_Re-Identification_CVPR_2022_paper.pdf)<br>:star:[code](https://github.com/baist/SINet)
* transformer
  * [Object-Region Video Transformers](https://openaccess.thecvf.com/content/CVPR2022/papers/Herzig_Object-Region_Video_Transformers_CVPR_2022_paper.pdf)<br>:star:[code](https://github.com/roeiherz/ORViT):house:[project](https://roeiherz.github.io/ORViT/)
* 图像字幕
  * [DeeCap: Dynamic Early Exiting for Efficient Image Captioning](https://openaccess.thecvf.com/content/CVPR2022/papers/Fei_DeeCap_Dynamic_Early_Exiting_for_Efficient_Image_Captioning_CVPR_2022_paper.pdf)<br>:star:[code](https://github.com/feizc/DeeCap)
* 超参数优化
  * [AME: Attention and Memory Enhancement in Hyper-Parameter Optimization](https://openaccess.thecvf.com/content/CVPR2022/papers/Xu_AME_Attention_and_Memory_Enhancement_in_Hyper-Parameter_Optimization_CVPR_2022_paper.pdf)
* 运动捕捉
  * [LiDARCap: Long-Range Marker-Less 3D Human Motion Capture With LiDAR Point Clouds](https://arxiv.org/abs/2203.14698)
* Image Dewarping
  * [Revisiting Document Image Dewarping by Grid Regularization](https://arxiv.org/abs/2203.16850)
* 小样本
  * [Semi-Supervised Few-Shot Learning via Multi-Factor Clustering](https://openaccess.thecvf.com/content/CVPR2022/papers/Ling_Semi-Supervised_Few-Shot_Learning_via_Multi-Factor_Clustering_CVPR_2022_paper.pdf)<br>:star:[code](https://gitlab.com/smartllvlab/cluster-fsl)
  * [Cross-Domain Few-Shot Learning With Task-Specific Adapters](https://arxiv.org/abs/2107.00358)<br>:star:[code](https://github.com/VICO-UoE/URL)
* 分割
  * [CMT-DeepLab: Clustering Mask Transformers for Panoptic Segmentation](https://openaccess.thecvf.com/content/CVPR2022/papers/Yu_CMT-DeepLab_Clustering_Mask_Transformers_for_Panoptic_Segmentation_CVPR_2022_paper.pdf)
  * [Novel Class Discovery in Semantic Segmentation](https://arxiv.org/abs/2112.01900)<br>:star:[code](https://github.com/HeliosZhao/NCDSS):house:[project](https://ncdss.github.io/)
* 视觉描述
  * [Weakly-Supervised Generation and Grounding of Visual Descriptions With Conditional Generative Models](https://openaccess.thecvf.com/content/CVPR2022/papers/Mavroudi_Weakly-Supervised_Generation_and_Grounding_of_Visual_Descriptions_With_Conditional_Generative_CVPR_2022_paper.pdf)
* SR
  * [GCFSR: A Generative and Controllable Face Super Resolution Method Without Facial and GAN Priors](https://arxiv.org/abs/2203.07319)<br>:star:[code](https://github.com/hejingwenhejingwen/GCFSR)
  * [Learning the Degradation Distribution for Blind Image Super-Resolution](https://arxiv.org/abs/2203.04962)<br>:star:[code](https://github.com/greatlog/UnpairedSR)
* 轨迹预测
  * [M2I: From Factored Marginal Trajectory Prediction to Interactive Prediction](https://openaccess.thecvf.com/content/CVPR2022/papers/Sun_M2I_From_Factored_Marginal_Trajectory_Prediction_to_Interactive_Prediction_CVPR_2022_paper.pdf)
* 点云
  * [Domain Adaptation on Point Clouds via Geometry-Aware Implicits](https://arxiv.org/abs/2112.09343)<br>:star:[code](https://github.com/Jhonve/ImplicitPCDA)
  * [SC2-PCR: A Second Order Spatial Compatibility for Efficient and Robust Point Cloud Registration](https://openaccess.thecvf.com/content/CVPR2022/papers/Chen_SC2-PCR_A_Second_Order_Spatial_Compatibility_for_Efficient_and_Robust_CVPR_2022_paper.pdf)<br>:star:[code](https://github.com/ZhiChen902/SC2-PCR)
* HOI
  * [NeuralHOFusion: Neural Volumetric Rendering Under Human-Object Interactions](https://arxiv.org/abs/2202.12825)
* 姿态
  * [Generalizable Human Pose Triangulation](https://openaccess.thecvf.com/content/CVPR2022/papers/Bartol_Generalizable_Human_Pose_Triangulation_CVPR_2022_paper.pdf)
* 基于文本指导的图像操作
  * [DiffusionCLIP: Text-Guided Diffusion Models for Robust Image Manipulation](https://arxiv.org/abs/2110.02711)<br>:star:[code](https://github.com/gwang-kim/DiffusionCLIP)
* 对抗
  * [BppAttack: Stealthy and Efficient Trojan Attacks Against Deep Neural Networks via Image Quantization and Contrastive Adversarial Learning](https://arxiv.org/abs/2205.13383)<br>:star:[code](https://github.com/RU-System-Software-and-Security/BppAttack)
  * [Pyramid Adversarial Training Improves ViT Performance](https://arxiv.org/abs/2111.15121)<br>:house:[project](https://pyramidat.github.io/)
* SGG
  * [Stacked Hybrid-Attention and Group Collaborative Learning for Unbiased Scene Graph Generation](https://arxiv.org/abs/2203.09811)<br>:star:[code](https://github.com/dongxingning/SHA-GCL-for-SGG)
* GAN
  * [Ensembling Off-the-Shelf Models for GAN Training](https://arxiv.org/abs/2112.09130)<br>:open_mouth:oral:star:[code](https://github.com/nupurkmr9/vision-aided-gan):house:[project](https://www.cs.cmu.edu/~vision-aided-gan/)
  * [Style Transformer for Image Inversion and Editing](https://arxiv.org/abs/2203.07932)<br>:star:[code](https://github.com/sapphire497/style-transformer)
* 形状重建
  * [Topology-Preserving Shape Reconstruction and Registration via Neural Diffeomorphic Flow](https://arxiv.org/abs/2203.08652)<br>:star:[code](https://github.com/Siwensun/Neural_Diffeomorphic_Flow--NDF)
* 目标检测
  * [Segment and Complete: Defending Object Detectors Against Adversarial Patch Attacks With Robust Patch Detection](https://arxiv.org/abs/2112.04532)<br>:star:[code](https://github.com/joellliu/SegmentAndComplete)
* Part Segmentation
  * [Learning Part Segmentation through Unsupervised Domain Adaptation from Synthetic Vehicles](https://arxiv.org/abs/2103.14098)<br>:house:[project](https://qliu24.github.io/udapart/)
* 相机成像
  * [Learning to Zoom Inside Camera Imaging Pipeline](https://openaccess.thecvf.com/content/CVPR2022/papers/Tang_Learning_To_Zoom_Inside_Camera_Imaging_Pipeline_CVPR_2022_paper.pdf)<br>
* 图像去雾
  * [Self-augmented Unpaired Image Dehazing via Density and Depth Decomposition](https://openaccess.thecvf.com/content/CVPR2022/papers/Yang_Self-Augmented_Unpaired_Image_Dehazing_via_Density_and_Depth_Decomposition_CVPR_2022_paper.pdf)<br>:star:[code](https://github.com/YaN9-Y/D4)<br>:newspaper:[解读](https://mp.weixin.qq.com/s/lSY1is6Fmm6A0Db0Jxo4qg)
* 图像生成
  * [Modeling Image Composition for Complex Scene Generation](https://arxiv.org/abs/2206.00923)<br>:star:[code](https://github.com/JohnDreamer/TwFA)<br>:newspaper:[解读](https://mp.weixin.qq.com/s/lSY1is6Fmm6A0Db0Jxo4qg)
* 连续学习
  * [Continual Learning with Lifelong Vision Transformer](https://openaccess.thecvf.com/content/CVPR2022/papers/Wang_Continual_Learning_With_Lifelong_Vision_Transformer_CVPR_2022_paper.pdf)<br>:newspaper:[解读](https://mp.weixin.qq.com/s/lSY1is6Fmm6A0Db0Jxo4qg)
* 元学习
  * [Learning to Learn and Remember Super Long Multi-Domain Task Sequence](https://openaccess.thecvf.com/content/CVPR2022/papers/Wang_Learning_To_Learn_and_Remember_Super_Long_Multi-Domain_Task_Sequence_CVPR_2022_paper.pdf)<br>:open_mouth:oral:star:[code](https://github.com/joey-wang123/SDML)<br>:newspaper:[解读](https://mp.weixin.qq.com/s/lSY1is6Fmm6A0Db0Jxo4qg)
* 目标检测
  * [ISNet: Shape Matters for Infrared Small Target Detection](https://openaccess.thecvf.com/content/CVPR2022/papers/Zhang_ISNet_Shape_Matters_for_Infrared_Small_Target_Detection_CVPR_2022_paper.pdf)<br>:star:[code](https://github.com/RuiZhang97/ISNe)<br>:newspaper:[解读](https://mp.weixin.qq.com/s/lSY1is6Fmm6A0Db0Jxo4qg)
* 姿态
  * [Location-Free Human Pose Estimation](https://arxiv.org/abs/2205.12619)<br>:newspaper:[解读](https://mp.weixin.qq.com/s/FYn0S46OA6xraH9ofNIctw)
* 人脸
  * [End-to-End Reconstruction-Classification Learning for Face Forgery Detection](https://openaccess.thecvf.com/content/CVPR2022/papers/Cao_End-to-End_Reconstruction-Classification_Learning_for_Face_Forgery_Detection_CVPR_2022_paper.pdf)<br>:newspaper:[解读](https://mp.weixin.qq.com/s/FYn0S46OA6xraH9ofNIctw)
* 分割
  * [HybridCR: Weakly-Supervised 3D Point Cloud Semantic Segmentation via Hybrid Contrastive Regularization](https://openaccess.thecvf.com/content/CVPR2022/papers/Li_HybridCR_Weakly-Supervised_3D_Point_Cloud_Semantic_Segmentation_via_Hybrid_Contrastive_CVPR_2022_paper.pdf)<br>:newspaper:[解读](https://mp.weixin.qq.com/s/FYn0S46OA6xraH9ofNIctw)
* 3D
  * [Neural Fields As Learnable Kernels for 3D Reconstruction](https://arxiv.org/abs/2111.13674)<br>:house:[project](https://nv-tlabs.github.io/nkf/)
* 图像补全
  * [Bridging Global Context Interactions for High-Fidelity Image Completion](https://arxiv.org/abs/2104.00845)<br>:star:[code](https://github.com/lyndonzheng/TFill)
* 渲染
  * [InfoNeRF: Ray Entropy Minimization for Few-Shot Neural Volume Rendering](https://arxiv.org/abs/2112.15399)<br>:star:[code](https://github.com/mjmjeong/InfoNeRF):house:[project](https://cv.snu.ac.kr/research/InfoNeRF/)
* 运动预测
  * [Future Transformer for Long-term Action Anticipation](https://arxiv.org/abs/2205.14022)<br>:star:[code](https://github.com/gongda0e/FUTR):house:[project](http://cvlab.postech.ac.kr/research/FUTR/)
* 主动学习
  * [Towards Robust and Reproducible Active Learning Using Neural Networks](https://arxiv.org/abs/2002.09564)<br>:star:[code](https://github.com/PrateekMunjal/TorchAL)
* 长尾
  * [Retrieval Augmented Classification for Long-Tail Visual Recognition](https://arxiv.org/abs/2202.11233)
* VL
  * [NLX-GPT: A Model for Natural Language Explanations in Vision and Vision-Language Tasks](https://openaccess.thecvf.com/content/CVPR2022/papers/Sammani_NLX-GPT_A_Model_for_Natural_Language_Explanations_in_Vision_and_CVPR_2022_paper.pdf)<br>:open_mouth:oral:star:[code](https://github.com/fawazsammani/nlxgpt)
* 自监督
  * [Sound and Visual Representation Learning With Multiple Pretraining Tasks](https://arxiv.org/abs/2201.01046)
* 去噪
  * [RePaint: Inpainting Using Denoising Diffusion Probabilistic Models](https://arxiv.org/abs/2201.09865)<br>:star:[code](https://github.com/andreas128/RePaint)
* 其它
  * [Learning With Neighbor Consistency for Noisy Labels](https://arxiv.org/abs/2202.02200)
  * [GeoEngine: A Platform for Production-Ready Geospatial Research](https://openaccess.thecvf.com/content/CVPR2022/papers/Verma_GeoEngine_A_Platform_for_Production-Ready_Geospatial_Research_CVPR_2022_paper.pdf)
  * [Using 3D Topological Connectivity for Ghost Particle Reduction in Flow Reconstruction](https://openaccess.thecvf.com/content/CVPR2022/papers/Tsalicoglou_Using_3D_Topological_Connectivity_for_Ghost_Particle_Reduction_in_Flow_CVPR_2022_paper.pdf)
  * [On the Integration of Self-Attention and Convolution](https://arxiv.org/abs/2111.14556)<br>:star:[code](https://github.com/LeapLabTHU/ACmix)
  * [Towards Better Plasticity-Stability Trade-Off in Incremental Learning: A Simple Linear Connector](https://arxiv.org/abs/2110.07905)<br>:star:[code](https://github.com/lingl1024/Connector)
  * [MAXIM: Multi-Axis MLP for Image Processing](https://arxiv.org/abs/2201.02973)<br>:open_mouth:oral:star:[code](https://github.com/google-research/maxim)
  * [Delving Into the Estimation Shift of Batch Normalization in a Network](https://arxiv.org/abs/2203.10778)<br>:star:[code](https://github.com/huangleiBuaa/XBNBlock)
  * [Learning Object Context for Novel-View Scene Layout Generation](https://openaccess.thecvf.com/content/CVPR2022/papers/Qiao_Learning_Object_Context_for_Novel-View_Scene_Layout_Generation_CVPR_2022_paper.pdf)
  * [Dist-PU: Positive-Unlabeled Learning From a Label Distribution Perspective](https://openaccess.thecvf.com/content/CVPR2022/papers/Zhao_Dist-PU_Positive-Unlabeled_Learning_From_a_Label_Distribution_Perspective_CVPR_2022_paper.pdf)
  * [Relative Pose From a Calibrated and an Uncalibrated Smartphone Image](https://openaccess.thecvf.com/content/CVPR2022/papers/Ding_Relative_Pose_From_a_Calibrated_and_an_Uncalibrated_Smartphone_Image_CVPR_2022_paper.pdf)
  * [The Devil Is in the Margin: Margin-Based Label Smoothing for Network Calibration](https://arxiv.org/abs/2111.15430)<br>:star:[code](https://github.com/by-liu/MbLS)

## 目录

|:cat:|:dog:|:tiger:|:wolf:|
|------|------|------|------|
|[1.其它](#1)|[2.Image Segmentation(图像分割)](#2)|[3.Image Progress(图像处理)](#4)|[4.Image Captioning(图像字幕)](#)|
|[5.Object Detection(目标检测)](#5)|[6.Object Tracking(目标跟踪)](#6)|[7.Point Cloud(点云)](#7)|[8.Action Detection(人体动作检测与识别)](#8)|
|[9.Human Pose Estimation(人体姿态估计)](#9)|[10.3D(三维视觉)](#10)|[11.Face](#11)|[12.Image-to-Image Translation(图像到图像翻译)](#12)|
|[13.GAN](#13)|[14.Video](#14)|[15.Transformer](#15)|[16.Semi/self-supervised learning(半/自监督)](#16)|
|[17.Medical Image(医学影像)](#17)|[18.Person Re-Identification(人员重识别)](#18)|[19.Neural Architecture Search(神经架构搜索)](#19)|[20.Autonomous vehicles(自动驾驶)](#20)|
|[21.UAV/Remote Sensing/Satellite Image(无人机/遥感/卫星图像)](#21)|[22.Image Synthesis/Generation(图像合成)](#22)|[23.Image Retrieval(图像检索)](#23)|[24.Super-Resolution(超分辨率)](#24)|
|[25.Fine-Grained/Image Classification(细粒度/图像分类)](#25)|[26.GCN/GNN](#26)|[27.Pose Estimation(物体姿势估计)](#27)|[28.Style Transfer(风格迁移)](#28)|
|[29.Augmented Reality/Virtual Reality/Robotics(增强/虚拟现实/机器人)](#29)|[30.Visual Answer Questions(视觉问答)](#30)|[31.Vision-Language(视觉语言)](#31)|[32.Data Augmentation(数据增强)](#32)|
|[33.Human-Object Interaction(人物交互)](#33)|[34.Model Compression/Knowledge Distillation/Pruning(模型压缩/知识蒸馏/剪枝)](#34)|[35.OCR](#35)|[36.Optical Flow(光流估计)](#36)|
|[37.Contrastive Learning(对比学习)](#37)|[38.Meta-Learning(元学习)](#38)|[39.Continual Learning(持续学习)](#39)|[40.Adversarial Learning(对抗学习)](#40)|
|[41.Incremental Learning(增量学习)](#41)|[42.Metric Learning(度量学习)](#42)|[43.Multi-Task Learning(多任务学习)](#43)|[44.Federated Learning(联邦学习)](#44)|
|[45.Dense Prediction(密集预测)](#45)|[46.Scene Graph Generation(场景图生成)](#46)|[47.Few/Zero-Shot Learning/Domain Generalization/Adaptation(小/零样本/域泛化/适应)](#47)|[48.Visual Grounding](#48)|
|[49.Image Geo-localization(图像地理定位)](#49)|[50.Anomaly Detection(异常检测)](#50)|[51.光学、几何、光场成像](#51)|[52.Human Motion Forecasting(人体运动预测)](#52)|
|[53.Sign Language Translation(手语翻译)](#53)|[54.Dataset(数据集)](#54)|[55.Novel View Synthesis(视图合成)](#55)|[56.Sound](#56)|
|[57.Gaze Estimation(视线估计)](#57)|[58.Neural rendering(神经渲染)](#58)|[59.动画](#59)|[60.Visual Emotion Analysis(视觉情感分析)](#60)|


* 聚类
  * [DeepDPM: Deep Clustering With an Unknown Number of Clusters](https://arxiv.org/abs/2203.14309)<br>:star:[code](https://github.com/BGU-CS-VIL/DeepDPM)
* 场景流
  * [Exploiting Rigidity Constraints for LiDAR Scene Flow Estimation](https://openaccess.thecvf.com/content/CVPR2022/papers/Dong_Exploiting_Rigidity_Constraints_for_LiDAR_Scene_Flow_Estimation_CVPR_2022_paper.pdf)<br>:star:[code](https://github.com/gtdong-ustc/LiDARSceneFlow)
* 图识别
  * [Improving Subgraph Recognition With Variational Graph Information Bottleneck](https://arxiv.org/abs/2112.09899)<br>:star:[code](https://github.com/Samyu0304/Improving-Subgraph-Recognition-with-Variation-Graph-Information-Bottleneck-VGIB-)
* 运动模糊
  * [Motion-From-Blur: 3D Shape and Motion Estimation of Motion-Blurred Objects in Videos](https://openaccess.thecvf.com/content/CVPR2022/papers/Rozumnyi_Motion-From-Blur_3D_Shape_and_Motion_Estimation_of_Motion-Blurred_Objects_in_CVPR_2022_paper.pdf)
* 人像眼镜和阴影消除
  * [Portrait Eyeglasses and Shadow Removal by Leveraging 3D Synthetic Data](https://arxiv.org/abs/2203.10474)<br>:star:[code](https://github.com/StoryMY/take-off-eyeglasses) 
* 识别唇语
  * [Sub-Word Level Lip Reading With Visual Attention](https://arxiv.org/abs/2110.07603) 
* 模拟时钟读数
  * [It's About Time: Analog Clock Reading in the Wild](https://openaccess.thecvf.com/content/CVPR2022/papers/Yang_Its_About_Time_Analog_Clock_Reading_in_the_Wild_CVPR_2022_paper.pdf)<br>:star:[code](https://github.com/charigyang/itsabouttime):house:[project](https://www.robots.ox.ac.uk/~vgg/research/time/)
* 指纹识别
  * [Fingerprinting Deep Neural Networks Globally via Universal Adversarial Perturbations](https://arxiv.org/abs/2202.08602)<br>:open_mouth:oral
* 基于草图的图像操作
  * [SketchEdit: Mask-Free Local Image Manipulation with Partial Sketches](https://arxiv.org/abs/2111.15078)<br>:star:[code](https://github.com/zengxianyu/sketchedit):house:[project](https://zengxianyu.github.io/sketchedit/)
* 草图识别
  * [Finding Badly Drawn Bunnies](https://openaccess.thecvf.com/content/CVPR2022/papers/Yang_Finding_Badly_Drawn_Bunnies_CVPR_2022_paper.pdf)
* 去偏移
  * [Debiased Learning From Naturally Imbalanced Pseudo-Labels](https://arxiv.org/abs/2201.01490)<br>:star:[code](https://github.com/frank-xwang/debiased-pseudo-labeling)
* 线段分类
  * [Transformer Based Line Segment Classifier With Image Context for Real-Time Vanishing Point Detection in Manhattan World](https://openaccess.thecvf.com/content/CVPR2022/papers/Tong_Transformer_Based_Line_Segment_Classifier_With_Image_Context_for_Real-Time_CVPR_2022_paper.pdf)
* Interactive object understanding
  * [Human Hands as Probes for Interactive Object Understanding](https://arxiv.org/abs/2112.09120)<br>:star:[code](https://github.com/uiuc-robovision/hands-as-probes):house:[project](https://s-gupta.github.io/hands-as-probes/)
* 数字人类
  * [GOAL: Generating 4D Whole-Body Motion for Hand-Object Grasping](https://arxiv.org/abs/2112.11454)<br>:star:[code](https://github.com/otaheri/GOAL):house:[project](https://goal.is.tue.mpg.de/)
* 强化学习
  * [DECORE: Deep Compression With Reinforcement Learning](https://arxiv.org/abs/2106.06091)
* 视觉关系检测
  * [A Probabilistic Graphical Model Based on Neural-symbolic Reasoning for Visual Relationship Detection](https://openaccess.thecvf.com/content/CVPR2022/papers/Yu_A_Probabilistic_Graphical_Model_Based_on_Neural-Symbolic_Reasoning_for_Visual_CVPR_2022_paper.pdf)
* 裂缝识别
  * [Geometry-Aware Guided Loss for Deep Crack Recognition](https://openaccess.thecvf.com/content/CVPR2022/papers/Chen_Geometry-Aware_Guided_Loss_for_Deep_Crack_Recognition_CVPR_2022_paper.pdf)
* 眼球认证
  * [EyePAD++: A Distillation-Based Approach for Joint Eye Authentication and Presentation Attack Detection Using Periocular Images](https://openaccess.thecvf.com/content/CVPR2022/papers/Dhar_EyePAD_A_Distillation-Based_Approach_for_Joint_Eye_Authentication_and_Presentation_CVPR_2022_paper.pdf)


## Open-Set Recognition(开集识别)
* [Task-Adaptive Negative Envision for Few-Shot Open-Set Recognition](https://openaccess.thecvf.com/content/CVPR2022/papers/Huang_Task-Adaptive_Negative_Envision_for_Few-Shot_Open-Set_Recognition_CVPR_2022_paper.pdf)<br>:star:[code](https://github.com/shiyuanh/TANE)

## Active Learning(主动学习)
* [Active Learning for Open-Set Annotation](https://arxiv.org/abs/2201.06758)
* [Active Learning by Feature Mixing](https://arxiv.org/abs/2203.07034)
* [Towards Robust and Reproducible Active Learning Using Neural Networks](https://arxiv.org/abs/2002.09564)<br>:star:[code](https://github.com/PrateekMunjal/TorchAL)

## Backdoor Attacks(后门攻击)
* [DEFEAT: Deep Hidden Feature Backdoor Attacks by Imperceptible Perturbation and Latent Representation Constraints](https://openaccess.thecvf.com/content/CVPR2022/papers/Zhao_DEFEAT_Deep_Hidden_Feature_Backdoor_Attacks_by_Imperceptible_Perturbation_and_CVPR_2022_paper.pdf)
* [Better Trigger Inversion Optimization in Backdoor Scanning](https://openaccess.thecvf.com/content/CVPR2022/papers/Tao_Better_Trigger_Inversion_Optimization_in_Backdoor_Scanning_CVPR_2022_paper.pdf)

## Multi-view Clustering(多视图聚类)
* [Highly-efficient Incomplete Large-scale Multi-view Clustering with Consensus Bipartite Graph](https://openaccess.thecvf.com/content/CVPR2022/papers/Wang_Highly-Efficient_Incomplete_Large-Scale_Multi-View_Clustering_With_Consensus_Bipartite_Graph_CVPR_2022_paper.pdf)<br>:star:[code](https://github.com/wangsiwei2010/CVPR22-IMVC-CBG)
* [Multi-Level Feature Learning for Contrastive Multi-View Clustering](https://arxiv.org/abs/2106.11193)<br>:star:[code](https://github.com/SubmissionsIn/MFLVC)
* [Deep Safe Multi-View Clustering: Reducing the Risk of Clustering Performance Degradation Caused by View Increase](https://openaccess.thecvf.com/content/CVPR2022/papers/Tang_Deep_Safe_Multi-View_Clustering_Reducing_the_Risk_of_Clustering_Performance_CVPR_2022_paper.pdf)
  
## Machine Translation(机器翻译)
* [VALHALLA: Visual Hallucination for Machine Translation](https://arxiv.org/abs/2206.00100)<br>:house:[project](http://www.svcl.ucsd.edu/projects/valhalla/)

## Object Counting(目标计数)
* [Rethinking Spatial Invariance of Convolutional Networks for Object Counting](https://arxiv.org/abs/2206.05253)<br>:star:[code]([https://github.com/ZhengChang467/STIPHR](https://github.com/zhiqic/Rethinking-Counting)):newspaper:[解读](https://zhuanlan.zhihu.com/p/528028523)
* [Represent, Compare, and Learn: A Similarity-Aware Framework for Class-Agnostic Counting](https://arxiv.org/abs/2203.08354)<br>:star:[code](https://github.com/flyinglynx/Bilinear-Matching-Network)
  
## computer-aided design (CAD)
* [Neural Face Identification in a 2D Wireframe Projection of a Manifold Object](https://arxiv.org/abs/2203.04229)<br>:star:[code](https://github.com/manycore-research/faceformer)
* [JoinABLe: Learning Bottom-up Assembly of Parametric CAD Joints](https://arxiv.org/abs/2111.12772)<br>:star:[code](https://github.com/AutodeskAILab/JoinABLe)
* [ROCA: Robust CAD Model Retrieval and Alignment from a Single Image](https://openaccess.thecvf.com/content/CVPR2022/papers/Gumeli_ROCA_Robust_CAD_Model_Retrieval_and_Alignment_From_a_Single_CVPR_2022_paper.pdf)<br>:star:[code](https://github.com/cangumeli/ROCA)

## Transfer Learning(迁移学习)
* [Revisiting Learnable Affines for Batch Norm in Few-Shot Transfer Learning](https://openaccess.thecvf.com/content/CVPR2022/papers/Yazdanpanah_Revisiting_Learnable_Affines_for_Batch_Norm_in_Few-Shot_Transfer_Learning_CVPR_2022_paper.pdf)

## Graph Matching(图匹配)
  * [Graph-Context Attention Networks for Size-Varied Deep Graph Matching](https://openaccess.thecvf.com/content/CVPR2022/papers/Jiang_Graph-Context_Attention_Networks_for_Size-Varied_Deep_Graph_Matching_CVPR_2022_paper.pdf)<br>:star:[code](https://github.com/ZhehengJiang/GCAN)

## Noise Modeling(图像噪声建模)
  * [Noise2NoiseFlow: Realistic Camera Noise Modeling Without Clean Images](https://arxiv.org/abs/2206.01103)<br>:house:[project](https://yorkucvil.github.io/Noise2NoiseFlow/)

<a name="60"/>

## 60.Visual Emotion Analysis(视觉情感分析)
* [MDAN: Multi-level Dependent Attention Network for Visual Emotion Analysis](https://arxiv.org/abs/2203.13443)

<a name="59"/>

## 59.动画
* [APES: Articulated Part Extraction From Sprite Sheets](https://openaccess.thecvf.com/content/CVPR2022/papers/Xu_APES_Articulated_Part_Extraction_From_Sprite_Sheets_CVPR_2022_paper.pdf)<br>:house:[project](https://zhan-xu.github.io/parts/)
* [BANMo: Building Animatable 3D Neural Models From Many Casual Videos](https://arxiv.org/abs/2112.12761)<br>:open_mouth:oral:house:[project](https://banmo-www.github.io/)
* [Neural Head Avatars From Monocular RGB Videos](https://arxiv.org/abs/2112.01554)<br>:star:[code](https://github.com/philgras/neural-head-avatars):house:[project](https://philgras.github.io/neural_head_avatars/neural_head_avatars.html)
* 图像动画
  * [Thin-Plate Spline Motion Model for Image Animation](https://arxiv.org/abs/2203.14367)<br>:star:[code](https://github.com/yoyo-nb/Thin-Plate-Spline-Motion-Model)
* 人物动画
  * [Structured Local Radiance Fields for Human Avatar Modeling](https://arxiv.org/abs/2203.14478)
* 3D character animation(三维角色动画)
  * 皮肤预测  
    * [SkinningNet: Two-Stream Graph Convolutional Neural Network for Skinning Prediction of Synthetic Characters](https://arxiv.org/abs/2203.04746)<br>:house:[project](https://imatge-upc.github.io/skinningnet/)
* 3D 舞蹈生成
  * [Bailando: 3D Dance Generation by Actor-Critic GPT with Choreographic Memory](https://arxiv.org/abs/2203.13055)<br>:star:[code](https://github.com/lisiyao21/Bailando/)
  * [A Brand New Dance Partner: Music-Conditioned Pluralistic Dancing Controlled by Multiple Dance Genres](https://openaccess.thecvf.com/content/CVPR2022/papers/Kim_A_Brand_New_Dance_Partner_Music-Conditioned_Pluralistic_Dancing_Controlled_by_CVPR_2022_paper.pdf)
* 静止图像到动画
  * [Controllable Animation of Fluid Elements in Still Images](https://arxiv.org/abs/2112.03051)<br>:house:[project](https://controllable-cinemagraphs.github.io/) 

<a name="58"/>

## 58.Neural rendering(神经渲染)
* [Learning Motion-Dependent Appearance for High-Fidelity Rendering of Dynamic Humans from a Single Camera](https://arxiv.org/abs/2203.12780)          
* [IRON: Inverse Rendering by Optimizing Neural SDFs and Materials from Photometric Images](https://arxiv.org/abs/2204.02232)<br>:open_mouth:oral:house:[project](https://kai-46.github.io/IRON-website/)
* [SqueezeNeRF: Further factorized FastNeRF for memory-efficient inference](https://arxiv.org/abs/2204.02585)
* [Direct Voxel Grid Optimization: Super-fast Convergence for Radiance Fields Reconstruction](https://arxiv.org/abs/2111.11215)<br>:star:[code](https://github.com/sunset1995/DirectVoxGO)
* [Modeling Indirect Illumination for Inverse Rendering](https://arxiv.org/abs/2204.06837)<br>:star:[code](https://github.com/zju3dv/invrender):house:[project](https://zju3dv.github.io/invrender/)
* [GenDR: A Generalized Differentiable Renderer](https://arxiv.org/abs/2204.13845)<br>:star:[code](https://github.com/Felix-Petersen/gendr)<br>泛化可微渲染器
* [CLIP-NeRF: Text-and-Image Driven Manipulation of Neural Radiance Fields](https://arxiv.org/abs/2112.05139)<br>:star:[code](https://github.com/cassiePython/CLIPNeRF):house:[project](https://cassiepython.github.io/clipnerf/)
* [NeRF-Editing: Geometry Editing of Neural Radiance Fields](https://arxiv.org/abs/2205.04978)
* [AR-NeRF: Unsupervised Learning of Depth and Defocus Effects from Natural Images with Aperture Rendering Neural Radiance Fields](https://arxiv.org/abs/2206.06100)<br>:house:[project](https://www.kecl.ntt.co.jp/people/kaneko.takuhiro/projects/ar-nerf/)
* [Neural Rays for Occlusion-Aware Image-Based Rendering](https://arxiv.org/abs/2107.13421)<br>:star:[code](https://github.com/liuyuan-pal/NeuRay):house:[project](https://liuyuan-pal.github.io/NeuRay/)
* [EfficientNeRF Efficient Neural Radiance Fields](https://arxiv.org/abs/2206.00878)<br>:star:[code](https://github.com/dvlab-research/EfficientNeRF)
* [CoNeRF: Controllable Neural Radiance Fields](https://arxiv.org/abs/2112.01983)<br>:star:[code](https://github.com/kacperkan/conerf):house:[project](https://conerf.github.io/)
* [Mip-NeRF 360: Unbounded Anti-Aliased Neural Radiance Fields](https://arxiv.org/abs/2111.12077)<br>:house:[project](https://jonbarron.info/mipnerf360/)
* [Hallucinated Neural Radiance Fields in the Wild](https://arxiv.org/abs/2111.15246)<br>:star:[code](https://github.com/rover-xingyu/Ha-NeRF):house:[project](https://rover-xingyu.github.io/Ha-NeRF/)
* [HumanNeRF: Free-viewpoint Rendering of Moving People from Monocular Video](https://arxiv.org/abs/2201.04127)<br>:open_mouth:oral:star:[code](https://github.com/chungyiweng/humannerf):house:[project](https://grail.cs.washington.edu/projects/humannerf/):tv:[video](https://youtu.be/GM-RoZEymmw)
* [Ref-NeRF: Structured View-Dependent Appearance for Neural Radiance Fields](https://openaccess.thecvf.com/content/CVPR2022/papers/Verbin_Ref-NeRF_Structured_View-Dependent_Appearance_for_Neural_Radiance_Fields_CVPR_2022_paper.pdf)
* [Deblur-NeRF: Neural Radiance Fields From Blurry Images](https://openaccess.thecvf.com/content/CVPR2022/papers/Ma_Deblur-NeRF_Neural_Radiance_Fields_From_Blurry_Images_CVPR_2022_paper.pdf)<br>:star:[code](https://github.com/limacv/Deblur-NeRF):house:[project](https://limacv.github.io/deblurnerf/)
* [NeRFReN: Neural Radiance Fields With Reflections](https://arxiv.org/abs/2111.15234)<br>:house:[project](https://bennyguo.github.io/nerfren/)
* [Depth-Supervised NeRF: Fewer Views and Faster Training for Free](https://arxiv.org/abs/2107.02791)<br>:star:[code](https://github.com/dunbar12138/DSNeRF):house:[project](http://www.cs.cmu.edu/~dsnerf/)
* [Dense Depth Priors for Neural Radiance Fields From Sparse Input Views](https://arxiv.org/abs/2112.03288)<br>:star:[code](https://github.com/barbararoessle/dense_depth_priors_nerf):house:[project](https://barbararoessle.github.io/dense_depth_priors_nerf/):tv:[video](https://www.youtube.com/watch?v=zzkvvdcvksc)
* [Light Field Neural Rendering](https://arxiv.org/abs/2112.09687)<br>:star:[code](https://github.com/google-research/google-research/tree/master/light_field_neural_rendering):house:[project](https://light-field-neural-rendering.github.io/)
* [InfoNeRF: Ray Entropy Minimization for Few-Shot Neural Volume Rendering](https://arxiv.org/abs/2112.15399)<br>:star:[code](https://github.com/mjmjeong/InfoNeRF):house:[project](https://cv.snu.ac.kr/research/InfoNeRF/)

<a name="57"/>

## 57.Gaze Estimation(视线估计)
* [GazeOnce: Real-Time Multi-Person Gaze Estimation](https://arxiv.org/abs/2204.09480)
* [Contrastive Regression for Domain Adaptation on Gaze Estimation](https://openaccess.thecvf.com/content/CVPR2022/papers/Wang_Contrastive_Regression_for_Domain_Adaptation_on_Gaze_Estimation_CVPR_2022_paper.pdf) 
* [Generalizing Gaze Estimation With Rotation Consistency](https://openaccess.thecvf.com/content/CVPR2022/papers/Bao_Generalizing_Gaze_Estimation_With_Rotation_Consistency_CVPR_2022_paper.pdf)
* [GaTector: A Unified Framework for Gaze Object Prediction](https://arxiv.org/abs/2112.03549)
* [Dynamic 3D Gaze From Afar: Deep Gaze Estimation From Temporal Eye-Head-Body Coordination](https://openaccess.thecvf.com/content/CVPR2022/papers/Nonaka_Dynamic_3D_Gaze_From_Afar_Deep_Gaze_Estimation_From_Temporal_CVPR_2022_paper.pdf)<br>:house:[project](https://vision.ist.i.kyoto-u.ac.jp/)
  
<a name="56"/>

## 56.Sound
* [Finding Fallen Objects via Asynchronous Audio-Visual Integration](https://openaccess.thecvf.com/content/CVPR2022/papers/Gan_Finding_Fallen_Objects_via_Asynchronous_Audio-Visual_Integration_CVPR_2022_paper.pdf)<br>:house:[project](http://fallen-object.csail.mit.edu/)
* 声源定位
  * [Self-Supervised Predictive Learning: A Negative-Free Method for Sound Source Localization in Visual Scenes](https://arxiv.org/abs/2203.13412)<br>:star:[code](https://github.com/zjsong/SSPL)
  * [Mix and Localize: Localizing Sound Sources in Mixtures](https://openaccess.thecvf.com/content/CVPR2022/papers/Hu_Mix_and_Localize_Localizing_Sound_Sources_in_Mixtures_CVPR_2022_paper.pdf)
* 音频配对
  * [It's Time for Artistic Correspondence in Music and Video](https://openaccess.thecvf.com/content/CVPR2022/papers/Suris_Its_Time_for_Artistic_Correspondence_in_Music_and_Video_CVPR_2022_paper.pdf)<br>:house:[project](https://musicforvideo.cs.columbia.edu/) 
* 语音克隆
  * [V2C: Visual Voice Cloning](https://arxiv.org/abs/2111.12890)<br>:star:[code](https://github.com/chenqi008/V2C)
* 视听语音增强
  * [Audio-Visual Speech Codecs: Rethinking Audio-Visual Speech Enhancement by Re-Synthesis](https://openaccess.thecvf.com/content/CVPR2022/papers/Yang_Audio-Visual_Speech_Codecs_Rethinking_Audio-Visual_Speech_Enhancement_by_Re-Synthesis_CVPR_2022_paper.pdf)<br>:tv:[video](https://github.com/facebookresearch/facestar/releases/download/paper_materials/video.mp4)
* 文本转语音
  * [More Than Words: In-the-Wild Visually-Driven Prosody for Text-to-Speech](https://arxiv.org/abs/2111.10139)<br>:star:[code](https://google-research.github.io/lingvo-lab/vdtts/)
* 语音转人脸图像
  * [Cross-Modal Perceptionist: Can Face Geometry be Gleaned from Voices?](https://arxiv.org/abs/2203.09824)<br>:star:[code](https://github.com/choyingw/Cross-Modal-Perceptionist):house:[project](https://choyingw.github.io/works/Voice2Mesh/index.html)

<a name="55"/>

## 55.Novel View Synthesis(视图合成)
* [NPBG++: Accelerating Neural Point-Based Graphics](https://arxiv.org/abs/2203.)<br>:house:[project](htt.io/npbgpp/)
* [Scene Representation Transformer: Geometry-Free Novel View Synthesis Through Set-Latent Scene Representations](https://arxiv.org/abs/2111.13152)<br>:house:[project](https://srt-paper.github.io/)
* [AutoRF: Learning 3D Object Radiance Fields from Single View Observations](https://arxiv.org/abs/2204.03593)<br>:house:[project](https://sirwyver.github.io/AutoRF/)
* [NeurMiPs: Neural Mixture of Planar Experts for View Synthesis](https://arxiv.org/abs/2204.13696)<br>:star:[code](https://github.com/zhihao-lin/neurmips):house:[project](https://zhihao-lin.github.io/neurmips/):tv:[video](https://youtu.be/PV1dCTWL5Oo):newspaper:[解读](https://zhuanlan.zhihu.com/p/507053208)
* [GeoNeRF: Generalizing NeRF with Geometry Priors](https://arxiv.org/abs/2111.13539)<br>:star:[code](https://www.idiap.ch/paper/geonerf):house:[project](https://www.idiap.ch/paper/geonerf/):tv:[video](https://www.youtube.com/watch?v=-jNBsG3IP54)
* [FWD: R eal-Time Novel View Synthesis With Forward Warping and Depth](https://openaccess.thecvf.com/content/CVPR2022/papers/Cao_FWD_Real-Time_Novel_View_Synthesis_With_Forward_Warping_and_Depth_CVPR_2022_paper.pdf)<br>:star:[code](https://github.com/Caoang327/fwd_code)
* [Block-NeRF: Scalable Large Scene Neural View Synthesis](https://openaccess.thecvf.com/content/CVPR2022/papers/Tancik_Block-NeRF_Scalable_Large_Scene_Neural_View_Synthesis_CVPR_2022_paper.pdf)
* 视图连接
  * [Connecting the Complementary-View Videos: Joint Camera Identification and Subject Association](https://openaccess.thecvf.com/content/CVPR2022/papers/Han_Connecting_the_Complementary-View_Videos_Joint_Camera_Identification_and_Subject_Association_CVPR_2022_paper.pdf)<br>:star:[code](https://github.com/RuizeHan/DMHA)

<a name="54"/>

## 54.Dataset(数据集)
* [ObjectFolder 2.0: A Multisensory Object Dataset for Sim2Real Transfer](https://arxiv.org/abs/2204.02389)<br>:star:[code](https://github.com/rhgao/ObjectFolder):house:[project](https://ai.stanford.edu/~rhgao/objectfolder2.0/):newspaper:[粗解](https://zhuanlan.zhihu.com/p/493615566)
* [Assembly101: A Large-Scale Multi-View Video Dataset for Understanding Procedural Activities](https://arxiv.org/abs/2203.14712)<br>:star:[code](https://github.com/assembly-101?tab=repositories):house:[project](https://assembly-101.github.io/)
* [3MASSIV: Multilingual, Multimodal and Multi-Aspect dataset of Social Media Short Videos](https://arxiv.org/abs/2203.14456)
* [Hephaestus: A large scale multitask dataset towards InSAR understanding](https://arxiv.org/abs/2204.09435)
* [SmartPortraits: Depth Powered Handheld Smartphone Dataset of Human Portraits for State Estimation, Reconstruction and Synthesis](https://arxiv.org/abs/2204.10211)<br>:sunflower:[dataset](https://mobileroboticsskoltech.github.io/SmartPortraits/)
* [AKB-48: A Real-World Articulated Object Knowledge Base](https://arxiv.org/abs/2202.08432)<br>:star:[code](https://liuliu66.github.io/articulationobjects/)<br>:newspaper:[粗解](https://news.sjtu.edu.cn/jdzh/20220330/169456.html)
* [Primitive3D: 3D Object Dataset Synthesis from Randomly Assembled Primitives](https://arxiv.org/abs/2205.12627)
* [ZeroWaste Dataset: Towards Deformable Object Segmentation in Cluttered Scenes](https://arxiv.org/abs/2106.02740)<br>:star:[code](https://github.com/dbash/zerowaste/):house:[project](http://ai.bu.edu/zerowaste/)
* [ETHSeg: An Amodel Instance Segmentation Network and a Real-World Dataset for X-Ray Waste Inspection](https://openaccess.thecvf.com/content/CVPR2022/papers/Qiu_ETHSeg_An_Amodel_Instance_Segmentation_Network_and_a_Real-World_Dataset_CVPR_2022_paper.pdf)<br>一个Amodel实例分割网络和一个用于X射线废物检查的真实数据集
* [MAD: A Scalable Dataset for Language Grounding in Videos From Movie Audio Descriptions](https://openaccess.thecvf.com/content/CVPR2022/papers/Soldan_MAD_A_Scalable_Dataset_for_Language_Grounding_in_Videos_From_CVPR_2022_paper.pdf)<br>:sunflower:[dataset](https://github.com/Soldelli/MAD)<br>一个可扩展的数据集，用于从电影音频描述中获得视频的Language Grounding 
* 卫星数据集
  * [DynamicEarthNet: Daily Multi-Spectral Satellite Dataset for Semantic Change Segmentation](https://arxiv.org/abs/2203.12560)
* 动物行为理解数据集
  * [Animal Kingdom: A Large and Diverse Dataset for Animal Behavior Understanding](https://arxiv.org/abs/2204.08129)<br>:open_mouth:oral:house:[project](https://sutdcv.github.io/Animal-Kingdom/):sunflower:[dataset](https://github.com/SUTDCV/Animal-Kingdom) 
* 数据集(森林监测)
  * [The Auto Arborist Dataset: A Large-Scale Benchmark for Multiview Urban Forest Monitoring Under Domain Shift](https://openaccess.thecvf.com/content/CVPR2022/papers/Beery_The_Auto_Arborist_Dataset_A_Large-Scale_Benchmark_for_Multiview_Urban_CVPR_2022_paper.pdf)<br>:sunflower:[dataset](https://google.github.io/auto-arborist/)   
* 3D目标理解
  * [ABO: Dataset and Benchmarks for Real-World 3D Object Understanding](https://arxiv.org/abs/2110.06199)<br>:sunflower:[dataset](https://amazon-berkeley-objects.s3.amazonaws.com/index.html)
* 数据集(AutoMine)
  * [AutoMine: An Unmanned Mine Dataset](https://openaccess.thecvf.com/content/CVPR2022/papers/Li_AutoMine_An_Unmanned_Mine_Dataset_CVPR_2022_paper.pdf)<br>:sunflower:[dataset](https://automine.cc/)
* 数据集(人脸表情识别)
  * [FERV39k: A Large-Scale Multi-Scene Dataset for Facial Expression Recognition in Videos](https://arxiv.org/abs/2203.09463)<br>:star:[code](https://github.com/wangyanckxx/FERV39k)
* 数据集(手势识别)
  * [LD-ConGR: A Large RGB-D Video Dataset for Long-Distance Continuous Gesture Recognition](https://openaccess.thecvf.com/content/CVPR2022/papers/Liu_LD-ConGR_A_Large_RGB-D_Video_Dataset_for_Long-Distance_Continuous_Gesture_CVPR_2022_paper.pdf)<br>:star:[code](https://github.com/Diananini/LD-ConGR-CVPR2022)

<a name="53"/>

## 53.Sign Language Translation(手语翻译)
* [A Simple Multi-Modality Transfer Learning Baseline for Sign Language Translation](https://arxiv.org/abs/2203.04287)
* [Signing at Scale: Learning to Co-Articulate Signs for Large-Scale Photo-Realistic Sign Language Production](https://arxiv.org/abs/2203.15354)
* [MLSLT: Towards Multilingual Sign Language Translation](https://openaccess.thecvf.com/content/CVPR2022/papers/Yin_MLSLT_Towards_Multilingual_Sign_Language_Translation_CVPR_2022_paper.pdf)<br>:house:[project](https://mlslt.github.io/)
* 手语识别
  * [C2SLR: Consistency-Enhanced Continuous Sign Language Recognition](https://openaccess.thecvf.com/content/CVPR2022/papers/Zuo_C2SLR_Consistency-Enhanced_Continuous_Sign_Language_Recognition_CVPR_2022_paper.pdf)

<a name="52"/>

## 52.Human Motion Forecasting(人体运动预测)
* [Motron: Multimodal Probabilistic Human Motion Forecasting](https://arxiv.org/abs/2203.04132)
* [Progressively Generating Better Initial Guesses Towards Next Stages for High-Quality Human Motion Prediction](https://arxiv.org/abs/2203.16051)<br>:star:[code](https://github.com/705062791/PGBIG)
* [Spatio-Temporal Gating-Adjacency GCN for Human Motion Prediction](https://openaccess.thecvf.com/content/CVPR2022/papers/Zhong_Spatio-Temporal_Gating-Adjacency_GCN_for_Human_Motion_Prediction_CVPR_2022_paper.pdf)
* [MotionAug: Augmentation With Physical Correction for Human Motion Prediction](https://arxiv.org/abs/2203.09116)<br>:star:[code](https://github.com/meaten/MotionAug)
* [Future Transformer for Long-term Action Anticipation](https://arxiv.org/abs/2205.14022)<br>:star:[code](https://github.com/gongda0e/FUTR):house:[project](http://cvlab.postech.ac.kr/research/FUTR/)


<a name="51"/>

## 51.光学、几何、光场成像
* [Compressive Single-Photon 3D Cameras](https://openaccess.thecvf.com/content/CVPR2022/papers/Gutierrez-Barragan_Compressive_Single-Photon_3D_Cameras_CVPR_2022_paper.pdf)
* Light Field(光场)
  * [Occlusion-Aware Cost Constructor for Light Field Depth Estimation](https://arxiv.org/abs/2203.01576)<br>:star:[code](https://github.com/YingqianWang/OACC-Net):newspaper:[粗解](https://zhuanlan.zhihu.com/p/475067096)
  * [Neural Point Light Fields](https://arxiv.org/abs/2112.01473)<br>:star:[code](https://github.com/princeton-computational-imaging/neural-point-light-fields):house:[project](https://light.princeton.edu/publication/neural-point-light-fields/)
  * [Acquiring a Dynamic Light Field Through a Single-Shot Coded Image](https://arxiv.org/abs/2204.12089)
* 深度重建
  * [Deep Hyperspectral-Depth Reconstruction Using Single Color-Dot Projection](https://arxiv.org/abs/2204.03929)<br>:star:[code](https://github.com/GrayMask/DHD):house:[project](http://www.ok.sc.e.titech.ac.jp/res/DHD/):tv:[video](https://youtu.be/LgGDqDf034g)
* 快门校正
  * [Learning Adaptive Warping for Real-World Rolling Shutter Correction](https://arxiv.org/abs/2204.13886)<br>:star:[code](https://github.com/ljzycmd/BSRSC)
* 热红外成像
  * [Infrared Invisible Clothing:Hiding from Infrared Detectors at Multiple Angles in Real World](https://arxiv.org/abs/2205.05909)<br>:open_mouth:oral
* 相机姿势估计
  * [DiffPoseNet: Direct Differentiable Camera Pose Estimation](https://openaccess.thecvf.com/content/CVPR2022/papers/Parameshwara_DiffPoseNet_Direct_Differentiable_Camera_Pose_Estimation_CVPR_2022_paper.pdf)
* 相机重定位
  * [SceneSqueezer: Learning to Compress Scene for Camera Relocalization](https://openaccess.thecvf.com/content/CVPR2022/papers/Yang_SceneSqueezer_Learning_To_Compress_Scene_for_Camera_Relocalization_CVPR_2022_paper.pdf)<br>:open_mouth:oral
* 成像
  * [Adaptive Gating for Single-Photon 3D Imaging](https://arxiv.org/abs/2111.15047)
  * [All-photon Polarimetric Time-of-Flight Imaging](https://arxiv.org/abs/2112.09278)
* 光学
  * [Quantization-aware Deep Optics for Diffractive Snapshot Hyperspectral Imaging](https://openaccess.thecvf.com/content/CVPR2022/papers/Li_Quantization-Aware_Deep_Optics_for_Diffractive_Snapshot_Hyperspectral_Imaging_CVPR_2022_paper.pdf)<br>:star:[code](https://github.com/wang-lizhi/QuantizationAwareDeepOptics)
* 相机姿势
  * [Camera Pose Estimation Using Implicit Distortion Models](https://openaccess.thecvf.com/content/CVPR2022/papers/Pan_Camera_Pose_Estimation_Using_Implicit_Distortion_Models_CVPR_2022_paper.pdf)
* 相机成像
  * [Learning to Zoom Inside Camera Imaging Pipeline](https://openaccess.thecvf.com/content/CVPR2022/papers/Tang_Learning_To_Zoom_Inside_Camera_Imaging_Pipeline_CVPR_2022_paper.pdf)<br>

<a name="50"/>

## 50.Anomaly Detection(异常检测)
* [Catching Both Gray and Black Swans: Open-set Supervised Anomaly Detection](https://arxiv.org/abs/2203.14506)<br>:star:[code](https://github.com/choubo/DRA)
* [Self-Supervised Predictive Convolutional Attentive Block for Anomaly Detection](https://arxiv.org/abs/2111.09099)<br>:star:[code](https://github.com/ristea/sspcab)
* [Anomaly Detection via Reverse Distillation From One-Class Embedding](https://arxiv.org/abs/2201.10703)
* [Towards Total Recall in Industrial Anomaly Detection](https://arxiv.org/abs/2106.08265)<br>:star:[code](https://github.com/amazon-research/patchcore-inspection)
* 离群点检测
  * [Robust outlier detection by de-biasing VAE likelihoods](https://arxiv.org/abs/2108.08760)

<a name="49"/>

## 49.Image Geo-localization(图像地理定位)
* [TransGeo: Transformer Is All You Need for Cross-view Image Geo-localization](https://arxiv.org/abs/2204.00097)<br>:star:[code](https://github.com/Jeff-Zilence/TransGeo2022)
* 视觉地理定位
  * [Rethinking Visual Geo-localization for Large-Scale Applications](https://arxiv.org/abs/2204.02287)<br>:star:[code](https://github.com/gmberton/CosPlace)
  * [Deep Visual Geo-localization Benchmark](https://arxiv.org/abs/2204.03444)<br>:open_mouth:oral:house:[project](https://deep-vg-bench.herokuapp.com/)
* 轨迹重建
  * [MonoTrack: Shuttle trajectory reconstruction from monocular badminton video](https://arxiv.org/abs/2204.01899)

<a name="48"/>

## 48.Visual Grounding
* [Multi-View Transformer for 3D Visual Grounding](https://arxiv.org/abs/2204.02174)<br>:star:[code](https://github.com/sega-hsj/MVT-3DVG)
* [Improving Visual Grounding with Visual-Linguistic Verification and Iterative Reasoning](https://arxiv.org/abs/2205.00272)<br>:star:[code](https://github.com/yangli18/VLTVG)<br>视觉定位，通过自然语言定位目标位置 （很有意思的研究）
* [Shifting More Attention to Visual Backbone: Query-Modulated Refinement Networks for End-to-End Visual Grounding](https://arxiv.org/abs/2203.15442)<br>:star:[code](https://github.com/LukeForeverYoung/QRNet)   

<a name="47"/>

## 47.Few/Zero-Shot Learning/Domain Generalization/Adaptation(小/零样本/域泛化/适应)
* 小样本
  * [Ranking Distance Calibration for Cross-Domain Few-Shot Learning](https://arxiv.org/abs/2112.00260)
  * [Few-shot Learning with Noisy Labels](https://arxiv.org/abs/2204.05494)
  * [Pushing the Limits of Simple Pipelines for Few-Shot Learning: External Data and Fine-Tuning Make a Difference](https://arxiv.org/abs/2204.07305)<br>:house:[project](https://hushell.github.io/pmf/):tv:[video](https://youtu.be/iEC9lh18laQ)
  * [Few-shot Backdoor Defense Using Shapley Estimation](https://arxiv.org/abs/2112.14889)<br>:newspaper:[解读](https://mp.weixin.qq.com/s/lSY1is6Fmm6A0Db0Jxo4qg)
  * [Attribute Surrogates Learning and Spectral Tokens Pooling in Transformers for Few-Shot Learning](https://arxiv.org/abs/2203.09064)<br>:star:[code](https://github.com/StomachCold/HCTransformers)
  * [EASE: Unsupervised Discriminant Subspace Learning for Transductive Few-Shot Learning](https://openaccess.thecvf.com/content/CVPR2022/papers/Zhu_EASE_Unsupervised_Discriminant_Subspace_Learning_for_Transductive_Few-Shot_Learning_CVPR_2022_paper.pdf)<br>:star:[code](https://github.com/allenhaozhu/EASE)
  * [Semi-Supervised Few-Shot Learning via Multi-Factor Clustering](https://openaccess.thecvf.com/content/CVPR2022/papers/Ling_Semi-Supervised_Few-Shot_Learning_via_Multi-Factor_Clustering_CVPR_2022_paper.pdf)<br>:star:[code](https://gitlab.com/smartllvlab/cluster-fsl)
  * [Cross-Domain Few-Shot Learning With Task-Specific Adapters](https://arxiv.org/abs/2107.00358)<br>:star:[code](https://github.com/VICO-UoE/URL)
* 零样本
  * [MSDN: Mutually Semantic Distillation Network for Zero-Shot Learning](https://arxiv.org/abs/2203.03137)<br>:star:[code](https://github.com/shiming-chen/MSDN):newspaper:[粗解](https://zhuanlan.zhihu.com/p/477624433)
  * [Unseen Classes at a Later Time? No Problem](https://arxiv.org/abs/2203.16517)<br>:star:[code](https://github.com/sumitramalagi/Unseen-classes-at-a-later-time)
  * [En-Compactness: Self-Distillation Embedding & Contrastive Generation for Generalized Zero-Shot Learning](https://openaccess.thecvf.com/content/CVPR2022/papers/Kong_En-Compactness_Self-Distillation_Embedding__Contrastive_Generation_for_Generalized_Zero-Shot_Learning_CVPR_2022_paper.pdf)<br>:newspaper:[解读](https://mp.weixin.qq.com/s/FYn0S46OA6xraH9ofNIctw)
  * [Non-Generative Generalized Zero-Shot Learning via Task-Correlated Disentanglement and Controllable Samples Synthesis](https://arxiv.org/abs/2203.05335)
  * [Siamese Contrastive Embedding Network for Compositional Zero-Shot Learning](https://openaccess.thecvf.com/content/CVPR2022/papers/Li_Siamese_Contrastive_Embedding_Network_for_Compositional_Zero-Shot_Learning_CVPR_2022_paper.pdf)<br>:star:[code](https://github.com/XDUxyLi/SCEN-master)
  * [KG-SP: Knowledge Guided Simple Primitives for Open World Compositional Zero-Shot Learning](https://arxiv.org/abs/2205.06784)<br>:star:[code](https://github.com/ExplainableML/KG-SP)<br>:newspaper:[解读](https://zhuanlan.zhihu.com/p/515190727)
  * [Uni-Perceiver: Pre-Training Unified Architecture for Generic Perception for Zero-Shot and Few-Shot Tasks](https://openaccess.thecvf.com/content/CVPR2022/papers/Zhu_Uni-Perceiver_Pre-Training_Unified_Architecture_for_Generic_Perception_for_Zero-Shot_and_CVPR_2022_paper.pdf)
  * [Distinguishing Unseen From Seen for Generalized Zero-Shot Learning](https://openaccess.thecvf.com/content/CVPR2022/papers/Su_Distinguishing_Unseen_From_Seen_for_Generalized_Zero-Shot_Learning_CVPR_2022_paper.pdf)
  * [VGSE: Visually-Grounded Semantic Embeddings for Zero-Shot Learning](https://arxiv.org/abs/2203.10444)<br>:star:[code](https://github.com/wenjiaXu/VGSE)<br>:newspaper:[零样本学习，大幅减少人工标注！马普所和北邮提出富含视觉信息的类别语义嵌入](https://mp.weixin.qq.com/s/VODFh-oW27w2DCnlgLRLBA)
  * [Audio-Visual Generalised Zero-Shot Learning With Cross-Modal Attention and Language](https://arxiv.org/abs/2203.03598)<br>:star:[code](https://github.com/ExplainableML/AVCA-GZSL)
* 域泛化
  * [Compound Domain Generalization via Meta-Knowledge Encoding](https://arxiv.org/pdf/2203.13006.pdf)
  * [Causality Inspired Representation Learning for Domain Generalization](https://arxiv.org/abs/2203.14237)<br>:star:[code](https://github.com/BIT-DA/CIRL)
  * [Towards Unsupervised Domain Generalization](https://arxiv.org/abs/2107.06219)<br>:newspaper:[CVPR 2022丨清华大学提出：无监督域泛化 (UDG)](https://mp.weixin.qq.com/s/ifC6GI5oVpE4ncttmJipjQ)<br>本次任务的主要目标是域泛化（domain generalization(DG)），是首篇将DG推广到unsupervised learning 领域的，并提出一个新的研究领域 unsupervised domain generalization(UDG)。
  * [Towards Principled Disentanglement for Domain Generalization](https://arxiv.org/abs/2111.13839)<br>:open_mouth:oral:star:[code](https://github.com/hlzhang109/DDG)
  * [Meta Convolutional Neural Networks for Single Domain Generalization](https://openaccess.thecvf.com/content/CVPR2022/papers/Wan_Meta_Convolutional_Neural_Networks_for_Single_Domain_Generalization_CVPR_2022_paper.pdf)
  * [PCL: Proxy-Based Contrastive Learning for Domain Generalization](https://openaccess.thecvf.com/content/CVPR2022/papers/Yao_PCL_Proxy-Based_Contrastive_Learning_for_Domain_Generalization_CVPR_2022_paper.pdf)
  * [Localized Adversarial Domain Generalization](https://arxiv.org/abs/2205.04114)
  * [Unsupervised Domain Generalization by Learning a Bridge Across Domains](https://arxiv.org/abs/2112.02300)
  * [Style Neophile: Constantly Seeking Novel Styles for Domain Generalization](https://openaccess.thecvf.com/content/CVPR2022/papers/Kang_Style_Neophile_Constantly_Seeking_Novel_Styles_for_Domain_Generalization_CVPR_2022_paper.pdf)<br>
  * 域外泛化
    * [The Two Dimensions of Worst-case Training and the Integrated Effect for Out-of-domain Generalization](https://arxiv.org/abs/2204.04384)
* 域适应
  * [Continual Test-Time Domain Adaptation](https://arxiv./abs/2203.13591)<br>:star:[code](https://github.com/qinenergy/cotta)
  * [Safe Self-Refinement for Transformer-based Domain Adaptation](https://arxiv.org/abs/2204.07683)<br>:star:[code](https://github.com/tsun/SSRT):newspaper:[解读](https://zhuanlan.zhihu.com/p/501027339)
  * [Source-Free Domain Adaptation via Distribution Estimation](https://arxiv.org/abs/2204.11257)<br>:newspaper:[解读](https://mp.weixin.qq.com/s/lSY1is6Fmm6A0Db0Jxo4qg)
  * [Learning Distinctive Margin toward Active Domain Adaptation](https://arxiv.org/abs/2203.05738)<br>:star:[code](https://github.com/TencentYoutuResearch/ActiveLearning-SDM)<br>:newspaper:[解读](https://mp.weixin.qq.com/s/FYn0S46OA6xraH9ofNIctw)
  * [DINE: Domain Adaptation from Single and Multiple Black-box Predictors](https://arxiv.org/abs/2104.01539)<br>:star:[code](https://github.com/tim-learn/DINE/)
  * [Exploring Domain-Invariant Parameters for Source Free Domain Adaptation](https://openaccess.thecvf.com/content/CVPR2022/papers/Wang_Exploring_Domain-Invariant_Parameters_for_Source_Free_Domain_Adaptation_CVPR_2022_paper.pdf)
  * [Physically Disentangled Intra- and Inter-Domain Adaptation for Varicolored Haze Removal](https://openaccess.thecvf.com/content/CVPR2022/papers/Li_Physically_Disentangled_Intra-_and_Inter-Domain_Adaptation_for_Varicolored_Haze_Removal_CVPR_2022_paper.pdf)<br>:star:[code](https://github.com/HuaYuuu/PDI2A-CVPR2022)
 * [No-Reference Point Cloud Quality Assessment via Domain Adaptation](https://arxiv.org/abs/2112.02851)<br>:star:[code](https://github.com/Qi-Yangsjtu/IT-PCQA)
 * [Slimmable Domain Adaptation](https://openaccess.thecvf.com/content/CVPR2022/papers/Meng_Slimmable_Domain_Adaptation_CVPR_2022_paper.pdf)<br>:star:[code](https://github.com/HIK-LAB/SlimDA)
 * 无监督域适应
    * [Reusing the Task-specific Classifier as a Discriminator: Discriminator-free Adversarial Domain Adaptation](https://arxiv.org/abs/2204.03838)<br>:star:[code](https://github.com/xiaoachen98/DALN)
    * [Category Contrast for Unsupervised Domain Adaptation in Visual Tasks](https://arxiv.org/abs/2106.02885)
    * [The Norm Must Go On: Dynamic Unsupervised Domain Adaptation by Normalization](https://arxiv.org/abs/2112.00463)<br>:star:[code](https://github.com/jmiemirza/DUA)
    * [Spectral Unsupervised Domain Adaptation for Visual Recognition](http://arxiv.org/abs/2204.13983)
    
<a name="46"/>

## 46.Scene Graph Generation(场景图生成)
* [PPDL: Predicate Probability Distribution Based Loss for Unbiased Scene Graph Generation](https://openaccess.thecvf.com/content/CVPR2022/papers/Li_PPDL_Predicate_Probability_Distribution_Based_Loss_for_Unbiased_Scene_Graph_CVPR_2022_paper.pdf)
* [Fine-Grained Predicates Learning for Scene Graph Generation](https://arxiv.org/abs/2204.02597)<br>:star:[code](https://github.com/XinyuLyu/FGPL)
* [HL-Net: Heterophily Learning Network for Scene Graph Generatio](https://arxiv.org/abs/2205.01316)<br>:star:[code](https://github.com/siml3/HL-Net)<br>场景图生成：异质学习网络<br>:newspaper:[解读](https://mp.weixin.qq.com/s/lSY1is6Fmm6A0Db0Jxo4qg)
* [RU-Net: Regularized Unrolling Network for Scene Graph Generation](https://arxiv.org/abs/2205.01297)<br>:star:[code](https://github.com/siml3/RU-Net)<br>场景图生成：正则展开网络<br>:newspaper:[解读](https://mp.weixin.qq.com/s/lSY1is6Fmm6A0Db0Jxo4qg)
* [The Devil is in the Labels: Noisy Label Correction for Robust Scene Graph Generation](https://arxiv.org/abs/2206.03014)<br>:star:[code](https://github.com/muktilin/NICE)
* [Dynamic Scene Graph Generation via Anticipatory Pre-Training](https://openaccess.thecvf.com/content/CVPR2022/papers/Li_Dynamic_Scene_Graph_Generation_via_Anticipatory_Pre-Training_CVPR_2022_paper.pdf)
* [Stacked Hybrid-Attention and Group Collaborative Learning for Unbiased Scene Graph Generation](https://arxiv.org/abs/2203.09811)<br>:star:[code](https://github.com/dongxingning/SHA-GCL-for-SGG)
* 视频场景图生成
  * [Classification-Then-Grounding: Reformulating Video Scene Graphs As Temporal Bipartite Graphs](https://openaccess.thecvf.com/content/CVPR2022/papers/Gao_Classification-Then-Grounding_Reformulating_Video_Scene_Graphs_As_Temporal_Bipartite_Graphs_CVPR_2022_paper.pdf)<br>:star:[code](https://github.com/Dawn-LX/VidSGG-BIG)

<a name="45"/>

## 45.Dense Prediction(密集预测)
* [Does Robustness on ImageNet Transfer to Downstream Tasks?](https://arxiv.org/abs/2204.03934)

<a name="44"/>

## 44.Federated Learning(联邦学习)
* [CD2-pFed: Cyclic Distillation-guided Channel Decoupling for Model Personalization in Federated Learning](https://arxiv.org/abs/2204.03880)
* [Auditing Privacy Defenses in Federated Learning via Generative Gradient Leakage](https://arxiv.org/pdf/2203.15696.pdf)<br>:star:[code](https://github.com/zhuohangli/GGL)
* [FedCorr: Multi-Stage Federated Learning for Label Noise Correction](https://arxiv.org/abs/2204.04677)<br>:star:[code](https://github.com/Xu-Jingyi/FedCorr)
* [Fine-tuning Global Model via Data-Free Knowledge Distillation for Non-IID Federated Learning](https://arxiv.org/abs/2203.09249)
* [Rethinking Architecture Design for Tackling Data Heterogeneity in Federated Learning](https://arxiv.org/abs/2106.06047)
* [Layer-Wised Model Aggregation for Personalized Federated Learning](http://arxiv.org/abs/2205.03993)
* [Federated Learning With Position-Aware Neurons](https://arxiv.org/abs/2203.14666)
* [Local Learning Matters: Rethinking Data Heterogeneity in Federated Learning](https://arxiv.org/abs/2111.14213)<br>:star:[code](https://github.com/mmendiet/FedAlign)
* [FedDC: Federated Learning With Non-IID Data via Local Drift Decoupling and Correction](https://arxiv.org/abs/2203.11751)<br>:star:[code](https://github.com/gaoliang13/FedDC)
* [Learn From Others and Be Yourself in Heterogeneous Federated Learning](https://openaccess.thecvf.com/content/CVPR2022/papers/Huang_Learn_From_Others_and_Be_Yourself_in_Heterogeneous_Federated_Learning_CVPR_2022_paper.pdf)<br>:star:[code](https://github.com/WenkeHuang/FCCL)

<a name="43"/>

## 43.Multi-Task Learning(多任务学习)
* [Controllable Dynamic Multi-Task Architectures](https://arxiv.org/abs/2203.14949)<br>:house:[project](https://www.nec-labs.com/~mas/DYMU/)
* [Task Adaptive Parameter Sharing for Multi-Task Learning](https://arxiv.org/abs/2203.16708)

<a name="42"/>

## 42.Metric Learning(度量学习)
* [Self-Taught Metric Learning without Labels](https://arxiv.org/abs/2205.01903)<br>:star:[code](https://github.com/tjddus9597/STML-CVPR22):house:[project](http://cvlab.postech.ac.kr/research/STML/)
* [Enhancing Adversarial Robustness for Deep Metric Learning](https://arxiv.org/abs/2203.01439)

<a name="41"/>

## 41.Incremental Learning(增量学习)
* 增量学习
  * [Energy-based Latent Aligner for Incremental Learning](https://arxiv.org/abs/2203.14952)<br>:star:[code](https://github.com/JosephKJ/ELI)
  * [General Incremental Learning with Domain-aware Categorical Representations](https://arxiv.org/abs/2204.04078)
  * [Forward Compatible Few-Shot Class-Incremental Learning](https://arxiv.org/abs/2203.06953)<br>:star:[code](https://github.com/zhoudw-zdw/CVPR22-Fact)
  * [Mimicking the Oracle: An Initial Phase Decorrelation Approach for Class Incremental Learning](https://arxiv.org/abs/2112.04731)<br>:star:[code](https://github.com/Yujun-Shi/CwD)
* 类增量学习
  * [Doodle It Yourself: Class Incremental Learning by Drawing a Few Sketches](https://arxiv.org/abs/2203.14843)
  * [Constrained Few-shot Class-incremental Learning](https://arxiv.org/abs/2203.16588)<br>:star:[code](https://github.com/IBM/constrained-FSCIL)
  * [Class-Incremental Learning with Strong Pre-trained Models](https://arxiv.org/abs/2204.03634)
  * [Class-Incremental Learning by Knowledge Distillation With Adaptive Feature Consolidation](https://arxiv.org/abs/2204.00895)<br>:star:[code](https://github.com/kminsoo/AFC)
  * [Bring Evanescent Representations to Life in Lifelong Class Incremental Learning](https://openaccess.thecvf.com/content/CVPR2022/papers/Toldo_Bring_Evanescent_Representations_to_Life_in_Lifelong_Class_Incremental_Learning_CVPR_2022_paper.pdf)
  * [Self-Sustaining Representation Expansion for Non-Exemplar Class-Incremental Learning](https://arxiv.org/abs/2203.06359)
  * [MetaFSCIL: A Meta-Learning Approach for Few-Shot Class Incremental Learning](https://openaccess.thecvf.com/content/CVPR2022/papers/Chi_MetaFSCIL_A_Meta-Learning_Approach_for_Few-Shot_Class_Incremental_Learning_CVPR_2022_paper.pdf)

<a name="40"/>

## 40.Adversarial Learning(对抗学习)
* [Give Me Your Attention: Dot-Product Attention Considered Harmful for Adversarial Patch Robustness](https://arxiv.org/abs/2203.13639)
* [Masking Adversarial Damage: Finding Adversarial Saliency for Robust and Sparse Network](https://arxiv.org/abs/2204.02738)
* [Towards Practical Certifiable Patch Defense with Vision Transformer](https://arxiv.org/abs/2203.08519)<br>:newspaper:[解读](https://mp.weixin.qq.com/s/FYn0S46OA6xraH9ofNIctw)
* [Enhancing Adversarial Training with Second-Order Statistics of Weights](https://arxiv.org/abs/2203.06020)<br>:star:[code](https://github.com/Alexkael/S2O)
* [Practical Evaluation of Adversarial Robustness via Adaptive Auto Attack](https://arxiv.org/abs/2203.05154)<br>:star:[code](https://github.com/liuye6666/adaptive_auto_attack)
* [Improving Adversarial Transferability via Neuron Attribution-Based Attacks](https://arxiv.org/abs/2204.00008)<br>:star:[code](https://github.com/jpzhang1810/NAA)
* [Two Coupled Rejection Metrics Can Tell Adversarial Examples Apart](https://arxiv.org/abs/2105.14785)<br>:star:[code](https://github.com/P2333/Rectified-Rejection)
* [Bounded Adversarial Attack on Deep Content Features](https://openaccess.thecvf.com/content/CVPR2022/papers/Xu_Bounded_Adversarial_Attack_on_Deep_Content_Features_CVPR_2022_paper.pdf)
* [Subspace Adversarial Training](https://arxiv.org/abs/2111.12229)<br>:star:[code](https://github.com/nblt/Sub-AT)
* [Cross-Modal Transferable Adversarial Attacks From Images to Videos](https://arxiv.org/abs/2112.05379)<br>:star:[code](https://github.com/zhipeng-wei/Image-to-Video-I2V-attack)
* [Understanding and Increasing Efficiency of Frank-Wolfe Adversarial Training](https://arxiv.org/abs/2012.12368)<br>:star:[code](https://github.com/TheoT1/FW-AT-Adapt)
* [Quarantine: Sparsity Can Uncover the Trojan Attack Trigger for Free](https://arxiv.org/abs/2205.11819)<br>:star:[code](https://github.com/VITA-Group/Backdoor-LTH)
* [Robust Combination of Distributed Gradients Under Adversarial Perturbations](https://openaccess.thecvf.com/content/CVPR2022/papers/Kim_Robust_Combination_of_Distributed_Gradients_Under_Adversarial_Perturbations_CVPR_2022_paper.pdf)
* [Adversarial Texture for Fooling Person Detectors in the Physical World](https://arxiv.org/abs/2203.03373)
* [DTA: Physical Camouflage Attacks Using Differentiable Transformation Network](https://arxiv.org/abs/2203.09831)<br>:house:[project](https://islab-ai.github.io/dta-cvpr2022/)
* [BppAttack: Stealthy and Efficient Trojan Attacks Against Deep Neural Networks via Image Quantization and Contrastive Adversarial Learning](https://arxiv.org/abs/2205.13383)<br>:star:[code](https://github.com/RU-System-Software-and-Security/BppAttack)
* [Pyramid Adversarial Training Improves ViT Performance](https://arxiv.org/abs/2111.15121)<br>:house:[project](https://pyramidat.github.io/)
* 对抗样本  
  * [Label-Only Model Inversion Attacks via Boundary Repulsion](https://arxiv.org/abs/2203.01925)
  * [Self-supervised Learning of Adversarial Example: Towards Good Generalizations for Deepfake Detection](https://arxiv.org/abs/2203.1220)<br>:star:[code](https://github.com/liangchen527/SLADD8)
  * [Improving the Transferability of Targeted Adversarial Examples Through Object-Based Diverse Input](https://arxiv.org/abs/2203.09123)<br>:star:[code](https://github.com/dreamflake/ODI)
* 对抗攻击
  * [Shadows can be Dangerous: Stealthy and Effective Physical-world Adversarial Attack by Natural Phenomenon](https://arxiv.org/abs/2203.03818)
  * [Transferable Sparse Adversarial Attack](https://arxiv.org/abs/2105.14727)<br>:star:[code](https://github.com/shaguopohuaizhe/TSAA)
  * [Towards Efficient Data Free Black-Box Adversarial Attack](https://openaccess.thecvf.com/content/CVPR2022/papers/Zhang_Towards_Efficient_Data_Free_Black-Box_Adversarial_Attack_CVPR_2022_paper.pdf)
* 黑盒
  * [Investigating Top-k White-Box and Transferable Black-box Attack](https://arxiv.org/abs/2204.00089)<br>:star:[code](https://github.com/ChaoningZhang/Top-k-Transferable-Attack)
  * [DST: Dynamic Substitute Training for Data-free Black-box Attack](https://arxiv.org/abs/2204.00972)<br>:house:[project](https://wxwangiris.github.io/DST)
  * [Bandits for Structure Perturbation-based Black-box Attacks to Graph Neural Networks with Theoretical Guarantees](https://arxiv.org/abs/2205.03546)<br>:open_mouth:oral:star:[code](https://github.com/Metaoblivion/Bandit_GNN_Attack)
* 对抗训练
  * [LAS-AT: Adversarial Training with Learnable Attack Strategy](https://arxiv.org/pdf/2203.06616.pdf)<br>:open_mouth:oral:star:[code](https://github.com/jiaxiaojunQAQ/LAS-AT)<br>:newspaper:[CVPR 2022 中科院、腾讯提出LAS-AT，利用“可学习攻击策略”进行“对抗训练”](https://mp.weixin.qq.com/s/Aj9x61LY8tJICf8hUlz8ug)
 
<a name="39"/>

## 39.Continual Learning(持续学习)
* [On Generalizing Beyond Domains in Cross-Domain Continual Learning](https://arxiv.org/abs/2203.03970)
* [Probing Representation Forgetting in Supervised and Unsupervised Continual Learning](https://arxiv.org/abs/2203.13381)
* [Online Continual Learning on a Contaminated Data Stream with Blurry Task Boundaries](https://arxiv.org/abs/2203.15355)<br>:star:[code](https://github.com/clovaai/puridiver)
* [Learning To Prompt for Continual Learning](https://arxiv.org/abs/2112.08654)<br>:star:[code](https://github.com/google-research/l2p)
* [Learning Bayesian Sparse Networks With Full Experience Replay for Continual Learning](https://arxiv.org/abs/2202.10203)
* [Not Just Selection, but Exploration: Online Class-Incremental Continual Learning via Dual View Consistency](https://openaccess.thecvf.com/content/CVPR2022/papers/Gu_Not_Just_Selection_but_Exploration_Online_Class-Incremental_Continual_Learning_via_CVPR_2022_paper.pdf)<br>:star:[code](https://github.com/YananGu/DVC)
* [Continual Learning for Visual Search With Backward Consistent Feature Embedding](https://arxiv.org/abs/2205.13384)<br>:star:[code](https://github.com/ivclab/CVS)
* [Meta-Attention for ViT-Backed Continual Learning](https://arxiv.org/abs/2203.11684)<br>:star:[code](https://github.com/zju-vipa/MEAT-TIL)
* [Continual Learning with Lifelong Vision Transformer](https://openaccess.thecvf.com/content/CVPR2022/papers/Wang_Continual_Learning_With_Lifelong_Vision_Transformer_CVPR_2022_paper.pdf)<br>:newspaper:[解读](https://mp.weixin.qq.com/s/lSY1is6Fmm6A0Db0Jxo4qg)

<a name="38"/>

## 38.Meta-Learning(元学习)
* [What Matters For Meta-Learning Vision Regression Tasks?](https://arxiv.org/abs/2203.04905)
* [Multidimensional Belief Quantification for Label-Efficient Meta-Learning](https://arxiv.org/abs/2203.12768) 
* [Dynamic Kernel Selection for Improved Generalization and Memory Efficiency in Meta-learning](https://arxiv.org/abs/2206.01690)
* [Learning to Learn and Remember Super Long Multi-Domain Task Sequence](https://openaccess.thecvf.com/content/CVPR2022/papers/Wang_Learning_To_Learn_and_Remember_Super_Long_Multi-Domain_Task_Sequence_CVPR_2022_paper.pdf)<br>:open_mouth:oral:star:[code](https://github.com/joey-wang123/SDML)<br>:newspaper:[解读](https://mp.weixin.qq.com/s/lSY1is6Fmm6A0Db0Jxo4qg)

<a name="37"/>

## 37.Contrastive Learning(对比学习)
* [Selective-Supervised Contrastive Learning with Noisy Labels](https://arxiv.org/abs/2203.04181)<br>:star:[code](https://github.com/ShikunLi/Sel-CL):newspaper:[粗解](https://zhuanlan.zhihu.com/p/478070143)
* [Frame-wise Action Representations for Long Videos via Sequence Contrastive Learning](https://arxiv.org/abs/2203.14957)<br>:star:[code](https://github.com/minghchen/CARL_code)
* [Cam-Ready: UNICON: Combating Label Noise Through Uniform Selection and Contrastive Learning](https://arxiv.org/abs/2203.14542)<br>:star:[code](https://github.com/nazmul-karim170/UNICON-Noisy-Labe)
* [Use All The Labels: A Hierarchical Multi-Label Contrastive Learning Framework](https://arxiv.org/abs/2204.13207)<br>:star:[code](https://github.com/salesforce/hierarchicalContrastiveLearning)
* [Crafting Better Contrastive Views for Siamese Representation Learning](https://arxiv.org/abs/2202.03278)<br>:open_mouth:oral:star:[code](https://github.com/xyupeng/ContrastiveCrop)
* [Dual Temperature Helps Contrastive Learning Without Many Negative Samples: Towards Understanding and Simplifying MoCo](https://arxiv.org/abs/2203.17248)<br>:star:[code](https://github.com/ChaoningZhang/Dual-temperature)
* [Estimating Fine-Grained Noise Model via Contrastive Learning](https://arxiv.org/abs/2204.01716)
* [Contextual Outpainting With Object-Level Contrastive Learning](https://openaccess.thecvf.com/content/CVPR2022/papers/Li_Contextual_Outpainting_With_Object-Level_Contrastive_Learning_CVPR_2022_paper.pdf)<br>:house:[project](https://ddlee-cn.github.io/cto-gan/)
* [Rethinking the Augmentation Module in Contrastive Learning: Learning Hierarchical Augmentation Invariance With Expanded Views](https://arxiv.org/abs/2206.00227)
* [Contrastive Dual Gating: Learning Sparse Features With Contrastive Learning](https://openaccess.thecvf.com/content/CVPR2022/papers/Meng_Contrastive_Dual_Gating_Learning_Sparse_Features_With_Contrastive_Learning_CVPR_2022_paper.pdf)
* [Noise Is Also Useful: Negative Correlation-Steered Latent Contrastive Learning](https://openaccess.thecvf.com/content/CVPR2022/papers/Yan_Noise_Is_Also_Useful_Negative_Correlation-Steered_Latent_Contrastive_Learning_CVPR_2022_paper.pdf)
* [On Learning Contrastive Representations for Learning With Noisy Labels](https://arxiv.org/abs/2203.01785)
* [Unsupervised Deraining: Where Contrastive Learning Meets Self-Similarity](https://arxiv.org/abs/2203.11509)
* [Robust Contrastive Learning Against Noisy Views](https://arxiv.org/abs/2201.04309)<br>:star:[code](https://github.com/chingyaoc/RINCE)
* [Unified Contrastive Learning in Image-Text-Label Space](https://arxiv.org/abs/2204.03610)<br>:star:[code](https://github.com/microsoft/UniCL)
* [Consistent Explanations by Contrastive Learning](https://arxiv.org/abs/2110.00527)<br>:star:[code](https://github.com/UCDvision/CGC)


<a name="36"/>

## 36.Optical Flow(光流估计)
* [CRAFT: Cross-Attentional Flow Transformer for Robust Optical Flow](https://arxiv.org/abs/2203.16896)<br>:star:[code](https://github.com/askerlee/craft)
* [DIP: Deep Inverse Patchmatch for High-Resolution Optical Flow](https://arxiv.org/abs/2204.00330)<br>:star:[code](https://github.com/zihuazheng/DIP)
* [Imposing Consistency for Optical Flow Estimation](https://arxiv.org/abs/2204.07262)
* [Deep Equilibrium Optical Flow Estimation](https://arxiv.org/abs/2204.08442)<br>:star:[code](https://github.com/locuslab/deq-flow):newspaper:[解读](https://zhuanlan.zhihu.com/p/501027339)
* [GMFlow: Learning Optical Flow via Global Matching](https://arxiv.org/abs/2111.13680)<br>:open_mouth:oral:star:[code](https://github.com/haofeixu/gmflow):newspaper:[解读](https://mp.weixin.qq.com/s/lSY1is6Fmm6A0Db0Jxo4qg)
* [Optical Flow Estimation for Spiking Camera](https://arxiv.org/abs/2110.03916)<br>:star:[code](https://github.com/Acnext/Optical-Flow-For-Spiking-Camera)
* [Learning Optical Flow with Kernel Patch Attention](https://openaccess.thecvf.com/content/CVPR2022/papers/Luo_Learning_Optical_Flow_With_Kernel_Patch_Attention_CVPR_2022_paper.pdf)<br>:star:[code](https://github.com/megvii-research/KPAFlow):newspaper:[解读](https://mp.weixin.qq.com/s/kZue3ds348UXQI86xrwudQ)
* [CamLiFlow: Bidirectional Camera-LiDAR Fusion for Joint Optical Flow and Scene Flow Estimation](https://arxiv.org/abs/2111.10502)<br>:star:[code](https://github.com/MCG-NJU/CamLiFlow)
* [Global Matching With Overlapping Attention for Optical Flow Estimation](https://arxiv.org/abs/2203.11335)<br>:star:[code](https://github.com/xiaofeng94/GMFlowNet)

<a name="35"/>

## 35.OCR
* [XYLayoutLM: Towards Layout-Aware Multimodal Networks for Visually-Rich Document Understanding](https://arxiv.org/abs/2203.06947)
* [SwinTextSpotter: Scene Text Spotting via Better Synergy Between Text Detection and Text Recognition](https://arxiv.org/abs/2203.10209)<br>:star:[code](https://github.com/mxin262/SwinTextSpotter)
* 场景文本检测
  * [Towards End-to-End Unified Scene Text Detection and Layout Analysis](https://arxiv.org/abs/2203.15143)<br>:star:[code](https://github.com/google-research-datasets/hiertext)
  * [Pushing the Performance Limit of Scene Text Recognizer without Human Annotation](https://arxiv.org/abs/2204.07714)
  * [Vision-Language Pre-Training for Boosting Scene Text Detectors](https://arxiv.org/abs/2204.13867)<br>:star:[code](https://github.com/CVI-SZU/STKM)<br>视觉语言预训练，场景文本检测,代码将开源，地址尚未公布。
  * [Few Could Be Better Than All: Feature Sampling and Grouping for Scene Text Detection](https://arxiv.org/abs/2203.15221)
* 场景文本识别
  * [SimAN: Exploring Self-Supervised Representation Learning of Scene Text via Similarity-Aware Normalization](https://arxiv.org/abs/2203.10492)<br>:star:[code](https://github.com/Canjie-Luo/Real-300K)  
* Text Spotting
  * [Text Spotting Transformers](https://arxiv.org/abs/2204.01918)<br>:star:[code](https://github.com/mlpc-ucsd/TESTR):newspaper:[粗解](https://zhuanlan.zhihu.com/p/493615566)
* LOGO设计
  * [Aesthetic Text Logo Synthesis via Content-aware Layout Inferring](https://arxiv.org/abs/2204.02701)<br>:star:[code](https://github.com/yizhiwang96/TextLogoLayout)<br>:newspaper:[CVPR 2022 | 北大、腾讯提出文字logo生成模型，脑洞大开堪比设计师](https://mp.weixin.qq.com/s/gjrdktxwbTDPWWeIK5wVNQ)
* 字体生成
  * [XMP-Font: Self-Supervised Cross-Modality Pre-training for Few-Shot Font Generation](https://arxiv.org/abs/2204.05084) 
  * [(Oral)Look Closer to Supervise Better: One-Shot Font Generation via Component-Based Discriminator](https://arxiv.org/abs/2205.00146)<br>字体生成（很有商业价值的方向）
  * [Few-Shot Font Generation by Learning Fine-Grained Local Styles](https://arxiv.org/abs/2205.09965)
* 文本识别
  * [Open-set Text Recognition via Character-Context Decoupling](https://arxiv.org/abs/2204.05535)
* 表格结构识别
  * [Neural Collaborative Graph Machines for Table Structure Recognition](https://arxiv.org/abs/2111.13359)<br>:newspaper:[解读](https://mp.weixin.qq.com/s/FYn0S46OA6xraH9ofNIctw)
* 文本美观预测评估
  * [Does Text Attract Attention on E-Commerce Images: A Novel Saliency Prediction Dataset and Method](https://openaccess.thecvf.com/content/CVPR2022/papers/Jiang_Does_Text_Attract_Attention_on_E-Commerce_Images_A_Novel_Saliency_CVPR_2022_paper.pdf)<br>:star:[code](https://github.com/leafy-lee/E-commercial-dataset)
* 表结构理解
  * [TableFormer: Table Structure Understanding with Transformers](https://arxiv.org/abs/2203.01017)
* 文本分割
  * [BTS: A Bi-Lingual Benchmark for Text Segmentation in the Wild](https://openaccess.thecvf.com/content/CVPR2022/papers/Xu_BTS_A_Bi-Lingual_Benchmark_for_Text_Segmentation_in_the_Wild_CVPR_2022_paper.pdf)
* 表格检测
  * [PubTables-1M: Towards Comprehensive Table Extraction From Unstructured Documents](https://openaccess.thecvf.com/content/CVPR2022/papers/Smock_PubTables-1M_Towards_Comprehensive_Table_Extraction_From_Unstructured_Documents_CVPR_2022_paper.pdf)<br>:star:[code](https://github.com/microsoft/table-transformer)
* 文本修复
  * [Fourier Document Restoration for Robust Document Dewarping and Recognition](https://arxiv.org/abs/2203.09910)<br>:house:[project](https://sg-vilab.github.io/event/warpdoc/)

<a name="34"/>

## 34.Model Compression/Knowledge Distillation/Pruning(模型压缩/知识蒸馏/剪枝)
* 知识蒸馏
  * [Knowledge Distillation with the Reused Teacher Classifier](https://arxiv.org/abs/2203.14001)
  * [DearKD: Data-Efficient Early Knowledge Distillation for Vision Transformers](https://arxiv.org/abs/2204.12997)<br>:newspaper:[解读](https://mp.weixin.qq.com/s/lSY1is6Fmm6A0Db0Jxo4qg)
  * [Decoupled Knowledge Distillation](https://arxiv.org/abs/2203.08679)<br>:star:[code](https://github.com/megvii-research/mdistiller)<br>:newspaper:[解耦知识蒸馏，让Hinton在7年前提出的方法重回SOTA行列](https://mp.weixin.qq.com/s/ozLLnUf8KggVzbPxeegQ3g)
  * [Knowledge Distillation via the Target-aware Transformer](https://arxiv.org/abs/2205.10793)<br>:open_mouth:oral:star:[code](https://github.com/sihaoevery/TaT)<br>:newspaper:[RMIT&阿里&UTS&中山提出Target-aware Transformer，进行one-to-all知识蒸馏！性能SOTA](https://mp.weixin.qq.com/s/hz8julfb0ahYeT8kxGvS9w)
  * [Evaluation-oriented Knowledge Distillation for Deep Face Recognition](https://arxiv.org/abs/2206.02325)<br>:open_mouth:oral:star:[code](https://github.com/Tencent/TFace/tree/master/recognition/tasks/ekd)<br>:newspaper:[解读1](https://zhuanlan.zhihu.com/p/525331776)<br>:newspaper:[解读2](https://mp.weixin.qq.com/s/FYn0S46OA6xraH9ofNIctw)
  * [Open-Vocabulary One-Stage Detection With Hierarchical Visual-Language Knowledge Distillation](https://arxiv.org/abs/2203.10593)<br>:star:[code](https://github.com/mengqiDyangge/HierKD)
  * [Self-Distillation From the Last Mini-Batch for Consistency Regularization](https://arxiv.org/abs/2203.16172)<br>:star:[code](https://github.com/Meta-knowledge-Lab/DLB)
  * [Knowledge Distillation As Efficient Pre-Training: Faster Convergence, Higher Data-Efficiency, and Better Transferability](https://openaccess.thecvf.com/content/CVPR2022/papers/He_Knowledge_Distillation_As_Efficient_Pre-Training_Faster_Convergence_Higher_Data-Efficiency_and_CVPR_2022_paper.pdf)<br>:star:[code](https://github.com/CVMI-Lab/KDEP)
* 模型压缩
  * [CHEX: CHannel EXploration for CNN Model Compression](https://arxiv.org/abs/2203.15794)
  * [DiSparse: Disentangled Sparsification for Multitask Model Compression](https://openaccess.thecvf.com/content/CVPR2022/papers/Sun_DiSparse_Disentangled_Sparsification_for_Multitask_Model_Compression_CVPR_2022_paper.pdf)<br>:star:[code](https://github.com/SHI-Labs/DiSparse-Multitask-Model-Compression) 
* 剪枝
  * [Revisiting Random Channel Pruning for Neural Network Compression](https://arxiv.org/abs/2205.05676)<br>:star:[code](https://github.com/ofsoundof/random_channel_pruning)<br>:newspaper:[解读](https://zhuanlan.zhihu.com/p/513130382)
  * [Fire Together Wire Together: A Dynamic Pruning Approach With Self-Supervised Mask Prediction](https://arxiv.org/abs/2110.08232)
* 量化
  * [A Deeper Dive Into What Deep Spatiotemporal Networks Encode: Quantifying Static vs. Dynamic Information](https://arxiv.org/abs/2206.02846)<br>:star:[code](https://github.com/YorkUCVIL/Static-Dynamic-Interpretability/):house:[project](https://yorkucvil.github.io/Static-Dynamic-Interpretability/)
  * [Mr.BiQ: Post-Training Non-Uniform Quantization Based on Minimizing the Reconstruction Error](https://openaccess.thecvf.com/content/CVPR2022/papers/Jeon_Mr.BiQ_Post-Training_Non-Uniform_Quantization_Based_on_Minimizing_the_Reconstruction_Error_CVPR_2022_paper.pdf)
  * [Nonuniform-to-Uniform Quantization: Towards Accurate Quantization via Generalized Straight-Through Estimation](https://arxiv.org/abs/2111.14826)<br>:star:[code](https://github.com/liuzechun/Nonuniform-to-Uniform-Quantization)
  * [AlignQ: Alignment Quantization With ADMM-Based Correlation Preservation](https://openaccess.thecvf.com/content/CVPR2022/papers/Chen_AlignQ_Alignment_Quantization_With_ADMM-Based_Correlation_Preservation_CVPR_2022_paper.pdf)<br>:star:[code](https://github.com/tinganchen/AlignQ)
  * [Data-Free Network Compression via Parametric Non-Uniform Mixed Precision Quantization](https://openaccess.thecvf.com/content/CVPR2022/papers/Chikin_Data-Free_Network_Compression_via_Parametric_Non-Uniform_Mixed_Precision_Quantization_CVPR_2022_paper.pdf)
* 超参数优化
  * [AME: Attention and Memory Enhancement in Hyper-Parameter Optimization](https://openaccess.thecvf.com/content/CVPR2022/papers/Xu_AME_Attention_and_Memory_Enhancement_in_Hyper-Parameter_Optimization_CVPR_2022_paper.pdf)

<a name="33"/>

## 33.Human-Object Interaction(人物交互)
* [HOI4D: A 4D Egocentric Dataset for Category-Level Human-Object Interaction](https://arxiv.org/abs/2203.01577)<br>:star:[code](https://hoi4d.github.io/)
* [MSTR: Multi-Scale Transformer for End-to-End Human-Object Interaction Detection](https://arxiv.org/abs/2203.14709)
* [GEN-VLKT: Simplify Association and Enhance Interaction Understanding for HOI Detection](https://arxiv.org/abs/2203.13954)<br>:star:[code](https://github.com/YueLiao/gen-vlkt)
* [Distillation Using Oracle Queries for Transformer-Based Human-Object Interaction Detection](https://openaccess.thecvf.com/content/CVPR2022/papers/Qu_Distillation_Using_Oracle_Queries_for_Transformer-Based_Human-Object_Interaction_Detection_CVPR_2022_paper.pdf)<br>:star:[code](https://github.com/SherlockHolmes221/DOQ)
* [OakInk: A Large-scale Knowledge Repository for Understanding Hand-Object Interaction](https://arxiv.org/abs/2203.15709)<br>:star:[code](https://github.com/lixiny/OakInk)<br>:newspaper:[粗解](https://news.sjtu.edu.cn/jdzh/20220330/169456.html)
* [D-Grasp: Physically Plausible Dynamic Grasp Synthesis for Hand-Object Interactions](https://arxiv.org/abs/2112.03028)<br>:house:[code](https://eth-ait.github.io/d-grasp/)
* [Learning Transferable Human-Object Interaction Detector With Natural Language Supervision](https://openaccess.thecvf.com/content/CVPR2022/papers/Wang_Learning_Transferable_Human-Object_Interaction_Detector_With_Natural_Language_Supervision_CVPR_2022_paper.pdf)<br>:star:[code](https://github.com/scwangdyd/promting_hoi)
* [What to look at and where: Semantic and Spatial Refined Transformer for detecting human-object interactions](https://arxiv.org/abs/2204.00746)<br>:open_mouth:oral
* [Human-Object Interaction Detection via Disentangled Transformer](https://arxiv.org/abs/2204.09290)
* [Consistency Learning via Decoding Path Augmentation for Transformers in Human Object Interaction Detection](https://arxiv.org/abs/2204.04836)<br>:star:[code](https://github.com/mlvlab/CPChoi):newspaper:[解读](https://zhuanlan.zhihu.com/p/497009845)
* [Interactiveness Field in Human-Object Interactions](https://arxiv.org/abs/2204.07718)<br>:star:[code](https://github.com/Foruck/Interactiveness-Field)
* [Stability-driven Contact Reconstruction From Monocular Color Images](https://arxiv.org/abs/2205.00848)<br>:star:[code](https://www.yangangwang.com/ (代码将开源))<br>单目彩色图像的手物交互重建，人机交互
* [ Interactiveness Field of Human-Object Interactions](https://arxiv.org/abs/2204.07718)<br>:star:[code](https://github.com/Foruck/Interactiveness-Field)<br>:newspaper:[粗解](https://news.sjtu.edu.cn/jdzh/20220330/169456.html)
* [Exploring Structure-aware Transformer over Interaction Proposals for Human-Object Interaction Detection](https://arxiv.org/abs/2206.06291)<br>:star:[code](https://github.com/zyong812/STIP)<br>:newspaper:[解读1](https://zhuanlan.zhihu.com/p/528607351)<br>:newspaper:[解读2](https://mp.weixin.qq.com/s/lSY1is6Fmm6A0Db0Jxo4qg)
* [Bongard-HOI: Benchmarking Few-Shot Visual Reasoning for Human-Object Interactions](https://openaccess.thecvf.com/content/CVPR2022/papers/Jiang_Bongard-HOI_Benchmarking_Few-Shot_Visual_Reasoning_for_Human-Object_Interactions_CVPR_2022_paper.pdf)<br>:open_mouth:oral:star:[code](https://github.com/nvlabs/Bongard-HOI)  
* [Efficient Two-Stage Detection of Human-Object Interactions With a Novel Unary-Pairwise Transformer](https://arxiv.org/abs/2112.01838)<br>:house:[project](https://fredzzhang.com/unary-pairwise-transformers)
* [NeuralHOFusion: Neural Volumetric Rendering Under Human-Object Interactions](https://arxiv.org/abs/2202.12825)
* HOI跟踪
  * [BEHAVE: Dataset and Method for Tracking Human Object Interactions](https://arxiv.org/abs/2204.06950)<br>:house:[project](http://virtualhumans.mpi-inf.mpg.de/behave/)

<a name="32"/>

## 32.Data Augmentation(数据增强)
* 🐦️[AlignMix: Improving representation by interpolating aligned features](https://arxiv.org/abs/2103.15375)
* [3D Common Corruptions and Data Augmentation](https://arxiv.org/abs/2203.01441)<br>:star:[code](https://github.com/EPFL-VILAB/3DCommonCorruptions):house:[project](https://3dcommoncorruptions.epfl.ch/):tv:[video](https://youtu.be/vtkXaS0Q6I4):newspaper:[粗解](https://zhuanlan.zhihu.com/p/475067096)
* [Kubric: A scalable dataset generator](https://arxiv.org/abs/2203.03570)
* [Robust Optimization As Data Augmentation for Large-Scale Graphs](https://arxiv.org/abs/2010.09891)<br>:star:[code](https://github.com/devnkong/FLAG)
* [AIM: an Auto-Augmenter for Images and Meshes](https://openaccess.thecvf.com/content/CVPR2022/papers/Singh_AIM_An_Auto-Augmenter_for_Images_and_Meshes_CVPR_2022_paper.pdf)<br>:star:[code](https://github.com/VimsLab/AIM)

<a name="31"/>

## 31.Vision-Language(视觉语言)
* [Unsupervised Vision-Language Parsing: Seamlessly Bridging Visual Scene Graphs with Language Structures via Dependency Relationships](https://arxiv.org/abs/2203.14260)<br>:star:[code](https://github.com/bigai-research/VLGAE)
* [VL-InterpreT: An Interactive Visualization Tool for Interpreting Vision-Language Transformers](https://arxiv.org/abs/2203.17247)
* [Winoground: Probing Vision and Language Models for Visio-Linguistic Compositionality](https://arxiv.org/abs/2204.03162)<br>:sunflower:[dataset](https://huggingface.co/datasets/facebook/winoground)
* [Robust Cross-Modal Representation Learning with Progressive Self-Distillation](https://arxiv.org/abs/2204.04588)
* [Prompt Distribution Learning](https://arxiv.org/abs/2205.03340)<br>在下游的识别任务中，作者提出的方法在12个数据集上均展示出了一致性的性能提升。
* [Vision-Language Pre-Training with Triple Contrastive Learning](https://arxiv.org/abs/2202.10401)<br>:star:[code](https://github.com/uta-smile/TCL)
* [Improving features Visual Grounding with Visual-Linguistic Veriﬁcation and Iterative Reasoning](https://arxiv.org/abs/2205.00272)<br>:star:[code](https://github.com/yangli18/VLTVG)<br>:newspaper:[国科大&港中文提出带视觉语言验证和迭代推理的Visual Grounding框架，性能SOTA，代码已开源！](https://mp.weixin.qq.com/s/9lNWCax_gBHgg70a8eVJ_w)
* [Towards General Purpose Vision Systems: An End-to-End Task-Agnostic Vision-Language Architecture](https://openaccess.thecvf.com/content/CVPR2022/papers/Gupta_Towards_General_Purpose_Vision_Systems_An_End-to-End_Task-Agnostic_Vision-Language_Architecture_CVPR_2022_paper.pdf)<br>:star:[code](https://github.com/allenai/gpv-1/):house:[project](https://prior.allenai.org/projects/gpv)
* [VL-Adapter: Parameter-Efficient Transfer Learning for Vision-and-Language Tasks](https://openaccess.thecvf.com/content/CVPR2022/papers/Sung_VL-Adapter_Parameter-Efficient_Transfer_Learning_for_Vision-and-Language_Tasks_CVPR_2022_paper.pdf)<br>:star:[code](https://github.com/ylsung/VL_adapter)
* [Lite-MDETR: A Lightweight Multi-Modal Detector](https://openaccess.thecvf.com/content/CVPR2022/papers/Lou_Lite-MDETR_A_Lightweight_Multi-Modal_Detector_CVPR_2022_paper.pdf)
* [Align and Prompt: Video-and-Language Pre-Training With Entity Prompts](https://arxiv.org/abs/2112.09583)<br>:star:[code](https://github.com/salesforce/ALPRO)
* [Unsupervised Vision-and-Language Pre-Training via Retrieval-Based Multi-Granular Alignment](https://arxiv.org/abs/2203.00242)
* [RegionCLIP: Region-based Language-Image Pretraining](https://arxiv.org/abs/2112.09106)(https://github.com/microsoft/RegionCLIP)
* [Grounded Language-Image Pre-Training](https://arxiv.org/abs/2112.03857)<br>:star:[code](https://github.com/microsoft/GLIP)
* [Advancing High-Resolution Video-Language Representation With Large-Scale Video Transcriptions](https://arxiv.org/abs/2111.10337)<br>:star:[code](https://github.com/microsoft/XPretrain/tree/main/hd-vila)
* [Conditional Prompt Learning for Vision-Language Models](https://arxiv.org/abs/2203.05557)<br>:star:[code](https://github.com/KaiyangZhou/CoOp)
* [Multi-Modal Alignment Using Representation Codebook](https://arxiv.org/abs/2203.00048)
* [NLX-GPT: A Model for Natural Language Explanations in Vision and Vision-Language Tasks](https://openaccess.thecvf.com/content/CVPR2022/papers/Sammani_NLX-GPT_A_Model_for_Natural_Language_Explanations_in_Vision_and_CVPR_2022_paper.pdf)<br>:open_mouth:oral:star:[code](https://github.com/fawazsammani/nlxgpt)
* VLN
  * [EnvEdit: Environment Editing for Vision-and-Language Navigation](https://arxiv.org/abs/2203.15685)<br>:star:[code](https://github.com/jialuli-luka/EnvEdit)
  * [Counterfactual Cycle-Consistent Learning for Instruction Following and Generation in Vision-Language Navigation](https://arxiv.org/abs/2203.16586)<br>:star:[code](https://github.com/HanqingWangAI/CCC-VLN)
  * [Reinforced Structured State-Evolution for Vision-Language Navigation](https://arxiv.org/abs/2204.09280)<br>:star:[code](https://github.com/chenjinyubuaa/SEvol):newspaper:[解读](https://zhuanlan.zhihu.com/p/502240740)
  * [Cross-modal Map Learning for Vision and Language Navigation](https://arxiv.org/abs/2203.05137)<br>:star:[code](https://github.com/ggeorgak11/CM2):house:[project](https://ggeorgak11.github.io/CM2-project/) 
  * [One Step at a Time: Long-Horizon Vision-and-Language Navigation With Milestones](https://arxiv.org/abs/2202.07028)
  * [What do navigation agents learn about their environment?](https://openaccess.thecvf.com/content/CVPR2022/papers/Dwivedi_What_Do_Navigation_Agents_Learn_About_Their_Environment_CVPR_2022_paper.pdf)
  * [Bridging the Gap Between Learning in Discrete and Continuous Environments for Vision-and-Language Navigation](https://arxiv.org/abs/2203.02764)<br>:star:[code](https://github.com/YicongHong/Discrete-Continuous-VLN)
  * [ADAPT: Vision-Language Navigation With Modality-Aligned Action Prompts](https://arxiv.org/abs/2205.15509)
* 视频-文本表示学习
  * [Video-Text Representation Learning via Differentiable Weak Temporal Alignment](https://arxiv.org/abs/2203.16784)<br>:star:[code](https://github.com/mlvlab/VT-TWINS)
* 视觉表征学习
  * [Unsupervised Visual Representation Learning by Online Constrained K-Means](https://openaccess.thecvf.com/content/CVPR2022/papers/Qian_Unsupervised_Visual_Representation_Learning_by_Online_Constrained_K-Means_CVPR_2022_paper.pdf)
  * [When Does Contrastive Visual Representation Learning Work?](https://arxiv.org/abs/2105.05837)
* 视觉导航
  * [PONI: Potential Functions for ObjectGoal Navigation With Interaction-Free Learning](https://openaccess.thecvf.com/content/CVPR2022/papers/Ramakrishnan_PONI_Potential_Functions_for_ObjectGoal_Navigation_With_Interaction-Free_Learning_CVPR_2022_paper.pdf)<br>:star:[code](https://github.com/srama2512/PONI):house:[project](https://vision.cs.utexas.edu/projects/poni/)
* 视觉描述
  * [Weakly-Supervised Generation and Grounding of Visual Descriptions With Conditional Generative Models](https://openaccess.thecvf.com/content/CVPR2022/papers/Mavroudi_Weakly-Supervised_Generation_and_Grounding_of_Visual_Descriptions_With_Conditional_Generative_CVPR_2022_paper.pdf)

<a name="30"/>

## 30.Visual Answer Questions(视觉问答)
* VQA
  * [SimVQA: Exploring Simulated Environments for Visual Question Answering](https://arxiv.org/abs/2203.17219)<br>:house:[project](https://simvqa.github.io)
  * [SwapMix: Diagnosing and Regularizing the Over-Reliance on Visual Context in Visual Question Answering](https://arxiv.org/abs/2204.02285)<br>:star:[code](https://github.com/vipulgupta1011/swapmix):newspaper:[粗解](https://zhuanlan.zhihu.com/p/493615566)
  * [V-Doc: Visual Questions Answers With Documents](https://openaccess.thecvf.com/content/CVPR2022/papers/Ding_V-Doc_Visual_Questions_Answers_With_Documents_CVPR_2022_paper.pdf)<br>:star:[code](https://github.com/usydnlp/vdoc)
  * [Grounding Answers for Visual Questions Asked by Visually Impaired People](https://arxiv.org/abs/2202.01993)<br>:house:[project](https://vizwiz.org/tasks-and-datasets/answer-grounding-for-vqa/)
  * [Query and Attention Augmentation for Knowledge-Based Explainable Reasoning](https://openaccess.thecvf.com/content/CVPR2022/papers/Zhang_Query_and_Attention_Augmentation_for_Knowledge-Based_Explainable_Reasoning_CVPR_2022_paper.pdf)<br>:star:[code](https://github.com/SuperJohnZhang/QAA)
  * [MuKEA: Multimodal Knowledge Extraction and Accumulation for Knowledge-Based Visual Question Answering](https://arxiv.org/abs/2203.09138)<br>:star:[code](https://github.com/AndersonStra/MuKEA)
  * [Transform-Retrieve-Generate: Natural Language-Centric Outside-Knowledge Visual Question Answering](https://openaccess.thecvf.com/content/CVPR2022/papers/Gao_Transform-Retrieve-Generate_Natural_Language-Centric_Outside-Knowledge_Visual_Question_Answering_CVPR_2022_paper.pdf)
  * [LaTr: Layout-Aware Transformer for Scene-Text VQA](https://arxiv.org/abs/2112.12494)
  * [WebQA: Multihop and Multimodal QA](https://arxiv.org/abs/2109.00590)* AVQA
  * [Learning to Answer Questions in Dynamic Audio-Visual Scenarios](https://arxiv.org/abs/2203.14072)<br>:open_mouth:oral:star:[code](https://github.com/GeWu-Lab/MUSIC-AVQA)<br>:newspaper:[CVPR 2022 Oral | 人大高瓴AI学院提出面向动态视音场景的问答学习任务](https://mp.weixin.qq.com/s/6rWjlkMK8G8aNA93RMFtRQ)
  * [Dual-Key Multimodal Backdoors for Visual Question Answering](https://arxiv.org/abs/2112.07668)<br>:star:[code](https://github.com/SRI-CSL/TrinityMultimodalTrojAI)
  * [Maintaining Reasoning Consistency in Compositional Visual Question Answering](https://openaccess.thecvf.com/content/CVPR2022/papers/Jing_Maintaining_Reasoning_Consistency_in_Compositional_Visual_Question_Answering_CVPR_2022_paper.pdf)<br>:star:[code](https://github.com/jingchenchen/ReasoningConsistency-VQA)
* Video-QA
  * [Measuring Compositional Consistency for Video Question Answering](https://arxiv.org/abs/2204.07190) 
  * [Invariant Grounding for Video Question Answering](https://arxiv.org/abs/2206.02349)<br>:open_mouth:oral:star:[code](https://github.com/yl3800/IGV):newspaper:[解读](https://zhuanlan.zhihu.com/p/525331776)

<a name="29"/>

## 29.SLAM/Augmented Reality/Virtual Reality/Robotics(增强/虚拟现实/机器人)
* SLAM
  * [NICE-SLAM: Neural Implicit Scalable Encoding for SLAM](https://openaccess.thecvf.com/content/CVPR2022/papers/Zhu_NICE-SLAM_Neural_Implicit_Scalable_Encoding_for_SLAM_CVPR_2022_paper.pdf)<br>:star:[code](https://github.com/cvg/nice-slam):house:[project](https://pengsongyou.github.io/nice-slam):tv:[video](https://www.youtube.com/watch?v=V5hYTz5os0M)
* 目标导航
  * [Online Learning of Reusable Abstract Models for Object Goal Navigation](https://arxiv.org/abs/2203.02583)
  * [Is Mapping Necessary for Realistic PointGoal Navigation?](https://arxiv.org/abs/2206.00997)<br>:star:[code](https://github.com/rpartsey/pointgoal-navigation):house:[project](https://rpartsey.github.io/pointgoalnav/)
* try-on
  * [Dressing in the Wild by Watching Dance Videos](https://arxiv.org/abs/2203.15320)<br>:house:[project](https://awesome-wflow.github.io)
  * [Style-Based Global Appearance Flow for Virtual Try-On](https://arxiv.org/abs/2204.01046)<br>:star:[code](https://github.com/SenHe/Flow-Style-VTON)
  * [ClothFormer:Taming Video Virtual Try-on in All Module](https://arxiv.org/abs/2204.12151)<br>:open_mouth:oral:star:[code](https://github.com/luxiangju-PersonAI/ClothFormer):house:[project](https://cloth-former.github.io/):newspaper:[解读](https://zhuanlan.zhihu.com/p/505802169) 
  * [Weakly Supervised High-Fidelity Clothing Model Generation](https://arxiv.org/abs/2112.07200)  
  * [Full-Range Virtual Try-On With Recurrent Tri-Level Transform](https://openaccess.thecvf.com/content/CVPR2022/papers/Yang_Full-Range_Virtual_Try-On_With_Recurrent_Tri-Level_Transform_CVPR_2022_paper.pdf)<br>:house:[project](https://lzqhardworker.github.io/RT-VTON/)
* AR
  * [Episodic Memory Question Answering](https://arxiv.org/abs/2205.01652)<br>:open_mouth:oral:star:[code](https://samyak-268.github.io/emqa)<br>AI助理：情景记忆问答 （增强现实新任务，数据及代码均将开源）
* 机器人
  * 手-物姿态估计
    * [ArtiBoost: Boosting Articulated 3D Hand-Object Pose Estimation via Online Exploration and Synthesis](https://arxiv.org/abs/2109.05488)<br>:star:[code](https://github.com/lixiny/ArtiBoost)<br>:newspaper:[粗解](https://news.sjtu.edu.cn/jdzh/20220330/169456.html)  
* 机器人导航
  * [Coupling Vision and Proprioception for Navigation of Legged Robots](https://arxiv.org/abs/2112.02094)<br>:star:[code](https://github.com/MarkFzp/navigation-locomotion):house:[project](https://navigation-locomotion.github.io/):tv:[video](https://youtu.be/sZVvutQUAQ4)
    
<a name="28"/>

## 28.Style Transfer(风格迁移)
* [Pastiche Master: Exemplar-Based High-Resolution Portrait Style Transfer](https://arxiv.org/pdf/2203.13248.pdf)<br>:star:[code](https://github.com/williamyang1991/DualStyleGAN)
* [Industrial Style Transfer with Large-scale Geometric Warping and Content Preservation](https://arxiv.org/abs/2203.12835)<br>:star:[code](https://github.com/jcyang98/InST)
* [Exact Feature Distribution Matching for Arbitrary Style Transfer and Domain Generalization](https://arxiv.org/abs/2203.07740)<br>:open_mouth:oral:star:[code](https://github.com/YBZh/EFDM) 
* [HEAT: Holistic Edge Attention Transformer for Structured Reconstruction](https://arxiv.org/abs/2111.15143)<br>:star:[code](https://github.com/Shelsin/ArtIns)
* [StyTr2: Image Style Transfer With Transformers](https://openaccess.thecvf.com/content/CVPR2022/papers/Deng_StyTr2_Image_Style_Transfer_With_Transformers_CVPR_2022_paper.pdf)<br>:star:[code](https://github.com/diyiiyiii/StyTR-2)
* 运动风格迁移
  * [Style-ERD: Responsive and Coherent Online Motion Style Transfer](https://arxiv.org/abs/2203.02574)
* 运动迁移
  * [Structure-Aware Motion Transfer with Deformable Anchor Model](https://arxiv.org/abs/2204.05018)<br>:star:[code](https://github.com/JialeTao/DAM):newspaper:[解读](https://zhuanlan.zhihu.com/p/497009845)
* 场景风格化
  * [StylizedNeRF: Consistent 3D Scene Stylization as Stylized NeRF via 2D-3D Mutual Learning](https://arxiv.org/abs/2205.12183)
* 外观迁移
  * [Splicing ViT Features for Semantic Appearance Transfer](https://openaccess.thecvf.com/content/CVPR2022/papers/Tumanyan_Splicing_ViT_Features_for_Semantic_Appearance_Transfer_CVPR_2022_paper.pdf)<br>:open_mouth:oral:star:[code](https://github.com/omerbt/Splice):house:[project](https://splice-vit.github.io/)  
* 风格化
  * [Text2Mesh: Text-Driven Neural Stylization for Meshes](https://openaccess.thecvf.com/content/CVPR2022/papers/Michel_Text2Mesh_Text-Driven_Neural_Stylization_for_Meshes_CVPR_2022_paper.pdf)<br>:star:[code](https://github.com/threedle/text2mesh):house:[project](https://threedle.github.io/text2mesh/)
  
<a name="27"/>

## 27.Pose Estimation(物体姿势估计)
* [OSOP: A Multi-Stage One Shot Object Pose Estimation Framework](https://arxiv.org/abs/2203.15533)
* [OnePose: One-Shot Object Pose Estimation without CAD Models](https://arxiv.org/abs/2205.12257)<br>:star:[code](https://github.com/zju3dv/OnePose):house:[project](https://zju3dv.github.io/onepose/):newspaper:[解读](https://zhuanlan.zhihu.com/p/519556254)
* [ABPN: Adaptive Blend Pyramid Network for Real-Time Local Retouching of Ultra High-Resolution Photo](https://arxiv.org/abs/2205.08811)
* [On the Instability of Relative Pose Estimation and RANSAC's Role](https://openaccess.thecvf.com/content/CVPR2022/papers/Fan_On_the_Instability_of_Relative_Pose_Estimation_and_RANSACs_Role_CVPR_2022_paper.pdf)
* [SurfEmb: Dense and Continuous Correspondence Distributions for Object Pose Estimation With Learnt Surface Embeddings](https://arxiv.org/abs/2111.13489)<br>:star:[code](https://github.com/rasmushaugaard/surfemb):house:[project](https://surfemb.github.io/)
* [ConDor: Self-Supervised Canonicalization of 3D Pose for Partial Shapes](https://arxiv.org/abs/2201.07788)
<br>:star:[code](https://github.com/brown-ivl/ConDor):house:[project](https://ivl.cs.brown.edu/ConDor/):tv:[video](https://youtu.be/JKDiCvVPoSw)
* [GPV-Pose: Category-Level Object Pose Estimation via Geometry-Guided Point-Wise Voting](https://openaccess.thecvf.com/content/CVPR2022/papers/Di_GPV-Pose_Category-Level_Object_Pose_Estimation_via_Geometry-Guided_Point-Wise_Voting_CVPR_2022_paper.pdf)
* 4D
  * [Revealing Occlusions with 4D Neural Fields](https://arxiv.org/abs/2204.10916)<br>:open_mouth:oral:star:[code](https://github.com/basilevh/occlusions-4d):house:[project](https://occlusions.cs.columbia.edu/)
* 9D
  * [CPPF: Towards Robust Category-Level 9D Pose Estimation in the Wild](https://arxiv.org/abs/2203.03089)<br>:star:[code](https://github.com/qq456cvb/CPPF):newspaper:[粗解](https://zhuanlan.zhihu.com/p/477624433)[:notebook:](https://news.sjtu.edu.cn/jdzh/20220330/169456.html)
* 单目目标姿势估计
  * [EPro-PnP: Generalized End-to-End Probabilistic Perspective-n-Points for Monocular Object Pose Estimation](https://arxiv.org/pdf/2203.13254.pdf)<br>:star:[code](https://github.com/tjiiv-cprg/EPro-PnP)
* 6D
  * [RNNPose: Recurrent 6-DoF Object Pose Refinement with Robust Correspondence Field Estimation and Pose Optimization](https://arxiv.org/abs/2203.12870)<br>:star:[code](https://github.com/DecaYale/RNNPose)
  * [FS6D: Few-Shot 6D Pose Estimation of Novel Objects](https://arxiv.org/abs/2203.14628)<br>:star:[code](https://github.com/ethnhe/FS6D-PyTorch):house:[project](https://fs6d.github.io):newspaper:[解读](https://mp.weixin.qq.com/s/kZue3ds348UXQI86xrwudQ)
  * [Uni6D: A Unified CNN Framework without Projection Breakdown for 6D Pose Estimation](https://arxiv.org/abs/2203.14531)
  * [ES6D: A Computation Efficient and Symmetry-Aware 6D Pose Regression Framework](https://arxiv.org/abs/2204.01080)<br>:star:[code](https://github.com/GANWANSHUI/ES6D) 
  * [Focal Length and Object Pose Estimation via Render and Compare](https://arxiv.org/abs/2204.05145)<br>:star:[code](https://github.com/ponimatkin/focalpose):house:[project](https://ponimatkin.github.io/focalpose/):newspaper:[解读](https://zhuanlan.zhihu.com/p/497009845)
  * [DGECN: A Depth-Guided Edge Convolutional Network for End-to-End 6D Pose Estimation](https://arxiv.org/abs/2204.09983)<br>:star:[code](https://github.com/maplect/DGECN_CVPR2022):house:[project](http://graphvision.whu.edu.cn/):newspaper:[解读](https://zhuanlan.zhihu.com/p/502894478)
  * [Coupled Iterative Refinement for 6D Multi-Object Pose Estimation](https://arxiv.org/abs/2204.12516)<br>:star:[code](https://github.com/princeton-vl/Coupled-Iterative-Refinement):newspaper:[解读](https://zhuanlan.zhihu.com/p/506416975)
  * [ZebraPose: Coarse To Fine Surface Encoding for 6DoF Object Pose Estimation](https://arxiv.org/abs/2203.09418)<br>:star:[code](https://github.com/suyz526/ZebraPose)
  * [Symmetry and Uncertainty-Aware Object SLAM for 6DoF Object Pose Estimation](https://arxiv.org/abs/2205.01823)<br>:star:[code](https://github.com/rpng/suo_slam)
  * [OVE6D: Object Viewpoint Encoding for Depth-Based 6D Object Pose Estimation](https://openaccess.thecvf.com/content/CVPR2022/papers/Cai_OVE6D_Object_Viewpoint_Encoding_for_Depth-Based_6D_Object_Pose_Estimation_CVPR_2022_paper.pdf)<br>:star:[code](https://github.com/dingdingcai/OVE6D-pose)
* 3D Object Articulation
  * [Understanding 3D Object Articulation in Internet Videos](https://arxiv.org/abs/2203.16531)<br>:house:[project](https://jasonqsy.github.io/Articulation3D/)
* 3Dope
  * [Templates for 3D Object Pose Estimation Revisited: Generalization to New Objects and Robustness to Occlusions](https://arxiv.org/abs/2203.17234)<br>:star:[code](https://github.com/nv-nguyen/template-pose)

<a name="26"/>

## 26.GCN/GNN
* GNN
  * 🐦️[Lifelong Graph Learning](https://arxiv.org/pdf/2009.00647.pdf)<br>:star:[code](https://github.com/wang-chen/LGL)
  * [AEGNN: Asynchronous Event-based Graph Neural Networks](https://arxiv.org/abs/2203.17149)<br>:star:[code](https://github.com/uzh-rpg/aegnn/):house:[project](https://uzh-rpg.github.io/aegnn/)
  * ["The Pedestrian next to the Lamppost" Adaptive Object Graphs for Better Instantaneous Mapping](https://arxiv.org/abs/2204.02944)
  * [OrphicX: A Causality-Inspired Latent Variable Model for Interpreting Graph Neural Networks](https://arxiv.org/abs/2203.15209)<br>:open_mouth:oral:star:[code](https://github.com/WanyuGroup/CVPR2022-OrphicX)


<a name="25"/>

## 25.Fine-Grained/Image Classification(细粒度/图像分类)
* 细粒度分类
  * [Dynamic MLP for Fine-Grained Image Classification by Leveraging Geographical and Temporal Information](https://arxiv.org/abs/2203.03253)<br>:star:[code](https://github.com/ylingfeng/DynamicMLP):newspaper:[粗解](https://zhuanlan.zhihu.com/p/477624433):notebook:[粗解](https://mp.weixin.qq.com/s/kZue3ds348UXQI86xrwudQ)
  * [Fine-Grained Object Classification via Self-Supervised Pose Alignment](https://arxiv.org/abs/2203.15987)<br>:star:[code](https://github.com/yangxh11/P2P-Net)
* 图像分类
  * [Label Relation Graphs Enhanced Hierarchical Residual Network for Hierarchical Multi-Granularity Classification](https://arxiv.org/abs/2201.03194)<br>:star:[code](https://github.com/MonsterZhZh/HRN)
  * [DTFD-MIL: Double-Tier Feature Distillation Multiple Instance Learning for Histopathology Whole Slide Image Classification](https://arxiv.org/abs/2203.12081)<br>:star:[code](https://github.com/hrzhang1123/DTFD-MIL)
  * [Contrastive Test-Time Adaptation](https://arxiv.org/abs/2204.10377)<br>:house:[project](https://sites.google.com/view/adacontrast)
  * [A Comprehensive Study of Image Classification Model Sensitivity to Foregrounds, Backgrounds, and Visual Attributes](https://arxiv.org/abs/2201.10766)
  * [VisCUIT: Visual Auditor for Bias in CNN Image Classifier](https://arxiv.org/abs/2204.05899)<br>:tv:[video](https://www.youtube.com/watch?v=eNDbSyM4R_4)
  * [Multi-Label Iterated Learning for Image Classification With Label Ambiguity](https://openaccess.thecvf.com/content/CVPR2022/papers/Rajeswar_Multi-Label_Iterated_Learning_for_Image_Classification_With_Label_Ambiguity_CVPR_2022_paper.pdf)<br>:star:[code](https://github.com/rajeswar18/MILe)
  * [Efficient Classification of Very Large Images With Tiny Objects](https://arxiv.org/abs/2106.02694)
  * [Node-Aligned Graph Convolutional Network for Whole-Slide Image Representation and Classification](https://openaccess.thecvf.com/content/CVPR2022/papers/Guan_Node-Aligned_Graph_Convolutional_Network_for_Whole-Slide_Image_Representation_and_Classification_CVPR_2022_paper.pdf)<br>:star:[code](https://github.com/YohnGuan/NAGCN)
  * [Deformable ProtoPNet: An Interpretable Image Classifier Using Deformable Prototypes](https://arxiv.org/abs/2111.15000)<br>:star:[code](https://github.com/jdonnelly36/Deformable-ProtoPNet)
* 小样本分类
  * [CAD: Co-Adapting Discriminative Features for Improved Few-Shot Classification](https://arxiv.org/abs/2203.13465)
  * [Matching Feature Sets for Few-Shot Image Classification](https://arxiv.org/abs/2204.00949)<br>:star:[code](https://github.com/ArmanAfrasiyabi/SetFeat-fs):house:[project](https://lvsn.github.io/SetFeat/):tv:[video](https://www.youtube.com/embed/)
  * [Joint Distribution Matters: Deep Brownian Distance Covariance for Few-Shot Classification](https://arxiv.org/abs/2204.04567)<br>:open_mouth:oral:star:[code](https://github.com/Fei-Long121/DeepBDC):house:[project](http://www.peihuali.org/DeepBDC/):newspaper:[解读](https://zhuanlan.zhihu.com/p/497009845) 
  * [Learning to Affiliate: Mutual Centralized Learning for Few-shot Classification](https://arxiv.org/abs/2106.05517)<br>:newspaper:[解读](https://mp.weixin.qq.com/s/Ao_kZlAW_AqPcMdXg2Zerg)
  * [Generating Representative Samples for Few-Shot Classification](https://arxiv.org/abs/2205.02918)<br>:star:[code](https://github.com/cvlab-stonybrook/fsl-rsvae)<br>:newspaper:[粗解](https://zhuanlan.zhihu.com/p/511390027)<br>在小样本分类问题中，通过生成更多代表性样本，去除非代表性样本，改善了分类结果。实现了SOTA的结果。
  * [Improving Adversarially Robust Few-Shot Image Classification With Generalizable Representations](https://openaccess.thecvf.com/content/CVPR2022/papers/Dong_Improving_Adversarially_Robust_Few-Shot_Image_Classification_With_Generalizable_Representations_CVPR_2022_paper.pdf)
  * [Task Discrepancy Maximization for Fine-Grained Few-Shot Classification](https://openaccess.thecvf.com/content/CVPR2022/papers/Lee_Task_Discrepancy_Maximization_for_Fine-Grained_Few-Shot_Classification_CVPR_2022_paper.pdf)
  * 小样本分类与分割(FS-CS)
    * [Integrative Few-Shot Learning for Classification and Segmentation](https://arxiv.org/abs/2203.15712)<br>:star:[code](https://github.com/dahyun-kang/ifsl)
* 长尾识别
  * [Nested Collaborative Learning for Long-Tailed Visual Recognition](https://arxiv.org/abs/2203.15359)<br>:star:[code](https://github.com/Bazinga699/NCL)
  * [Long-Tailed Recognition via Weight Balancing](https://arxiv.org/abs/2203.14197)<br>:star:[code](https://github.com/ShadeAlsha/LTR-weight-balancing)
  * [Targeted Supervised Contrastive Learning for Long-Tailed Recognition](https://arxiv.org/abs/2111.13998)
  * [Long-Tail Recognition via Compositional Knowledge Transfer](https://openaccess.thecvf.com/content/CVPR2022/papers/Parisot_Long-Tail_Recognition_via_Compositional_Knowledge_Transfer_CVPR_2022_paper.pdf)
  * [RelTransformer: A Transformer-Based Long-Tail Visual Relationship Recognition](https://arxiv.org/abs/2104.11934)<br>:star:[code](https://github.com/Vision-CAIR/RelTransformer)
  * [Trustworthy Long-Tailed Classification](https://arxiv.org/abs/2111.09030)<br>:star:[code](https://github.com/lblaoke/TLC)
  * [Balanced Contrastive Learning for Long-Tailed Visual Recognition](https://openaccess.thecvf.com/content/CVPR2022/papers/Zhu_Balanced_Contrastive_Learning_for_Long-Tailed_Visual_Recognition_CVPR_2022_paper.pdf)
  * [The Majority Can Help the Minority: Context-Rich Minority Oversampling for Long-Tailed Classification](https://arxiv.org/abs/2112.00412)<br>:star:[code](https://github.com/naver-ai/cmo)
  * [Retrieval Augmented Classification for Long-Tail Visual Recognition](https://arxiv.org/abs/2202.11233)
* 细粒度识别
  * [Knowledge Mining with Scene Text for Fine-Grained Recognition](https://arxiv.org/abs/2203.14215)<br>:star:[code](https://github.com/lanfeng4659/KnowledgeMiningWithSceneText):newspaper:[解读](https://mp.weixin.qq.com/s/FYn0S46OA6xraH9ofNIctw)
* 多标签分类
  * [Large Loss Matters in Weakly Supervised Multi-Label Classification](https://arxiv.org/abs/2206.03740)<br>:star:[code](https://github.com/snucml/LargeLossMatters):house:[project](https://zhuanlan.zhihu.com/p/526268919)
* 类不平衡分类
  * [A Re-Balancing Strategy for Class-Imbalanced Classification Based on Instance Difficulty](https://openaccess.thecvf.com/content/CVPR2022/papers/Yu_A_Re-Balancing_Strategy_for_Class-Imbalanced_Classification_Based_on_Instance_Difficulty_CVPR_2022_paper.pdf)

<a name="24"/>

## 24.Super-Resolution(超分辨率)
* [Learning Graph Regularisation for Guided Super-Resolution](https://arxiv.org/abs/2203.14297)<br>:star:[code](https://github.com/prs-eth/graph-super-resolution)
* [Self-Supervised Super-Resolution for Multi-Exposure Push-Frame Satellites](https://arxiv.org/abs/2205.02031)<br>:star:[code](https://github.com/centreborelli/HDR-DSP-SR/):house:[project](https://centreborelli.github.io/HDR-DSP-SR/):newspaper:[解读](https://zhuanlan.zhihu.com/p/509470774/)
* [Deep Constrained Least Squares for Blind Image Super-Resolution](https://arxiv.org/abs/2202.07508)<br>:star:[code](https://github.com/megvii-research/DCLS-SR):newspaper:[解读](https://mp.weixin.qq.com/s/kZue3ds348UXQI86xrwudQ)
* [Discrete Cosine Transform Network for Guided Depth Map Super-Resolution](https://arxiv.org/abs/2104.06977)<br>:open_mouth:oral:star:[code](https://github.com/Zhaozixiang1228/GDSR-DCTNet)  
* [Details or Artifacts: A Locally Discriminative Learning Approach to Realistic Image Super-Resolution](https://arxiv.org/abs/2203.09195)<br>:star:[code](https://github.com/csjliang/LDL)
* [LAR-SR: A Local Autoregressive Model for Image Super-Resolution](https://openaccess.thecvf.com/content/CVPR2022/papers/Guo_LAR-SR_A_Local_Autoregressive_Model_for_Image_Super-Resolution_CVPR_2022_paper.pdf)
* [VideoINR: Learning Video Implicit Neural Representation for Continuous Space-Time Super-Resolution](https://openaccess.thecvf.com/content/CVPR2022/papers/Chen_VideoINR_Learning_Video_Implicit_Neural_Representation_for_Continuous_Space-Time_Super-Resolution_CVPR_2022_paper.pdf)<br>:star:[code](https://github.com/Picsart-AI-Research/VideoINR-Continuous-Space-Time-Super-Resolution)
* [Blind Image Super-Resolution With Elaborate Degradation Modeling on Noise and Kernel](https://arxiv.org/abs/2107.00986)<br>:star:[code](https://github.com/zsyOAOA/BSRDM)
* [Dual Adversarial Adaptation for Cross-Device Real-World Image Super-Resolution](https://arxiv.org/abs/2205.03524)<br>:star:[code](https://github.com/lonelyhope/DADA)
* [Reflash Dropout in Image Super-Resolution](http://arxiv.org/abs/2204.01446)
* [GCFSR: A Generative and Controllable Face Super Resolution Method Without Facial and GAN Priors](https://arxiv.org/abs/2203.07319)<br>:star:[code](https://github.com/hejingwenhejingwen/GCFSR)
* [Learning the Degradation Distribution for Blind Image Super-Resolution](https://arxiv.org/abs/2203.04962)<br>:star:[code](https://github.com/greatlog/UnpairedSR)
* VSR
  * [Stable Long-Term Recurrent Video Super-Resolution](https://openaccess.thecvf.com/content/CVPR2022/papers/Chiche_Stable_Long-Term_Recurrent_Video_Super-Resolution_CVPR_2022_paper.pdf)
  * [Reference-based Video Super-Resolution Using Multi-Camera Video Triplets](https://arxiv.org/abs/2203.14537)<br>:star:[code](https://github.com/codeslake/RefVSR)
  * [Learning Trajectory-Aware Transformer for Video Super-Resolution](https://arxiv.org/abs/2204.04216)<br>:open_mouth:oral:star:[code](https://github.com/researchmm/TTVSR)
  * [Investigating Tradeoffs in Real-World Video Super-Resolution](https://arxiv.org/pdf/2111.12704.pdf)<br>:star:[code](https://github.com/ckkelvinchan/RealBasicVSR):newspaper:[解读](https://zhuanlan.zhihu.com/p/437498177)
  * [BasicVSR++: Improving Video Super-Resolution with Enhanced Propagation and Alignment](https://arxiv.org/abs/2104.13371)<br>:star:[code](https://github.com/open-mmlab/mmediting):house:[project](https://ckkelvinchan.github.io/projects/BasicVSR++/):tv:[video](https://youtu.be/iIDml09CUc4)<br>🏆NTIRE 2021年视频修复和增强挑战赛冠军
  * [Look Back and Forth: Video Super-Resolution with Explicit Temporal Difference Modeling](https://arxiv.org/abs/2204.07114)<br>:newspaper:[ETDM：基于显式时间差分建模的视频超分辨率](https://mp.weixin.qq.com/s/X3DNy38FbfLv--pFljgfPw)
  * [Memory-Augmented Non-Local Attention for Video Super-Resolution](https://arxiv.org/abs/2108.11048)<br>:star:[code](https://github.com/jiy173/MANA):newspaper:[解读](https://mp.weixin.qq.com/s/lSY1is6Fmm6A0Db0Jxo4qg)
  * [Spatial-Temporal Space Hand-in-Hand: Spatial-Temporal Video Super-Resolution via Cycle-Projected Mutual Learning](https://arxiv.org/abs/2205.05264)<br>:star:[code](https://github.com/hhhhhumengshun/CycMuNet)<br>:newspaper:[解读](https://zhuanlan.zhihu.com/p/513130382)
  * [RSTT: Real-Time Spatial Temporal Transformer for Space-Time Video Super-Resolution](https://arxiv.org/abs/2203.14186)<br>:star:[code](https://github.com/llmpass/RSTT)

<a name="23"/>

## 23.Image Retrieval(图像检索)
* [Sketching without Worrying: Noise-Tolerant Sketch-Based Image Retrieval](https://arxiv.org/abs/2203.14817)<br>:star:[code](https://github.com/AyanKumarBhunia/Stroke_Subset_Selector-for-FGSBIR)
* [Correlation Verification for Image Retrieval](https://arxiv.org/abs/2204.01458)<br>:open_mouth:oral:star:[code](https://github.com/sungonce/CVNet)
* [Sketch3T: Test-Time Training for Zero-Shot SBIR](https://arxiv.org/abs/2203.14691)  
* [Beyond Cross-view Image Retrieval: Highly Accurate Vehicle Localization Using Satellite Image](https://arxiv.org/abs/2204.04752) 
* [Forward Compatible Training for Large-Scale Embedding Retrieval Systems](https://arxiv.org/abs/2112.02805)<br>:star:[code](https://github.com/apple/ml-fct)
* [Contextual Similarity Distillation for Asymmetric Image Retrieval](https://openaccess.thecvf.com/content/CVPR2022/papers/Wu_Contextual_Similarity_Distillation_for_Asymmetric_Image_Retrieval_CVPR_2022_paper.pdf)
* [Object-Aware Video-Language Pre-Training for Retrieval](https://arxiv.org/abs/2112.00656)<br>:star:[code](https://github.com/FingerRec/OA-Transformer)
* 视频检索
  * [Everything at Once - Multi-Modal Fusion Transformer for Video Retrieval](https://openaccess.thecvf.com/content/CVPR2022/papers/Shvetsova_Everything_at_Once_-_Multi-Modal_Fusion_Transformer_for_Video_Retrieval_CVPR_2022_paper.pdf)<br>:star:[code](https://github.com/ninatu/everything_at_once)
* 文本-视频检索
  * [X-Pool: Cross-Modal Language-Video Attention for Text-Video Retrieval](https://arxiv.org/abs/2203.15086)<br>:house:[project](https://layer6ai-labs.github.io/xpool/)<br>:newspaper:[X-Pool：多伦多大学提出基于文本的视频聚合方式，在视频文本检索上达到SOTA性能！](https://mp.weixin.qq.com/s/0cz1cI5a18ZQ-DORUmhZHA)
  * [Bridging Video-text Retrieval with Multiple Choice Questions](https://arxiv.org/abs/2201.04850)<br>:star:[code](https://github.com/TencentARC/MCQ)<br>:newspaper:[《BridgeFormer》港大&腾讯&伯克利提出带有多项选择任务的视频文本检索模型，性能SOTA！](https://mp.weixin.qq.com/s/MUwlACyQEcrA9Chw4it7iA)
* 跨模太检索
  * [ViSTA: Vision and Scene Text Aggregation for Cross-Modal Retrieval](https://arxiv.org/abs/2203.16778)
  * [Cross Modal Retrieval With Querybank Normalisation](https://arxiv.org/abs/2112.12777)<br>:star:[code](https://github.com/ioanacroi/qb-norm):house:[project](https://vladbogo.github.io/QB-Norm/)
  * [EI-CLIP: Entity-Aware Interventional Contrastive Learning for E-Commerce Cross-Modal Retrieval](https://openaccess.thecvf.com/content/CVPR2022/papers/Ma_EI-CLIP_Entity-Aware_Interventional_Contrastive_Learning_for_E-Commerce_Cross-Modal_Retrieval_CVPR_2022_paper.pdf)

 
<a name="22"/>

## 22.Image Synthesis/Generation(图像合成)
* [Interactive Image Synthesis with Panoptic Layout Generation](https://arxiv.org/abs/2203.02104)
* [Autoregressive Image Generation using Residual Quantization](https://arxiv.org/abs/2203.01941)<br>:star:[code](https://github.com/kakaobrain/rq-vae-transformer):newspaper:[粗解](https://zhuanlan.zhihu.com/p/476923554)
* [GIRAFFE HD: A High-Resolution 3D-aware Generative Model](https://arxiv.org/abs/2203.14954)
* [Arbitrary-Scale Image Synthesis](https://arxiv.org/abs/2204.02273)<br>:star:[code](https://github.com/vglsd/ScaleParty):newspaper:[粗解](https://zhuanlan.zhihu.com/p/493615566)
* [Multi-View Consistent Generative Adversarial Networks for 3D-aware Image Synthesis](https://arxiv.org/abs/2204.06160)<br>:star:[code](https://github.com/RenYurui/Neural-Texture-Extraction-Distribution):newspaper:[解读](https://zhuanlan.zhihu.com/p/498244289)
* [Learning to Memorize Feature Hallucination for One-Shot Image Generation](https://drive.google.com/file/d/1Gq-167f2ue30463K4XMkHOJThg9Yugk0/view?usp=sharing)<br>:newspaper:[解读](https://mp.weixin.qq.com/s/FYn0S46OA6xraH9ofNIctw)
* [StyleSwin: Transformer-Based GAN for High-Resolution Image Generation](https://arxiv.org/abs/2112.10762)<br>:star:[code](https://github.com/microsoft/StyleSwin)
* [Neural Texture Extraction and Distribution for Controllable Person Image Synthesis](https://arxiv.org/abs/2204.06160)<br>:star:[code](https://github.com/RenYurui/Neural-Texture-Extraction-Distribution)
* [Unpaired Cartoon Image Synthesis via Gated Cycle Mapping](https://openaccess.thecvf.com/content/CVPR2022/papers/Men_Unpaired_Cartoon_Image_Synthesis_via_Gated_Cycle_Mapping_CVPR_2022_paper.pdf)
* [Global Context With Discrete Diffusion in Vector Quantised Modelling for Image Generation](https://arxiv.org/abs/2112.01799)
* [3D Scene Painting via Semantic Image Synthesis](https://openaccess.thecvf.com/content/CVPR2022/papers/Jeong_3D_Scene_Painting_via_Semantic_Image_Synthesis_CVPR_2022_paper.pdf)
* [3D-Aware Image Synthesis via Learning Structural and Textural Representations](https://arxiv.org/abs/2112.10759)<br>:star:[code](https://github.com/genforce/volumegan):house:[project](https://genforce.github.io/volumegan/):tv:[video](https://www.youtube.com/watch?v=p85TVGJBMFc)
* [High-Resolution Image Synthesis With Latent Diffusion Models](https://arxiv.org/abs/2112.10752)<br>:star:[code](https://github.com/CompVis/latent-diffusion)
* [Retrieval-Based Spatially Adaptive Normalization for Semantic Image Synthesis](https://arxiv.org/abs/2204.02854)<br>:star:[code](https://github.com/Shi-Yupeng/RESAIL-For-SIS)
* [DPGEN: Differentially Private Generative Energy-Guided Network for Natural Image Synthesis](https://openaccess.thecvf.com/content/CVPR2022/papers/Chen_DPGEN_Differentially_Private_Generative_Energy-Guided_Network_for_Natural_Image_Synthesis_CVPR_2022_paper.pdf)<br>:star:[code](https://github.com/chiamuyu/DPGEN)
* 文本引导的图像处理
  * [ManiTrans: Entity-Level Text-Guided Image Manipulation via Token-wise Semantic Alignment and Generation](https://arxiv.org/abs/2204.04428)<br>:open_mouth:oral:house:[project](https://jawang19.github.io/manitrans/)
* 姿势引导的图像合成
  * [Exploring Dual-task Correlation for Pose Guided Person Image Generation](https://arxiv.org/abs/2203.02910)<br>:star:[code](https://github.com/PangzeCheung/Dual-task-Pose-Transformer-Network):newspaper:[粗解](https://zhuanlan.zhihu.com/p/477624433)
* 文本到图像合成
  * [StyleT2I: Toward Compositional and High-Fidelity Text-to-Image Synthesis](https://arxiv.org/abs/2203.15799)
  * [Text-to-Image Synthesis based on Object-Guided Joint-Decoding Transformer](https://fengxianghe.github.io/paper/wu2022text.pdf)<br>:newspaper:[解读](https://mp.weixin.qq.com/s/lSY1is6Fmm6A0Db0Jxo4qg)
  * [LAFITE: Towards Language-Free Training for Text-to-Image Generation](https://arxiv.org/abs/2111.13792)<br>:star:[code](https://github.com/drboog/Lafite)
  * [DF-GAN: A Simple and Effective Baseline for Text-to-Image Synthesis](https://openaccess.thecvf.com/content/CVPR2022/papers/Tao_DF-GAN_A_Simple_and_Effective_Baseline_for_Text-to-Image_Synthesis_CVPR_2022_paper.pdf)<br>:open_mouth:oral:star:[code](https://github.com/tobran/DF-GAN)
  * [Text to Image Generation With Semantic-Spatial Aware GAN](https://arxiv.org/abs/2104.00567)<br>:star:[code](https://github.com/wtliao/text2image)
* 图像翻译
  * [FlexIT: Towards Flexible Semantic Image Translation](https://arxiv.org/abs/2203.04705)
  * [A Style-aware Discriminator for Controllable Image Translation](https://arxiv.org/abs/2203.15375)
* 图像生成
  * [Marginal Contrastive Correspondence for Guided Image Generation](https://arxiv.org/abs/2204.00442)<br>:open_mouth:oral
  * [OSSGAN: Open-Set Semi-Supervised Image Generation](https://arxiv.org/abs/2204.14249)<br>:star:[code](https://github.com/raven38/OSSGAN)
  * [A Closer Look at Few-shot Image Generation](https://arxiv.org/abs/2205.03805)
  * [Modeling Image Composition for Complex Scene Generation](https://arxiv.org/abs/2206.00923)<br>:star:[code](https://github.com/JohnDreamer/TwFA)<br>:newspaper:[解读](https://mp.weixin.qq.com/s/lSY1is6Fmm6A0Db0Jxo4qg)
* 图像到本文
  * [ZeroCap: Zero-Shot Image-to-Text Generation for Visual-Semantic Arithmetic](https://arxiv.org/abs/2111.14447)<br>:star:[code](https://github.com/YoadTew/zero-shot-image-to-text)
* 文本-形状生成
  * [CLIP-Forge: Towards Zero-Shot Text-To-Shape Generation](https://openaccess.thecvf.com/content/CVPR2022/papers/Sanghi_CLIP-Forge_Towards_Zero-Shot_Text-To-Shape_Generation_CVPR_2022_paper.pdf)<br>:star:[code](https://github.com/AutodeskAILab/Clip-Forge)
* 图像-视频生成
  * [Make It Move: Controllable Image-to-Video Generation With Text Descriptions](https://arxiv.org/abs/2112.02815)<br>:star:[code](https://github.com/YouncyHu/MAGE)
* 基于文本的目标生成
  * [Zero-Shot Text-Guided Object Generation With Dream Fields](https://arxiv.org/abs/2112.01455)<br>:star:[code](https://github.com/google-research/google-research/tree/master/dreamfields):house:[project](https://ajayj.com/dreamfields)
* 人物图像生成
  * [Self-supervised Correlation Mining Network for Person Image Generation](https://arxiv.org/abs/2111.13307)

<a name="21"/>

## 21.UAV/Remote Sensing/Satellite Image(无人机/遥感/卫星图像)
* [CVNet: Contour Vibration Network for Building Extraction](https://openaccess.thecvf.com/content/CVPR2022/papers/Xu_CVNet_Contour_Vibration_Network_for_Building_Extraction_CVPR_2022_paper.pdf)<br>:star:[code](https://github.com/xzq-njust/CVNet)
* [CrossLoc: Scalable Aerial Localization Assisted by Multimodal Synthetic Data](https://arxiv.org/abs/2112.09081)<br>:house:[project](https://crossloc.github.io/)
* [Self-Supervised Material and Texture Representation Learning for Remote Sensing Tasks](https://arxiv.org/abs/2112.01715)<br>:star:[code](https://github.com/periakiva/MATTER)
* 遥感图像融合
  * [HyperTransformer: A Textural and Spectral Feature Fusion Transformer for Pansharpening](https://arxiv.org/abs/2203.02503)<br>:star:[code](https://github.com/wgcban/HyperTransformer):newspaper:[粗解](https://zhuanlan.zhihu.com/p/476923554)  
* 航空图像分割
  * [Revisiting Near/Remote Sensing with Geospatial Attention](https://arxiv.org/abs/2204.01807)

<a name="20"/>

## 20.Autonomous vehicles(自动驾驶)
* 自动驾驶
  * [Image-to-Lidar Self-Supervised Distillation for Autonomous Driving Data](https://arxiv.org/abs/2203.16258)<br>:star:[code](https://github.com/valeoai/SLidR)
  * [Exploiting Temporal Relations on Radar Perception for Autonomous Driving](https://arxiv.org/abs/2204.01184)
  * [COOPERNAUT: End-to-End Driving with Cooperative Perception for Networked Vehicles](https://arxiv.org/abs/2205.02222)<br>:star:[code](https://github.com/UT-Austin-RPL/Coopernaut):house:[project](https://ut-austin-rpl.github.io/Coopernaut/):newspaper:[解读](https://zhuanlan.zhihu.com/p/509470774/)
  * [Generating Useful Accident-Prone Driving Scenarios via a Learned Traffic Prior](https://arxiv.org/abs/2112.05077)<br>:house:[project](https://nv-tlabs.github.io/STRIVE/)
  * [Learning From All Vehicles](https://openaccess.thecvf.com/content/CVPR2022/papers/Chen_Learning_From_All_Vehicles_CVPR_2022_paper.pdf)<br>:star:[code](https://github.com/dotchen/LAV)
  * [Time3D: End-to-End Joint Monocular 3D Object Detection and Tracking for Autonomous Driving](https://arxiv.org/abs/2205.14882)
  * [Unifying Panoptic Segmentation for Autonomous Driving](https://openaccess.thecvf.com/content/CVPR2022/papers/Zendel_Unifying_Panoptic_Segmentation_for_Autonomous_Driving_CVPR_2022_paper.pdf)
  * [Investigating the Impact of Multi-LiDAR Placement on Object Detection for Autonomous Driving](https://arxiv.org/abs/2105.00373)<br>:star:[code](https://github.com/HanjiangHu/Multi-LiDAR-Placement-for-3D-Detection)
* 车道线检测
  * [Rethinking Efficient Lane Detection via Curve Modeling](https://arxiv.org/abs/2203.02431)<br>:star:[code](https://github.com/voldemortX/pytorch-auto-drive):newspaper:[粗解](https://zhuanlan.zhihu.com/p/476923554)<br>[:notebook:](https://mp.weixin.qq.com/s/FYn0S46OA6xraH9ofNIctw)
  * [Towards Driving-Oriented Metric for Lane Detection Models](https://arxiv.org/abs/2203.16851) 
  * [A Keypoint-based Global Association Network for Lane Detection](https://arxiv.org/abs/2204.07335)<br>:star:[code](https://github.com/Wolfwjs/GANet):newspaper:[解读](https://zhuanlan.zhihu.com/p/500351469)
  * 单目3D车道检测 
    * [ONCE-3DLanes: Building Monocular 3D Lane Detection](https://arxiv.org/abs/2205.00301)<br>:star:[code](https://once-3dlanes.github.io/)<br>车道线检测技术再演进
* 车道线描述
  * [Eigenlanes: Data-Driven Lane Descriptors for Structurally Diverse Lanes](https://arxiv.org/pdf/2203.15302.pdf)<br>:star:[code](https://github.com/dongkwonjin/Eigenlanes)
  * [CLRNet: Cross Layer Refinement Network for Lane Detection](https://arxiv.org/abs/2203.10350)<br>:star:[code](https://github.com/Turoad/CLRNet):newspaper:[解读](https://mp.weixin.qq.com/s/Ao_kZlAW_AqPcMdXg2Zerg) 
* 行为预测
  * 🐦️[JRDB-Act: A Large-scale Dataset for Spatio-temporal Action, Social Group and Activity Detection](https://arxiv.org/pdf/2106.08827.pdf)
* 自动驾驶场景重新照明
    * [SIMBAR: Single Image-Based Scene Relighting For Effective Data Augmentation For Automated Driving Vision Tasks](https://arxiv.org/abs/2204.00644)<br>:house:[project](https://simbarv1.github.io/)
* 行人轨迹预测
  * [Graph-based Spatial Transformer with Memory Replay for Multi-future Pedestrian Trajectory Prediction](https://arxiv.org/abs/2206.05712)<br>:star:[code](https://github.com/Jacobieee/ST-MR):newspaper:[解读](https://zhuanlan.zhihu.com/p/528607351)
  * [ATPFL: Automatic Trajectory Prediction Model Design Under Federated Learning Framework](https://openaccess.thecvf.com/content/CVPR2022/papers/Wang_ATPFL_Automatic_Trajectory_Prediction_Model_Design_Under_Federated_Learning_Framework_CVPR_2022_paper.pdf)
  * [Human Trajectory Prediction with Momentary Observation](https://openaccess.thecvf.com/content/CVPR2022/papers/Sun_Human_Trajectory_Prediction_With_Momentary_Observation_CVPR_2022_paper.pdf)<br>:newspaper:[粗解](https://news.sjtu.edu.cn/jdzh/20220330/169456.html)
* 轨迹预测
  * [MUSE-VAE: Multi-Scale VAE for Environment-Aware Long Term Trajectory Prediction](https://openaccess.thecvf.com/content/CVPR2022/papers/Lee_MUSE-VAE_Multi-Scale_VAE_for_Environment-Aware_Long_Term_Trajectory_Prediction_CVPR_2022_paper.pdf)
  * [Remember Intentions: Retrospective-Memory-Based Trajectory Prediction](https://arxiv.org/abs/2203.11474)<br>:star:[code](https://github.com/MediaBrain-SJTU/MemoNet)
  * [LTP: Lane-Based Trajectory Prediction for Autonomous Driving](https://openaccess.thecvf.com/content/CVPR2022/papers/Wang_LTP_Lane-Based_Trajectory_Prediction_for_Autonomous_Driving_CVPR_2022_paper.pdf)
  * [Vehicle trajectory prediction works, but not everywhere](https://arxiv.org/abs/2112.03909)<br>:house:[project](https://s-attack.github.io/)
  * [End-to-End Trajectory Distribution Prediction Based on Occupancy Grid Maps](https://arxiv.org/abs/2203.16910)<br>:star:[code](https://github.com/Kguo-cs/TDOR)
  * [Whose Track Is It Anyway? Improving Robustness to Tracking Errors With Affinity-Based Trajectory Prediction](https://openaccess.thecvf.com/content/CVPR2022/papers/Weng_Whose_Track_Is_It_Anyway_Improving_Robustness_to_Tracking_Errors_CVPR_2022_paper.pdf)
  * [Adaptive Trajectory Prediction via Transferable GNN](https://arxiv.org/abs/2203.05046)
  * [M2I: From Factored Marginal Trajectory Prediction to Interactive Prediction](https://openaccess.thecvf.com/content/CVPR2022/papers/Sun_M2I_From_Factored_Marginal_Trajectory_Prediction_to_Interactive_Prediction_CVPR_2022_paper.pdf)
* 车辆检测
  * [Modality-Agnostic Learning for Radar-Lidar Fusion in Vehicle Detection](https://openaccess.thecvf.com/content/CVPR2022/papers/Li_Modality-Agnostic_Learning_for_Radar-Lidar_Fusion_in_Vehicle_Detection_CVPR_2022_paper.pdf)

<a name="19"/>

## 19.Neural Architecture Search(神经架构搜索)
* 🐦️[ISNAS-DIP: Image-Specific Neural Architecture Search for Deep Image Prior](https://arxiv.org/abs/2111.15362)<br>:star:[code](https://github.com/ozgurkara99/ISNAS-DIP)
* [Arch-Graph: Acyclic Architecture Relation Predictor for Task-Transferable Neural Architecture Search](https://arxiv.org/abs/2204.05941)<br>:star:[code](https://github.com/Centaurus982034/Arch-Graph):newspaper:[解读](https://zhuanlan.zhihu.com/p/497769133)
* [GPUNet: Searching the Deployable Convolution Neural Networks for GPUs](https://arxiv.org/abs/2205.00841)<br>神经架构搜索，面向GPUs部署的轻量级网络结构搜索 （比谷歌EfficientNet-X系列、Meta FBNetV3 速度更快，甚至性能都要好，作者来自英伟达） 
* [Distribution Consistent Neural Architecture Search](https://openaccess.thecvf.com/content/CVPR2022/papers/Pan_Distribution_Consistent_Neural_Architecture_Search_CVPR_2022_paper.pdf)
* [Performance-Aware Mutual Knowledge Distillation for Improving Neural Architecture Search](https://openaccess.thecvf.com/content/CVPR2022/papers/Xie_Performance-Aware_Mutual_Knowledge_Distillation_for_Improving_Neural_Architecture_Search_CVPR_2022_paper.pdf)
* [BaLeNAS: Differentiable Architecture Search via the Bayesian Learning Rule](https://arxiv.org/abs/2111.13204)
* [GreedyNASv2: Greedier Search With a Greedy Path Filter](https://arxiv.org/abs/2111.12609)

<a name="18"/>

## 18.Person Re-Identification(人员重识别)
* 组重识别
  * [Modeling 3D Layout for Group Re-Identification](https://openaccess.thecvf.com/content/CVPR2022/papers/Zhang_Modeling_3D_Layout_for_Group_Re-Identification_CVPR_2022_paper.pdf)<br>:star:[code](https://github.com/LinlyAC/City1M-dataset)
* Reid
  * [Part-based Pseudo Label Refinement for Unsupervised Person Re-identification](https://arxiv.org/abs/2203.14675)<br>:star:[code](https://github.com/yoonkicho/PPLR)
  * [Camera-Conditioned Stable Feature Generation for Isolated Camera Supervised Person Re-IDentification](https://arxiv.org/abs/2203.15210)
  * [FMCNet: Feature-Level Modality Compensation for Visible-Infrared Person Re-Identification](https://openaccess.thecvf.com/content/CVPR2022/papers/Zhang_FMCNet_Feature-Level_Modality_Compensation_for_Visible-Infrared_Person_Re-Identification_CVPR_2022_paper.pdf)
  * [Large-Scale Pre-training for Person Re-identification with Noisy Labels](https://arxiv.org/abs/2203.16533)<br>:star:[code](https://github.com/DengpanFu/LUPerson-NL)
  * [Cloning Outfits from Real-World Images to 3D Characters for Generalizable Person Re-Identification](https://arxiv.org/abs/2204.02611)<br>:star:[code](https://github.com/Yanan-Wang-cs/ClonedPerson)
  * [Implicit Sample Extension for Unsupervised Person Re-Identification](https://arxiv.org/abs/2204.06892)<br>:star:[code](https://github.com/PaddlePaddle/PaddleClas):newspaper:[解读](https://zhuanlan.zhihu.com/p/498883232)
  * [Graph Sampling Based Deep Metric Learning for Generalizable Person Re-Identification](https://arxiv.org/abs/2104.01546)<br>:star:[code](https://github.com/ShengcaiLiao/QAConv)
  * [NFormer: Robust Person Re-identification with Neighbor Transformer](https://arxiv.org/abs/2204.09331)<br>:star:[code](https://github.com/haochenheheda/NFormer):newspaper:[解读](https://zhuanlan.zhihu.com/p/502240740)
  * [Dual Cross-Attention Learning for Fine-Grained Visual Categorization and Object Re-Identification](https://arxiv.org/abs/2205.02151)
  * [Unleashing Potential of Unsupervised Pre-Training With Intra-Identity Regularization for Person Re-Identification](https://openaccess.thecvf.com/content/CVPR2022/papers/Yang_Unleashing_Potential_of_Unsupervised_Pre-Training_With_Intra-Identity_Regularization_for_Person_CVPR_2022_paper.pdf)
  * [Learning With Twin Noisy Labels for Visible-Infrared Person Re-Identification](https://openaccess.thecvf.com/content/CVPR2022/papers/Yang_Learning_With_Twin_Noisy_Labels_for_Visible-Infrared_Person_Re-Identification_CVPR_2022_paper.pdf)<br>:star:[code](https://github.com/XLearning-SCU/2022-CVPR-DART)
  * [Lifelong Unsupervised Domain Adaptive Person Re-Identification With Coordinated Anti-Forgetting and Adaptation](https://arxiv.org/abs/2112.06632)<br>:house:[project](https://iccv2021-mmp.github.io/subpage/dataset.html)
  * [Learning Memory-Augmented Unidirectional Metrics for Cross-Modality Person Re-Identification](https://openaccess.thecvf.com/content/CVPR2022/papers/Liu_Learning_Memory-Augmented_Unidirectional_Metrics_for_Cross-Modality_Person_Re-Identification_CVPR_2022_paper.pdf)
  * [Augmented Geometric Distillation for Data-Free Incremental Person ReID](https://openaccess.thecvf.com/content/CVPR2022/papers/Lu_Augmented_Geometric_Distillation_for_Data-Free_Incremental_Person_ReID_CVPR_2022_paper.pdf)<br>:star:[code](https://github.com/eddielyc/Augmented-Geometric-Distillation)
  * [Salient-to-Broad Transition for Video Person Re-Identification](https://openaccess.thecvf.com/content/CVPR2022/papers/Bai_Salient-to-Broad_Transition_for_Video_Person_Re-Identification_CVPR_2022_paper.pdf)<br>:star:[code](https://github.com/baist/SINet)
  * 换装行人重识别
    * [Clothes-Changing Person Re-identification with RGB Modality Only](https://arxiv.org/abs/2204.06890)<br>:star:[code](https://github.com/guxinqian/Simple-CCReID):newspaper:[解读](https://zhuanlan.zhihu.com/p/498883232)
  * 遮挡行人重识别
    * [Feature Erasing and Diffusion Network for Occluded Person Re-Identification](https://arxiv.org/abs/2112.08740)
* 人群计数
  * [Leveraging Self-Supervision for Cross-Domain Crowd Counting](https://arxiv.org/abs/2103.16291)
  * [Boosting Crowd Counting via Multifaceted Attention](https://arxiv.org/abs/2203.02636)<br>:star:[code](https://github.com/LoraLinH/Boosting-Crowd-Counting-via-Multifaceted-Attention)
  * [Bi-level Alignment for Cross-Domain Crowd Counting](https://arxiv.org/abs/2205.05844)<br>:star:[code](https://github.com/Yankeegsj/BLA)<br>:newspaper:[解读](https://zhuanlan.zhihu.com/p/513702674)
  * [Crowd Counting in the Frequency Domain](https://openaccess.thecvf.com/content/CVPR2022/papers/Shu_Crowd_Counting_in_the_Frequency_Domain_CVPR_2022_paper.pdf)
* 行人检测
  * [STCrowd: A Multimodal Dataset for Pedestrian Perception in Crowded Scenes](https://arxiv.org/abs/2204.01026)<br>:star:[code](https://github.com/4DVLab/STCrowd)
* 步态识别
  * [Gait Recognition in the Wild with Dense 3D Representations and A Benchmark](https://arxiv.org/abs/2204.02569)<br>:star:[code](https://github.com/Gait3D/Gait3D-Benchmark):house:[project](https://gait3d.github.io/)<br>:newspaper:[解读](https://mp.weixin.qq.com/s/lSY1is6Fmm6A0Db0Jxo4qg)
* Person Search
  * [PSTR: End-to-End One-Step Person Search With Transformers](https://arxiv.org/abs/2204.03340)<br>:star:[code](https://github.com/JialeCao001/PSTR)


<a name="17"/>

## 17.Medical Image(医学影像)
* [Temporal Context Matters: Enhancing Single Image Prediction with Disease Progression Representations](https://arxiv.org/abs/2203.01933)<br>:open_mouth:oral
* [BoostMIS: Boosting Medical Image Semi-supervised Learning with Adaptive Pseudo Labeling and Informative Active Annotation](https://arxiv.org/abs/2203.02533)<br>:star:[code](https://github.com/wannature/BoostMIS)
* [DeepLIIF: An Online Platform for Quantification of Clinical Pathology Slides](https://arxiv.org/abs/2204.04494)
* [DiRA: Discriminative, Restorative, and Adversarial Learning for Self-supervised Medical Image Analysis](https://arxiv.org/abs/2204.10437)<br>:star:[code](https://github.com/fhaghighi/DiRA):newspaper:[解读](https://zhuanlan.zhihu.com/p/504499515)
* [Surpassing the Human Accuracy: Detecting Gallbladder Cancer from USG Images with Curriculum Learning](https://arxiv.org/abs/2204.11433)<br>:star:[code](https://github.com/sbasu276/GBCNet):house:[project](https://gbc-iitd.github.io/gbcnet)
* [What Makes Transfer Learning Work for Medical Images: Feature Reuse & Other Factors](https://arxiv.org/abs/2203.01825)
* 3D生物打印
  * [Generating 3D Bio-Printable Patches Using Wound Segmentation and Reconstruction to Treat Diabetic Foot Ulcers](https://arxiv.org/abs/2203.03814)<br>利用伤口分割和重建生成3D生物打印贴片来治疗糖尿病足溃疡
* SR（ＭRI）
  * [Transformer-empowered Multi-scale Contextual Matching and Aggregation for Multi-contrast MRI Super-resolution](https://arxiv.org/abs/2203.13963)<br>:star:[code](https://github.com/XAIMI-Lab/McMRSR)
* 医学图像分割
  * [CycleMix: A Holistic Strategy for Medical Image Segmentation From Scribble Supervision](https://arxiv.org/abs/2203.01475)<br>:star:[code](https://github.com/BWGZK/CycleMix)
  * [C-CAM: Causal CAM for Weakly Supervised Semantic Segmentation on Medical Image](https://openaccess.thecvf.com/content/CVPR2022/papers/Chen_C-CAM_Causal_CAM_for_Weakly_Supervised_Semantic_Segmentation_on_Medical_CVPR_2022_paper.pdf)<br>:star:[code](https://github.com/Tian-lab/C-CAM)
  * [HyperSegNAS: Bridging One-Shot Neural Architecture Search With 3D Medical Image Segmentation Using HyperNet](https://arxiv.org/abs/2112.10652)
* 医学图像配准
  * [Affine Medical Image Registration with Coarse-to-Fine Vision Transformer](https://arxiv.org/abs/2203.15216)<br>:star:[code](https://github.com/cwmok/C2FViT)
* 医学图像分析
  * [FIBA: Frequency-Injection based Backdoor Attack in Medical Image Analysis](https://arxiv.org/abs/2112.01148)<br>:star:[code](https://github.com/HazardFY/FIBA):newspaper:[解读](https://mp.weixin.qq.com/s/lSY1is6Fmm6A0Db0Jxo4qg)
* 自动生成报告
  * [Cross-modal Clinical Graph Transformer for Ophthalmic Report Generation](https://arxiv.org/abs/2206.01988)
* 医学图像分类
  * [ACPL: Anti-Curriculum Pseudo-Labelling for Semi-Supervised Medical Image Classification](https://arxiv.org/abs/2111.12918)<br>:star:[code](https://github.com/FBLADL/ACPL)
* CT合成
  * [Incremental Cross-View Mutual Distillation for Self-Supervised Medical CT Synthesis](https://arxiv.org/abs/2112.10325)
* 医学影像关键点检测
  * [Which Images To Label for Few-Shot Medical Landmark Detection?](https://arxiv.org/abs/2112.04386)
* MRI
  * [Vox2Cortex: Fast Explicit Reconstruction of Cortical Surfaces From 3D MRI Scans With Geometric Deep Neural Networks](https://openaccess.thecvf.com/content/CVPR2022/papers/Bongratz_Vox2Cortex_Fast_Explicit_Reconstruction_of_Cortical_Surfaces_From_3D_MRI_CVPR_2022_paper.pdf)<br>:star:[code](https://github.com/ai-med/Vox2Cortex)
  
<a name="16"/>

## 16.Semi/self-supervised learning(半/自监督)
* 自监督
  * [A study on the distribution of social biases in self-supervised learning visual models](https://arxiv.org/abs/2203.01854)<br>:star:[code](https://github.com/vpulab/SB-SSL)
  * [Learning Where to Learn in Cross-View Self-Supervised Learning](https://arxiv.org/abs/2203.14898)
  * [Leverage Your Local and Global Representations: A New Self-Supervised Learning Strategy](https://arxiv.org/abs/2203.17205)
  * [DATA: Domain-Aware and Task-Aware Self-Supervised Learning](https://arxiv.org/abs/2203.09041)<br>:star:[code](https://github.com/GAIA-vision/GAIA-ssl)
  * [Contextualized Spatio-Temporal Contrastive Learning With Self-Supervision](https://arxiv.org/abs/2112.05181)<br>:star:[code](https://github.com/tensorflow/models/tree/master/official/projects/const_cl)
  * [Self-Supervised Models Are Continual Learners](https://openaccess.thecvf.com/content/CVPR2022/papers/Fini_Self-Supervised_Models_Are_Continual_Learners_CVPR_2022_paper.pdf)<br>:star:[code](https://github.com/DonkeyShot21/cassle)
  * [Learning Pixel Trajectories With Multiscale Contrastive Random Walks](https://arxiv.org/abs/2201.08379)<br>:star:[code](https://github.com/jasonbian97/flowwalk):house:[project](https://jasonbian97.github.io/flowwalk/)
  * [Locality-Aware Inter- and Intra-Video Reconstruction for Self-Supervised Correspondence Learning](https://arxiv.org/abs/2203.14333)<br>:star:[code](https://github.com/lingorX/LIIR)
  * [Backdoor Attacks on Self-Supervised Learning](https://arxiv.org/abs/2105.10123)<br>:star:[code](https://github.com/UMBCvision/SSL-Backdoor)
  * [Neural Shape Mating: Self-Supervised Object Assembly With Adversarial Shape Priors](https://arxiv.org/abs/2205.14886)<br>:house:[project](https://neural-shape-mating.github.io/)
  * [Masked Feature Prediction for Self-Supervised Visual Pre-Training](https://arxiv.org/abs/2112.09133)<br>:star:[code](https://github.com/facebookresearch/pytorchvideo)
  * [Semantic-Aware Auto-Encoders for Self-Supervised Representation Learning](https://openaccess.thecvf.com/content/CVPR2022/papers/Wang_Semantic-Aware_Auto-Encoders_for_Self-Supervised_Representation_Learning_CVPR_2022_paper.pdf)<br>:star:[code](https://github.com/wanggrun/Semantic-Aware-AE)
  * [Patch-Level Representation Learning for Self-Supervised Vision Transformers](https://openaccess.thecvf.com/content/CVPR2022/papers/Yun_Patch-Level_Representation_Learning_for_Self-Supervised_Vision_Transformers_CVPR_2022_paper.pdf)
  * [A Simple Data Mixing Prior for Improving Self-Supervised Learning](https://openaccess.thecvf.com/content/CVPR2022/papers/Ren_A_Simple_Data_Mixing_Prior_for_Improving_Self-Supervised_Learning_CVPR_2022_paper.pdf)<br>:star:[code](https://github.com/OliverRensu/SDMP)
  * [Sound and Visual Representation Learning With Multiple Pretraining Tasks](https://arxiv.org/abs/2201.01046)
* 无监督
  * [RIM-Net: Recursive Implicit Fields for Unsupervised Learning of Hierarchical Shape Structures](https://openaccess.thecvf.com/content/CVPR2022/papers/Niu_RIM-Net_Recursive_Implicit_Fields_for_Unsupervised_Learning_of_Hierarchical_Shape_CVPR_2022_paper.pdf)
  * [RM-Depth: Unsupervised Learning of Recurrent Monocular Depth in Dynamic Scenes](https://openaccess.thecvf.com/content/CVPR2022/papers/Hui_RM-Depth_Unsupervised_Learning_of_Recurrent_Monocular_Depth_in_Dynamic_Scenes_CVPR_2022_paper.pdf)<br>:star:[code](https://github.com/twhui/RM-Depth)
  * [Harmony: A Generic Unsupervised Approach for Disentangling Semantic Content From Parameterized Transformations](https://openaccess.thecvf.com/content/CVPR2022/papers/Uddin_Harmony_A_Generic_Unsupervised_Approach_for_Disentangling_Semantic_Content_From_CVPR_2022_paper.pdf)
* 半监督
  * [Class-Aware Contrastive Semi-Supervised Learning](https://arxiv.org/abs/2203.02261)<br>:star:[code](https://github.com/TencentYoutuResearch/Classification-SemiCLS)<br>:newspaper:[解读](https://mp.weixin.qq.com/s/FYn0S46OA6xraH9ofNIctw)
  * [RSCFed: Random Sampling Consensus Federated Semi-supervised Learning](https://arxiv.org/abs/2203.13993)<br>:star:[code](https://github.com/XMed-Lab/RSCFed)
  * [FisherMatch: Semi-Supervised Rotation Regression via Entropy-based Filtering](https://arxiv.org/abs/2203.15765)<br>:open_mouth:oral:house:[project](https://yd-yin.github.io/FisherMatch/)
  * [Semi-Supervised Learning of Semantic Correspondence with Pseudo-Labels](https://arxiv.org/abs/2203.16038)
  * [SimMatch: Semi-Supervised Learning With Similarity Matching](https://arxiv.org/abs/2203.06915)<br>:star:[code](https://github.com/KyleZheng1997/simmatch)
  * [CoSSL: Co-Learning of Representation and Classifier for Imbalanced Semi-Supervised Learning](https://arxiv.org/abs/2112.04564)<br>:star:[code](https://github.com/YUE-FAN/CoSSL)
  * [DASO: Distribution-Aware Semantics-Oriented Pseudo-Label for Imbalanced Semi-Supervised Learning](https://arxiv.org/abs/2106.05682)<br>:star:[code](https://github.com/ytaek-oh/daso):house:[project](https://ytaek-oh.github.io/daso)
  * [Semi-Weakly-Supervised Learning of Complex Actions From Instructional Task Videos](https://openaccess.thecvf.com/content/CVPR2022/papers/Shen_Semi-Weakly-Supervised_Learning_of_Complex_Actions_From_Instructional_Task_Videos_CVPR_2022_paper.pdf)<br>:star:[code](https://github.com/Yuhan-Shen/SWSL)
* 弱监督
  * [P3IV: Probabilistic Procedure Planning from Instructional Videos with Weak Supervision](https://arxiv.org/abs/2205.02300)<br>:star:[code](https://github.com/SamsungLabs/procedure-planning)<br>使用教学视频进行概率性程序规划的弱监督方法
  * [Revisiting Weakly Supervised Pre-Training of Visual Perception Models](https://arxiv.org/abs/2201.08371)<br>:star:[code](https://github.com/facebookresearch/SWAG)
  * [Automatic Synthesis of Diverse Weak Supervision Sources for Behavior Analysis](https://arxiv.org/abs/2111.15186)<br>:star:[code](https://github.com/autoswap/autoswap_cvpr_2022)
  * [Decoupling Makes Weakly Supervised Local Feature Better](https://arxiv.org/abs/2201.02861)<br>:star:[code](https://github.com/The-Learning-And-Vision-Atelier-LAVA/PoSFeat)

<a name="15"/>

## 15.Transformer
* [Vision Transformer With Deformable Attention](https://arxiv.org/abs/2201.00520)<br>:star:[code](https://github.com/LeapLabTHU/DAT)
* [Delving Deep Into the Generalization of Vision Transformers Under Distribution Shifts](https://arxiv.org/abs/2106.07617)<br>:star:[code](https://github.com/Phoenix1153/ViT_OOD_generalization)
* [APRIL: Finding the Achilles' Heel on Privacy for Vision Transformers](https://arxiv.org/abs/2112.14087)
* [Fast Point Transformer](https://arxiv.org/abs/2112.04702)<br>:star:[code](https://github.com/POSTECH-CVLab/FastPointTransformer)
* [ChiTransformer:Towards Reliable Stereo from Cues](https://arxiv.org/abs/2203.04554)
* [Beyond Fixation: Dynamic Window Visual Transformer](https://arxiv.org/abs/2203.12856)<br>:star:[code](https://github.com/pzhren/DW-ViT)      
* [Training-free Transformer Architecture Search](https://arxiv.org/abs/2203.12217)<br>:newspaper:[解读](https://mp.weixin.qq.com/s/FYn0S46OA6xraH9ofNIctw)
* [Automated Progressive Learning for Efficient Training of Vision Transformers](https://arxiv.org/abs/2203.14509)<br>:star:[code](https://github.com/changlin31/AutoProg)
* [Collaborative Transformers for Grounded Situation Recognition](https://arxiv.org/abs/2203.16518)<br>:star:[code](https://github.com/jhcho99/CoFormer)
* [TubeDETR: Spatio-Temporal Video Grounding with Transformers](https://arxiv.org/abs/2203.16434)<br>:open_mouth:oral:star:[code](https://github.com/antoyang/TubeDETR):house:[project](https://antoyang.github.io/tubedetr.html)
* [Deformable Video Transformer](https://arxiv.org/abs/2203.16795)
* [MixFormer: Mixing Features across Windows and Dimensions](https://arxiv.org/abs/2204.02557)<br>:open_mouth:oral:star:[code](https://github.com/PaddlePaddle/PaddleClas):newspaper:[粗解](https://zhuanlan.zhihu.com/p/494237617)
* [Are Multimodal Transformers Robust to Missing Modality?](https://arxiv.org/abs/2204.05454)
* [MiniViT: Compressing Vision Transformers with Weight Multiplexing](https://arxiv.org/abs/2204.07154)
* [Multimodal Token Fusion for Vision Transformers](https://arxiv.org/abs/2204.08721)<br>:star:[code](https://github.com/huawei-noah/noah-research)
* [Not All Tokens Are Equal: Human-centric Visual Analysis via Token Clustering Transformer](https://arxiv.org/abs/2204.08680)<br>:open_mouth:oral:star:[code](https://github.com/zengwang430521/TCFormer):newspaper:[解读](https://zhuanlan.zhihu.com/p/501585273)
* [UTC: A Unified Transformer with Inter-Task Contrastive Learning for Visual Dialog](https://arxiv.org/abs/2205.00423)<br>对比学习用于视觉对话的统一Transformer架构
* [Patch Slimming for Efficient Vision Transformers](https://arxiv.org/abs/2106.02852)<br>:newspaper:[解读](https://mp.weixin.qq.com/s/lSY1is6Fmm6A0Db0Jxo4qg)
* [Swin Transformer V2: Scaling Up Capacity and Resolution](https://arxiv.org/abs/2111.09883)<br>:star:[code](https://github.com/microsoft/Swin-Transformer)<br>:newspaper:[大大刷新记录！Swin Transformer v2.0 来了，30亿参数！](https://mp.weixin.qq.com/s/fmWiwIscYX3fgFKmQwsRiQ)
* [SimMIM: A Simple Framework for Masked Image Modeling](https://arxiv.org/abs/2111.09886)<br>:star:[code](https://github.com/microsoft/SimMIM)
* [NomMer: Nominate Synergistic Context in Vision Transformer for Visual Recognition](https://arxiv.org/abs/2111.12994)<br>:star:[code](https://github.com/TencentYoutuResearch/VisualRecognition-NomMer)<br>:newspaper:[解读](https://mp.weixin.qq.com/s/FYn0S46OA6xraH9ofNIctw)
* [Mobile-Former: Bridging MobileNet and Transformer](https://arxiv.org/abs/2108.05895)<br>:star:[code](https://github.com/aaboys/mobileformer)
* [MulT: An End-to-End Multitask Learning Transformer](https://arxiv.org/abs/2205.08303)<br>:star:[code](https://github.com/IVRL/MulT):house:[project](https://ivrl.github.io/MulT/)
* [Scaling Vision Transformers to Gigapixel Images via Hierarchical Self-Supervised Learning](https://arxiv.org/abs/2206.02647)<br>:open_mouth:oral:star:[code](https://github.com/mahmoodlab/HIPT):newspaper:[解读](https://zhuanlan.zhihu.com/p/525331776)
* [CodedVTR: Codebook-Based Sparse Voxel Transformer With Geometric Guidance](https://arxiv.org/abs/2203.09887)
* [MSG-Transformer: Exchanging Local Spatial Information by Manipulating Messenger Tokens](https://openaccess.thecvf.com/content/CVPR2022/papers/Fang_MSG-Transformer_Exchanging_Local_Spatial_Information_by_Manipulating_Messenger_Tokens_CVPR_2022_paper.pdf)<br>:star:[code](https://github.com/hustvl/MSG-Transformer)
* [IRISformer: Dense Vision Transformers for Single-Image Inverse Rendering in Indoor Scenes](https://openaccess.thecvf.com/content/CVPR2022/papers/Zhu_IRISformer_Dense_Vision_Transformers_for_Single-Image_Inverse_Rendering_in_Indoor_CVPR_2022_paper.pdf)<br>:star:[code](https://github.com/ViLab-UCSD/IRISformer)
* [Reversible Vision Transformers](https://openaccess.thecvf.com/content/CVPR2022/papers/Mangalam_Reversible_Vision_Transformers_CVPR_2022_paper.pdf)<br>:star:[code](https://github.com/facebookresearch/mvit)
* [MetaFormer Is Actually What You Need for Vision](https://arxiv.org/abs/2111.11418)<br>:open_mouth:oral:star:[code](https://github.com/sail-sg/poolformer)
* [GradViT: Gradient Inversion of Vision Transformers](https://arxiv.org/abs/2203.11894)<br>:house:[project](https://gradvit.github.io/)
* [CSWin Transformer: A General Vision Transformer Backbone With Cross-Shaped Windows](https://arxiv.org/abs/2107.00652)<br>:star:[code](https://github.com/microsoft/CSWin-Transformer)
* [MViTv2: Improved Multiscale Vision Transformers for Classification and Detection](https://arxiv.org/abs/2112.01526)<br>:star:[code](https://github.com/facebookresearch/mvit)<br>:newspaper:[Meta&伯克利基于池化自注意力机制提出通用多尺度视觉Transformer，在ImageNet分类准确率达88.8%！开源](https://mp.weixin.qq.com/s/cfvDmALkPFvpXtHsUEX8zw)
* [A-ViT: Adaptive Tokens for Efficient Vision Transformer](https://arxiv.org/pdf/2112.07658.pdf)<br>:open_mouth:oral<br>:newspaper:[不重要的token可以提前停止计算！英伟达提出自适应token的高效视觉Transformer网络A-ViT，提高模型的吞吐量！](https://mp.weixin.qq.com/s/uifaPmPc5CBQKgwffQPb3Q)
* [Certified Patch Robustness via Smoothed Vision Transformers](https://arxiv.org/abs/2110.07719)<br>:star:[code](https://github.com/MadryLab/smoothed-vit)
* [The Principle of Diversity: Training Stronger Vision Transformers Calls for Reducing All Levels of Redundancy](https://arxiv.org/abs/2203.06345)<br>:star:[code](https://github.com/VITA-Group/Diverse-ViT)
* [Bootstrapping ViTs: Towards Liberating Vision Transformers From Pre-Training](https://arxiv.org/abs/2112.03552)<br>:star:[code](https://github.com/zhfeing/Bootstrapping-ViTs-pytorch)
* [Object-Region Video Transformers](https://openaccess.thecvf.com/content/CVPR2022/papers/Herzig_Object-Region_Video_Transformers_CVPR_2022_paper.pdf)<br>:star:[code](https://github.com/roeiherz/ORViT):house:[project](https://roeiherz.github.io/ORViT/)
* 形状补全
  * [ShapeFormer: Transformer-based Shape Completion via Sparse Representation](https://arxiv.org/abs/2201.10326)<br>:star:[code](https://github.com/qheldiv/shapeformer):house:[project](https://shapeformer.github.io/)


<a name="14"/>

## 14.Video
* [Stochastic Backpropagation: A Memory Efficient Strategy for Training Video Models](https://arxiv.org/abs/2203.16755)<br>:open_mouth:oral
* [Improving Video Model Transfer With Dynamic Representation Learning](https://openaccess.thecvf.com/content/CVPR2022/papers/Li_Improving_Video_Model_Transfer_With_Dynamic_Representation_Learning_CVPR_2022_paper.pdf)
* 动作分割
  * [Unsupervised Activity Segmentation by Joint Representation Learning and Online Clustering](https://arxiv.org/pdf/2105.13353.pdf)<br>:tv:[video](https://www.youtube.com/watch?v=i4Fh_3nzzUI)
  * [Weakly-Supervised Online Action Segmentation in Multi-View Instructional Videos](https://arxiv.org/abs/2203.13309)
* 动作理解
  * [How Do You Do It? Fine-Grained Action Understanding with Pseudo-Adverbs](https://arxiv.org/abs/2203.12344)
  * [Bridge-Prompt: Towards Ordinal Action Understanding in Instructional Videos](https://arxiv.org/abs/2203.14104)<br>:star:[code](https://github.com/ttlmh/Bridge-Prompt)
* Video Copy Detection(视频拷贝检测)
  * [A Large-scale Comprehensive Dataset and Copy-overlap Aware Evaluation Protocol for Segment-level Video Copy Detection](https://arxiv.org/abs/2203.02654)<br>:star:[code](https://github.com/alipay/VCSL)
* 视频合成  
  * [Show Me What and Tell Me How: Video Synthesis via Multimodal Conditioning](https://arxiv.org/abs/2203.02573)<br>:star:[code](https://github.com/snap-research/MMVID)
  * [Playable Environments: Video Manipulation in Space and Time](https://openaccess.thecvf.com/content/CVPR2022/papers/Menapace_Playable_Environments_Video_Manipulation_in_Space_and_Time_CVPR_2022_paper.pdf)<br>:star:[code](https://github.com/willi-menapace/PlayableEnvironments):house:[project](https://willi-menapace.github.io/playable-environments-website/)
  * [3D Moments from Near-Duplicate Photos](https://arxiv.org/abs/2205.06255)<br>:house:[project](https://3d-moments.github.io/)
* 视频异常检测
  * [Generative Cooperative Learning for Unsupervised Video Anomaly Detection](https://arxiv.org/abs/2203.03962)
  * [Bayesian Nonparametric Submodular Video Partition for Robust Anomaly Detection](https://arxiv.org/abs/2203.12840)  
  * [Deep Anomaly Discovery From Unlabeled Videos via Normality Advantage and Self-Paced Refinement](https://openaccess.thecvf.com/content/CVPR2022/papers/Yu_Deep_Anomaly_Discovery_From_Unlabeled_Videos_via_Normality_Advantage_and_CVPR_2022_paper.pdf)
* 视频监控
  * 轨迹预测
    * [How many Observations are Enough? Knowledge Distillation for Trajectory Forecasting](https://arxiv.org/abs/2203.04781)
    * [Stochastic Trajectory Prediction via Motion Indeterminacy Diffusion](https://arxiv.org/abs/2203.13777)<br>:star:[code](https://github.com/gutianpei/MID)
    * [Non-Probability Sampling Network for Stochastic Human Trajectory Prediction](https://arxiv.org/abs/2203.13471)<br>:star:[code](https://github.com/inhwanbae/NPSN)
* 视频时刻检索和视频高光检测
  * [UMT: Unified Multi-modal Transformers for Joint Video Moment Retrieval and Highlight Detection](https://arxiv.org/abs/2203.12745)<br>:star:[code](https://github.com/TencentARC/UMT)
  * [Learning Pixel-Level Distinctions for Video Highlight Detection](https://arxiv.org/abs/2204.04615)
  * [Contrastive Learning for Unsupervised Video Highlight Detection](https://openaccess.thecvf.com/content/CVPR2022/papers/Badamdorj_Contrastive_Learning_for_Unsupervised_Video_Highlight_Detection_CVPR_2022_paper.pdf)<br>:star:[code](https://github.com/tkbadamdorj/CHD)
* 视频时刻检索
  * [AxIoU: An Axiomatically Justified Measure for Video Moment Retrieval](https://arxiv.org/abs/2203.16062)
* 视频预测
  * [STRPM: A Spatiotemporal Residual Predictive Model for High-Resolution Video Prediction](https://arxiv.org/abs/2203.16084)
  * [Continual Predictive Learning from Videos](https://arxiv.org/abs/2204.05624)<br>:open_mouth:oral:star:[code](https://github.com/jc043/CPL)
  * [SimVP: Simpler yet Better Video Prediction](https://arxiv.org/abs/2206.05099)<br>:star:[code](https://github.com/gaozhangyang/SimVP-Simpler-yet-Better-Video-Prediction):newspaper:[解读](https://zhuanlan.zhihu.com/p/528028523)
  * [Comparing Correspondences: Video Prediction With Correspondence-Wise Losses](https://arxiv.org/abs/2104.09498)<br>:star:[code](https://github.com/dangeng/CorrWiseLosses):house:[project](https://dangeng.github.io/CorrWiseLosses/)
* 视频个体计数
  * [DR.VIC: Decomposition and Reasoning for Video Individual Counting](https://arxiv.org/abs/2203.12335)<br>:star:[code](https://github.com/taohan10200/DRNet)
* 视频插值
  * [Many-to-many Splatting for Efficient Video Frame Interpolation](https://arxiv.org/abs/2204.03513)<br>:star:[code](https://github.com/feinanshan/M2M_VFI)
  * [TimeReplayer: Unlocking the Potential of Event Cameras for Video Interpolation](https://arxiv.org/abs/2203.13859)<br>:house:[project](https://sites.google.com/view/timereplayer/)
  * [Long-term Video Frame Interpolation via Feature Propagation](https://arxiv.org/abs/2203.15427)
  * [Time Lens++: Event-based Frame Interpolation with Parametric Non-linear Flow and Multi-scale Fusion](https://arxiv.org/abs/2203.17191)
* 视觉对应（视频）
  * [Locality-Aware Inter-and Intra-Video Reconstruction for Self-Supervised Correspondence Learning](https://arxiv.org/abs/2203.14333)<br>:star:[code](https://github.com/0liliulei/LIIR)
* 视频识别
  * [BEVT: BERT Pretraining of Video Transformers](https://arxiv.org/abs/2112.01529)<br>:star:[code](https://github.com/xyzforever/BEVT)<br>:newspaper:[视频Transformer自监督预训练新范式，复旦、微软云AI实现视频识别新SOTA](https://mp.weixin.qq.com/s/9e_VyJZlKw8xk3t_z0mFtA)
  * [MLP-3D: A MLP-like 3D Architecture with Grouped Time Mixing](https://arxiv.org/abs/2206.06292)<br>:star:[code](https://github.com/ZhaofanQiu/MLP-3D):newspaper:[解读](https://zhuanlan.zhihu.com/p/528607351)
  * [MeMViT: Memory-Augmented Multiscale Vision Transformer for Efficient Long-Term Video Recognition](https://arxiv.org/abs/2201.08383)
  * [Multiview Transformers for Video Recognition](https://arxiv.org/abs/2201.04288)<br>:star:[code](https://github.com/google-research/scenic/tree/main/scenic/projects/mtv)
* 视频分类
  * 零样本视频分类
    * [Alignment-Uniformity aware Representation Learning for Zero-shot Video Classification](https://arxiv.org/abs/2203.15381)<br>:star:[code](https://github.com/ShipuLoveMili/CVPR2022-AURL)
  * 视频动作分类
    * [Learning To Recognize Procedural Activities With Distant Supervision](https://arxiv.org/abs/2201.10990)<br>:star:[code](https://github.com/facebookresearch/video-distant-supervision)
* 视频预测
  * [Modular Action Concept Grounding in Semantic Video Prediction](https://arxiv.org/abs/2011.11201)<br>:house:[project](http://www.pair.toronto.edu/mac/)
  * 手部动作预测
    * [Joint Hand Motion and Interaction Hotspots Prediction from Egocentric Videos](https://arxiv.org/abs/2204.01696)<br>:house:[project](https://stevenlsw.github.io/hoi-forecast/):tv:[video](https://www.youtube.com/watch?v=uCUTK9WOhpE)
* 视频分割
  * [Modeling Motion with Multi-Modal Features for Text-Based Video Segmentation](https://arxiv.org/abs/2204.02547)<br>:star:[code](https://github.com/wangbo-zhao/2022CVPR-MMMMTBVS)
  * VSS
    * [Scene Consistency Representation Learning for Video Scene Segmentation](https://arxiv.org/abs/2205.05487)<br>:star:[code](https://github.com/TencentYoutuResearch/SceneSegmentation-SCRL)<br>:newspaper:[解读1](https://zhuanlan.zhihu.com/p/513130382)<br>:newspaper:[解读2](https://mp.weixin.qq.com/s/FYn0S46OA6xraH9ofNIctw)
  * VOS
    * [Recurrent Dynamic Embedding for Video Object Segmentation](https://arxiv.org/abs/2205.03761)<br>:star:[code](https://github.com/Limingxing00/RDE-VOS-CVPR2022)
    * [Language-Bridged Spatial-Temporal Interaction for Referring Video Object Segmentation](https://arxiv.org/abs/2206.03789)<br>:star:[code](https://github.com/dzh19990407/LBDT):house:[project](https://zhuanlan.zhihu.com/p/526268919)
    * [Accelerating Video Object Segmentation With Compressed Video](https://arxiv.org/abs/2107.12192)<br>:star:[code](https://github.com/kai422/CoVOS)
    * [End-to-End Referring Video Object Segmentation With Multimodal Transformers](https://arxiv.org/abs/2111.14821)<br>:star:[code](https://github.com/mttr2021/MTTR)
    * [HODOR: High-Level Object Descriptors for Object Re-Segmentation in Video Learned From Static Images](https://arxiv.org/abs/2112.09131)<br>:star:[code](https://github.com/Ali2500/HODOR)
    * [SWEM: Towards Real-Time Video Object Segmentation With Sequential Weighted Expectation-Maximization](https://openaccess.thecvf.com/content/CVPR2022/papers/Lin_SWEM_Towards_Real-Time_Video_Object_Segmentation_With_Sequential_Weighted_Expectation-Maximization_CVPR_2022_paper.pdf)
    * [Language As Queries for Referring Video Object Segmentation](https://arxiv.org/abs/2201.00487)<br>:star:[code](https://github.com/wjn922/ReferFormer)
    * [Wnet: Audio-Guided Video Object Segmentation via Wavelet-Based Cross-Modal Denoising Networks](https://openaccess.thecvf.com/content/CVPR2022/papers/Pan_Wnet_Audio-Guided_Video_Object_Segmentation_via_Wavelet-Based_Cross-Modal_Denoising_Networks_CVPR_2022_paper.pdf)<br>:star:[code](https://github.com/asudahkzj/Wnet)
  * 视频实例分割(VIS)
    * [Efficient Video Instance Segmentation via Tracklet Query and Proposal](https://arxiv.org/abs/2203.01853)<br>:house:[project](https://jialianwu.com/projects/EfficientVIS.html):tv:[video](https://youtu.be/sSPMzgtMKCE):newspaper:[粗解](https://zhuanlan.zhihu.com/p/475067096)
    * [Temporally Efficient Vision Transformer for Video Instance Segmentation](https://arxiv.org/abs/2204.08412)<br>:open_mouth:oral:star:[code](https://github.com/hustvl/TeViT):newspaper:[解读](https://zhuanlan.zhihu.com/p/501027339)
    * [VISOLO: Grid-Based Space-Time Aggregation for Efficient Online Video Instance Segmentation](https://arxiv.org/abs/2112.04177)<br>:star:[code](https://github.com/SuHoHan95/VISOLO)
  * 视频语义分割
    * [Coarse-to-Fine Feature Mining for Video Semantic Segmentation](https://arxiv.org/abs/2204.03330)<br>:star:[code](https://github.com/GuoleiSun/VSS-CFFM)
  * 视频全景分割
    * [Video K-Net: A Simple, Strong, and Unified Baseline for Video Segmentation](https://arxiv.org/abs/2204.04656)<br>:open_mouth:oral:star:[code](https://github.com/lxtGH/Video-K-Net):newspaper:[解读](https://zhuanlan.zhihu.com/p/497009845)
* 视频影像处理
  * 视频恢复
    * [Neural Global Shutter: Learn to Restore Video from a Rolling Shutter Camera with Global Reset Feature](https://arxiv.org/abs/2204.00974)<br>:star:[code](https://github.com/lightChaserX/neural-global-shutter)
    * [Neural Compression-Based Feature Learning for Video Restoration](https://arxiv.org/abs/2203.09208)
  * 视频修复
    * [Towards An End-to-End Framework for Flow-Guided Video Inpainting](https://arxiv.org/abs/2204.02663)<br>:star:[code](https://github.com/MCG-NKU/E2FGVI)
    * [The DEVIL Is in the Details: A Diagnostic Evaluation Benchmark for Video Inpainting](https://openaccess.thecvf.com/content/CVPR2022/papers/Szeto_The_DEVIL_Is_in_the_Details_A_Diagnostic_Evaluation_Benchmark_CVPR_2022_paper.pdf)<br>:star:[code](https://github.com/MichiganCOG/devil)
    * [Revisiting Temporal Alignment for Video Restoration](https://arxiv.org/abs/2111.15288)<br>:star:[code](https://github.com/redrock303/Revisiting-Temporal-Alignment-for-Video-Restoration)
  * 视频去摩尔纹
    * [Video Demoireing with Relation-Based Temporal Consistency](https://arxiv.org/abs/2204.02957)<br>:house:[project](https://daipengwa.github.io/VDmoire_ProjectPage/):tv:[video](https://youtu.be/73mCqfWobBo)
  * 视频去模糊
    * [Multi-Scale Memory-Based Video Deblurring](https://arxiv.org/abs/2204.02977)<br>:star:[code](https://github.com/jibo27/MemDeblur)
  * 视频去噪
    * [Dancing under the stars: video denoising in starlight](https://arxiv.org/abs/2204.04210)<br>:star:[code](https://github.com/monakhova/starlight_denoising/)
  * 电影修复
    * [Bringing Old Films Back to Life](https://arxiv.org/abs/2203.17276)<br>:star:[code](https://github.com/raywzy/Bringing-Old-Films-Back-to-Life)
* 视频表征学习
  * [TransRank: Self-supervised Video Representation Learning via Ranking-based Transformation Recognition](https://arxiv.org/abs/2205.02028)<br>:open_mouth:oral:star:[code](https://github.com/kennymckormick/TransRank):newspaper:[解读](https://zhuanlan.zhihu.com/p/509470774/) 
  * [Motion-Aware Contrastive Video Representation Learning via Foreground-Background Merging](https://arxiv.org/abs/2109.15130)<br>:star:[code](https://github.com/Mark12Ding/FAME)
  * [Motion-Adjustable Neural Implicit Video Representation](https://openaccess.thecvf.com/content/CVPR2022/papers/Mai_Motion-Adjustable_Neural_Implicit_Video_Representation_CVPR_2022_paper.pdf)
  * 自监督视频表征学习
    * [Hierarchical Self-supervised Representation Learning for Movie Understanding](https://arxiv.org/abs/2204.03101)<br>:star:[code](https://github.com/alibaba-mmai-research/HiCo):house:[project](https://hico-cvpr2022.github.io/)
    * [Learning from Untrimmed Videos: Self-Supervised Video Representation Learning with Hierarchical Consistency](https://arxiv.org/abs/2204.03017)<br>:star:[code](https://github.com/alibaba-mmai-research/HiCo):house:[project](https://hico-cvpr2022.github.io/)
    * [Cross-Architecture Self-supervised Video Representation Learning](https://arxiv.org/abs/2205.13313)<br>:star:[code](https://github.com/guoshengcv/CACL)<br>:newspaper:[解读](https://zhuanlan.zhihu.com/p/520636472)<br>:newspaper:[不同网络结构的特征也能进行对比学习？蚂蚁&美团&南大&阿里提出跨架构自监督视频表示学习方法CACL，性能SOTA！](https://mp.weixin.qq.com/s/cM6RXGK-M6H57XvTcuT_QA)
  * 视频对比学习
    * [Probabilistic Representations for Video Contrastive Learning](https://arxiv.org/abs/2204.03946)
* 视频分解
  * [Deformable Sprites for Unsupervised Video Decomposition](https://arxiv.org/abs/2204.07151)<br>:open_mouth:oral:house:[project](https://deformable-sprites.github.io/)
* 视频阴影检测
  * [Video Shadow Detection via Spatio-Temporal Interpolation Consistency Training](http://graphvision.whu.edu.cn/papers/2022/LuXiao_CVPR.pdf)<br>:star:[code](https://github.com/yihong-97/STICT)
* 视频帧插值
  * [IFRNet: Intermediate Feature Refine Network for Efficient Frame Interpolation](https://drive.google.com/file/d/1oj4uCqu7E2913JKLohu7nZsj5SvECCdq/view)<br>:star:[code](https://github.com/ltkong218/IFRNet)<br>:newspaper:[解读](https://mp.weixin.qq.com/s/FYn0S46OA6xraH9ofNIctw)
  * [Video Frame Interpolation with Transformer](https://arxiv.org/abs/2205.07230)<br>:star:[code](https://github.com/dvlab-research/VFIformer)<br>:newspaper:[解读](https://zhuanlan.zhihu.com/p/515672497)
  * [Video Frame Interpolation Transformer](https://arxiv.org/abs/2111.13817)<br>:star:[code](https://github.com/zhshi0816/Video-Frame-Interpolation-Transformer)
* 视频理解
  * [Revisiting the "Video" in Video-Language Understanding](https://arxiv.org/abs/2206.01714)<br>:open_mouth:oral:star:[code](https://stanfordvl.github.io/atp-revisit-video-lang/)
  * [Long-Short Temporal Contrastive Learning of Video Transformers](https://arxiv.org/abs/2106.09212)
  * 通用事件边界检测(视频理解)
    * [UBoCo : Unsupervised Boundary Contrastive Learning for Generic Event Boundary Detection](https://arxiv.org/abs/2111.14799)
    * [Progressive Attention on Multi-Level Dense Difference Maps for Generic Event Boundary Detection](https://arxiv.org/abs/2112.04771)<br>:star:[code](https://github.com/MCG-NJU/DDM)
* 视频字幕
  * [End-to-End Generative Pretraining for Multimodal Video Captioning](https://arxiv.org/abs/2201.08264)<br>:newspaper:[谷歌多模态预训练框架：视频字幕、动作分类、问答全部实现SOTA](https://mp.weixin.qq.com/s/HEtYWDPu9gG_TCi137qigQ)
  * [Hierarchical Modular Network for Video Captioning](https://arxiv.org/abs/2111.12476)<br>:star:[code](https://github.com/MarcusNerva/HMN)
  * [SwinBERT: End-to-End Transformers With Sparse Attention for Video Captioning](https://arxiv.org/abs/2111.13196)<br>:star:[code](https://github.com/microsoft/SwinBERT)
* 视频重构
  * [E2V-SDE: From Asynchronous Events to Fast and Continuous Video Reconstruction via Neural Stochastic Differential Equations](https://openaccess.thecvf.com/content/CVPR2022/papers/Kim_E2V-SDE_From_Asynchronous_Events_to_Fast_and_Continuous_Video_Reconstruction_CVPR_2022_paper.pdf)
  * [Context-Aware Video Reconstruction for Rolling Shutter Cameras](https://arxiv.org/abs/2205.12912)<br>:star:[code](https://github.com/GitCVfb/CVR):newspaper:[解读](https://zhuanlan.zhihu.com/p/520070681)
* 视频相似度评估
  * [Tencent-MVSE: A Large-Scale Benchmark Dataset for Multi-Modal Video Similarity Evaluation](https://openaccess.thecvf.com/content/CVPR2022/papers/Zeng_Tencent-MVSE_A_Large-Scale_Benchmark_Dataset_for_Multi-Modal_Video_Similarity_Evaluation_CVPR_2022_paper.pdf)<br>:house:[project](https://tencent-mvse.github.io/)
* 视频摘要
  * [Joint Video Summarization and Moment Localization by Cross-Task Sample Transfer](https://openaccess.thecvf.com/content/CVPR2022/papers/Jiang_Joint_Video_Summarization_and_Moment_Localization_by_Cross-Task_Sample_Transfer_CVPR_2022_paper.pdf)<br>:house:[project](https://code-website.wixsite.com/iptnet)
  * [IntentVizor: Towards Generic Query Guided Interactive Video Summarization](https://arxiv.org/abs/2109.14834)<br>:star:[code](https://github.com/jnzs1836/intent-vizor)
* 视频编解码
  * [OCSampler: Compressing Videos to One Clip With Single-Step Sampling](https://arxiv.org/abs/2201.04388)
  * [Learning Based Multi-Modality Image and Video Compression](https://openaccess.thecvf.com/content/CVPR2022/papers/Lu_Learning_Based_Multi-Modality_Image_and_Video_Compression_CVPR_2022_paper.pdf)
* 视频建模
  * [Stand-Alone Inter-Frame Attention in Video Models](https://openaccess.thecvf.com/content/CVPR2022/papers/Long_Stand-Alone_Inter-Frame_Attention_in_Video_Models_CVPR_2022_paper.pdf)<br>:star:[code](https://github.com/FuchenUSTC/SIFA)<br>:newspaper:[解读](https://mp.weixin.qq.com/s/lSY1is6Fmm6A0Db0Jxo4qg)  

<a name="13"/>

## 13.GAN
* 🐦️[HyperInverter: Improving StyleGAN Inversion via Hypernetwork](http://arxiv.org/abs/2112.00719)<br>:house:[project](https://di-mi-ta.github.io/HyperInverter/)
* [InsetGAN for Full-Body Image Generation](https://arxiv.org/abs/2203.07293)<br>:house:[project](https://afruehstueck.github.io/insetgan/)<br>:newspaper:[1024x1024 分辨率，效果惊人！InsetGAN：全身图像生成](https://mp.weixin.qq.com/s/VguklTTYePqYa4_KQhDuuA)
* [Commonality in Natural Images Rescues GANs: Pretraining GANs with Generic and Privacy-free Synthetic Data](https://arxiv.org/abs/2204.04950)<br>:star:[code](https://github.com/FriedRonaldo/Primitives-PS)
* [Deep Image-based Illumination Harmonization](http://graphvision.whu.edu.cn/papers/2022/BaoZhongYun_CVPR.pdf)
* [GAN-Supervised Dense Visual Alignment](https://arxiv.org/abs/2112.05143)<br>:open_mouth:oral:star:[code](https://github.com/wpeebles/gangealing):house:[project](https://www.wpeebles.com/gangealing):tv:[video](https://youtu.be/Qa1ASS_NuzE)<br>:newspaper:[CVPR2022 Oral：GAN监督的密集视觉对齐，代码开源](https://mp.weixin.qq.com/s/t4vnZpWdG76GlnbPzJXPcw)
* [Styleformer: Transformer Based Generative Adversarial Networks With Style Vector](https://arxiv.org/abs/2106.07023)<br>:star:[code](https://github.com/Jeeseung-Park/Styleformer)<br>:newspaper:[解读](https://zhuanlan.zhihu.com/p/535269067)
* [HairMapper: Removing Hair from Portraits Using GANs](http://www.cad.zju.edu.cn/home/jin/cvpr2022/HairMapper.pdf)<br>:star:[code](https://github.com/oneThousand1000/non-hair-FFHQ)
* [Polymorphic-GAN: Generating Aligned Samples across Multiple Domains with Learned Morph Maps](https://arxiv.org/abs/2206.02903)<br>:open_mouth:oral:house:[project](https://nv-tlabs.github.io/PMGAN/)
* [Drop the GAN: In Defense of Patches Nearest Neighbors As Single Image Generative Models](https://arxiv.org/abs/2103.15545)
* [On Aliased Resizing and Surprising Subtleties in GAN Evaluation](https://arxiv.org/abs/2104.11222)<br>:star:[code](https://github.com/GaParmar/clean-fid):house:[project](https://www.cs.cmu.edu/~clean-fid/)
* [Few Shot Generative Model Adaption via Relaxed Spatial Structural Alignment](https://arxiv.org/abs/2203.04121)
* [Depth-Aware Generative Adversarial Network for Talking Head Video Generation](https://arxiv.org/abs/2203.06605)<br>:star:[code](https://github.com/harlanhong/CVPR2022-DaGAN)
* [Efficient Geometry-Aware 3D Generative Adversarial Networks](https://arxiv.org/abs/2112.07945)<br>:star:[code](https://github.com/NVlabs/eg3d):house:[project](https://nvlabs.github.io/eg3d/)
* [DO-GAN: A Double Oracle Framework for Generative Adversarial Networks](https://openaccess.thecvf.com/content/CVPR2022/papers/Aung_DO-GAN_A_Double_Oracle_Framework_for_Generative_Adversarial_Networks_CVPR_2022_paper.pdf)
* [GANSeg: Learning to Segment by Unsupervised Hierarchical Image Generation](https://arxiv.org/abs/2112.01036)<br>:star:[code](https://github.com/xingzhehe/GANSeg)
* [CoordGAN: Self-Supervised Dense Correspondences Emerge From GANs](https://arxiv.org/abs/2203.16521)<br>:star:[code](https://github.com/JitengMu/CoordGAN):house:[project](https://jitengmu.github.io/CoordGAN/):tv:[video](https://www.youtube.com/embed/FP27huY0Yu0)
* [HyperStyle: StyleGAN Inversion With HyperNetworks for Real Image Editing](https://arxiv.org/abs/2111.15666)<br>:star:[code](https://github.com/yuval-alaluf/hyperstyle):house:[project](https://yuval-alaluf.github.io/hyperstyle/)
* [Spatially-Adaptive Multilayer Selection for GAN Inversion and Editing](https://openaccess.thecvf.com/content/CVPR2022/papers/Parmar_Spatially-Adaptive_Multilayer_Selection_for_GAN_Inversion_and_Editing_CVPR_2022_paper.pdf)
* [Improving GAN Equilibrium by Raising Spatial Awareness](https://arxiv.org/abs/2112.00718)<br>:star:[code](https://github.com/genforce/eqgan-sa):house:[project](https://genforce.github.io/eqgan-sa/)
* [SphericGAN: Semi-Supervised Hyper-Spherical Generative Adversarial Networks for Fine-Grained Image Synthesis](https://openaccess.thecvf.com/content/CVPR2022/papers/Chen_SphericGAN_Semi-Supervised_Hyper-Spherical_Generative_Adversarial_Networks_for_Fine-Grained_Image_Synthesis_CVPR_2022_paper.pdf)
* [Pix2NeRF: Unsupervised Conditional p-GAN for Single Image to Neural Radiance Fields Translation](https://openaccess.thecvf.com/content/CVPR2022/papers/Cai_Pix2NeRF_Unsupervised_Conditional_p-GAN_for_Single_Image_to_Neural_Radiance_CVPR_2022_paper.pdf)<br>:star:[code](https://github.com/primecai/Pix2NeRF)
* [Think Twice Before Detecting GAN-Generated Fake Images From Their Spectral Domain Imprints](https://openaccess.thecvf.com/content/CVPR2022/papers/Dong_Think_Twice_Before_Detecting_GAN-Generated_Fake_Images_From_Their_Spectral_CVPR_2022_paper.pdf)
* [Ensembling Off-the-Shelf Models for GAN Training](https://arxiv.org/abs/2112.09130)<br>:open_mouth:oral:star:[code](https://github.com/nupurkmr9/vision-aided-gan):house:[project](https://www.cs.cmu.edu/~vision-aided-gan/)
* [Style Transformer for Image Inversion and Editing](https://arxiv.org/abs/2203.07932)<br>:star:[code](https://github.com/sapphire497/style-transformer)
* 图像篡改检测
  * [Proactive Image Manipulation Detection](https://arxiv.org/abs/2203.15880)<br>:star:[code](https://github.com/vishal3477/proactive_IMD)
* 头发编辑
  * [HairCLIP: Design Your Hair by Text and Reference Image](https://arxiv.org/abs/2112.05142)<br>:star:[code](https://github.com/wty-ustc/HairCLIP)

<a name="12"/>

## 12.Image-to-Image Translation(图像到图像翻译)
* [Exploring Patch-wise Semantic Relation for Contrastive Learning in Image-to-Image Translation Tasks](https://arxiv.org/abs/2203.01532)
* [Maximum Spatial Perturbation Consistency for Unpaired Image-to-Image Translation](https://arxiv.org/abs/2203.12707)<br>:star:[code](https://github.com/batmanlab/MSPC)
* [InstaFormer: Instance-Aware Image-to-Image Translation with Transformer](https://arxiv.org/abs/2203.16248)
* [Unsupervised Image-to-Image Translation with Generative Prior](https://arxiv.org/abs/2204.03641)<br>:star:[code](https://github.com/williamyang1991/GP-UNIT):house:[project](https://www.mmlab-ntu.com/project/gpunit/):tv:[video](https://www.youtube.com/watch?v=dDApWs_oDrM)
* [Alleviating Semantics Distortion in Unsupervised Low-Level Image-to-Image Translation via Structure Consistency Constraint](https://openaccess.thecvf.com/content/CVPR2022/papers/Guo_Alleviating_Semantics_Distortion_in_Unsupervised_Low-Level_Image-to-Image_Translation_via_Structure_CVPR_2022_paper.pdf)<br>:star:[code](https://github.com/CR-Gjx/SCC):newspaper:[解读](https://mp.weixin.qq.com/s/lSY1is6Fmm6A0Db0Jxo4qg)
* [Wavelet Knowledge Distillation: Towards Efficient Image-to-Image Translation](https://arxiv.org/abs/2203.06321)
* [QS-Attn: Query-Selected Attention for Contrastive Learning in I2I Translation](https://openaccess.thecvf.com/content/CVPR2022/papers/Hu_QS-Attn_Query-Selected_Attention_for_Contrastive_Learning_in_I2I_Translation_CVPR_2022_paper.pdf)<br>:star:[code](https://github.com/sapphire497/query-selected-attention)

<a name="11"/>

## 11.Face(人脸)
* [Synthetic Generation of Face Videos With Plethysmograph Physiology](https://openaccess.thecvf.com/content/CVPR2022/papers/Wang_Synthetic_Generation_of_Face_Videos_With_Plethysmograph_Physiology_CVPR_2022_paper.pdf)<br>:house:[project](https://visual.ee.ucla.edu/rppg_avatars.htm/)
* [Protecting Celebrities with Identity Consistency Transformer](https://arxiv.org/abs/2203.01318)
* [PhysFormer: Facial Video-based Physiological Measurement with Temporal Difference Transformer](https://arxiv.org/abs/2111.12082)<br>:star:[code](https://github.com/ZitongYu/PhysFormer)
* [How Much Does Input Data Type Impact Final Face Model Accuracy?](https://openaccess.thecvf.com/content/CVPR2022/papers/Luo_How_Much_Does_Input_Data_Type_Impact_Final_Face_Model_CVPR_2022_paper.pdf)
* [HP-Capsule: Unsupervised Face Part Discovery by Hierarchical Parsing Capsule Network](https://openaccess.thecvf.com/content/CVPR2022/papers/Yu_HP-Capsule_Unsupervised_Face_Part_Discovery_by_Hierarchical_Parsing_Capsule_Network_CVPR_2022_paper.pdf)
* Deepfake
  * [Voice-Face Homogeneity Tells Deepfake](https://arxiv.org/abs/2203.02195)<br>:star:[code](https://github.com/xaCheng1996/VFD):newspaper:[粗解](https://zhuanlan.zhihu.com/p/476923554)
* 妆容迁移
  * [Protecting Facial Privacy: Generating Adversarial Identity Masks via Style-robust Makeup Transfer](https://arxiv.org/abs/2203.03121)<br>:star:[code](https://github.com/CGCL-codes/AMT-GAN)
* 人脸识别
  * [Local-Adaptive Face Recognition via Graph-based Meta-Clustering and Regularized Adaptation](https://arxiv.org/abs/2203.14327)
  * [Killing Two Birds with One Stone:Efficient and Robust Training of Face Recognition CNNs by Partial FC](https://arxiv.org/abs/2203.15565)<br>:star:[code](https://github.com/deepinsight/insightface/tree/master/recognition)
  * [AdaFace: Quality Adaptive Margin for Face Recognition](https://arxiv.org/abs/2204.00964)<br>:open_mouth:oral:star:[code](https://github.com/mk-minchul/AdaFace)
  * [Killing Two Birds With One Stone: Efficient and Robust Training of Face Recognition CNNs by Partial FC](https://arxiv.org/abs/2203.15565)<br>:star:[code](https://github.com/deepinsight/insightface/tree/master/recognition)
  * [Learning To Learn Across Diverse Data Biases in Deep Face Recognition](https://openaccess.thecvf.com/content/CVPR2022/papers/Liu_Learning_To_Learn_Across_Diverse_Data_Biases_in_Deep_Face_CVPR_2022_paper.pdf)
* 人脸表情识别
  * [Towards Semi-Supervised Deep Facial Expression Recognition with An Adaptive Confidence Margin](https://arxiv.org/abs/2203.12341)<br>:star:[code](https://github.com/hangyu94/Ada-CM)
  * [Multi-Dimensional, Nuanced and Subjective - Measuring the Perception of Facial Expressions](https://openaccess.thecvf.com/content/CVPR2022/papers/Bryant_Multi-Dimensional_Nuanced_and_Subjective_-_Measuring_the_Perception_of_Facial_CVPR_2022_paper.pdf)
* 三维人像
  * [RigNeRF: Fully Controllable Neural 3D Portraits](https://openaccess.thecvf.com/content/CVPR2022/papers/Athar_RigNeRF_Fully_Controllable_Neural_3D_Portraits_CVPR_2022_paper.pdf)
* 3D人脸
  * [ImFace: A Nonlinear 3D Morphable Face Model with Implicit Neural Representations](https://arxiv.org/abs/2203.14510)
  * [Learning to Restore 3D Face from In-the-Wild Degraded Images](https://drive.google.com/file/d/14A4zxu6u5vQ9kROA4HvWllRQ72P8Ml6A/view?usp=sharing)<br>:newspaper:[解读](https://mp.weixin.qq.com/s/FYn0S46OA6xraH9ofNIctw)
* 活体检测
  * [PatchNet: A Simple Face Anti-Spoofing Framework via Fine-Grained Patch Recognition](https://arxiv.org/abs/2203.14325)
* 假脸检测
  * [Exploring Frequency Adversarial Attacks for Face Forgery Detection](https://arxiv.org/abs/2203.15674)<br>:newspaper:[粗解](https://mp.weixin.qq.com/s/FYn0S46OA6xraH9ofNIctw)
  * [Leveraging Real Talking Faces via Self-Supervision for Robust Forgery Detection](https://arxiv.org/abs/2201.07131)
  * [End-to-End Reconstruction-Classification Learning for Face Forgery Detection](https://openaccess.thecvf.com/content/CVPR2022/papers/Cao_End-to-End_Reconstruction-Classification_Learning_for_Face_Forgery_Detection_CVPR_2022_paper.pdf)<br>:newspaper:[解读](https://mp.weixin.qq.com/s/FYn0S46OA6xraH9ofNIctw)
* 人脸交换
  * [High-resolution Face Swapping via Latent Semantics Disentanglement](https://arxiv.org/abs/2203.15958)<br>:star:[code](https://github.com/cnnlstm/FSLSD_HiRes)
  * [Region-Aware Face Swapping](https://arxiv.org/abs/2203.04564)
  * [Smooth-Swap: A Simple Enhancement for Face-Swapping With Smoothness](https://openaccess.thecvf.com/content/CVPR2022/papers/Kim_Smooth-Swap_A_Simple_Enhancement_for_Face-Swapping_With_Smoothness_CVPR_2022_paper.pdf)
* 人脸属性分类
  * [Fair Contrastive Learning for Facial Attribute Classification](https://arxiv.org/abs/2203.16209)<br>:star:[code](https://github.com/sungho-CoolG/FSCL)
* Face Relighting(人脸重照光)
  * [Face Relighting with Geometrically Consistent Shadows](https://arxiv.org/abs/2203.16681)
* 人脸编辑
  * [TransEditor: Transformer-Based Dual-Space GAN for Highly Controllable Facial Editing](https://arxiv.org/abs/2203.17266)<br>:star:[code](https://github.com/BillyXYB/TransEditor):house:[project](https://billyxyb.github.io/TransEditor/)
  * [FENeRF: Face Editing in Neural Radiance Fields](https://arxiv.org/abs/2111.15490)<br>:star:[code](https://github.com/MrTornado24/FENeRF):house:[project](https://mrtornado24.github.io/FENeRF/)
* 人脸幻构
  * [Escaping Data Scarcity for High-Resolution Heterogeneous Face Hallucination](https://arxiv.org/abs/2203.16669)
* Deepfake检测
  * [Detecting Deepfakes with Self-Blended Images](https://arxiv.org/abs/2204.08376)<br>:open_mouth:oral:star:[code](https://github.com/mapooon/SelfBlendedImages)
  * [DeepFake Disrupter: The Detector of DeepFake Is My Friend](https://openaccess.thecvf.com/content/CVPR2022/papers/Wang_DeepFake_Disrupter_The_Detector_of_DeepFake_Is_My_Friend_CVPR_2022_paper.pdf)
* 人脸重建
  * [JIFF: Jointly-aligned Implicit Face Function for High Quality Single View Clothed Human Reconstruction](https://arxiv.org/abs/2204.10549)<br>:star:[code](https://github.com/yukangcao/JIFF):house:[project](https://yukangcao.github.io/JIFF/):newspaper:[解读](https://zhuanlan.zhihu.com/p/504499515)
  * 人脸三维重建
    * [Generating Diverse 3D Reconstructions From a Single Occluded Face Image](https://arxiv.org/abs/2112.00879)<br>:star:[code](https://github.com/human-analysis/diverse3dface)
* 人脸捕捉
  * [EMOCA: Emotion Driven Monocular Face Capture and Animation](https://arxiv.org/abs/2204.11312)<br>:house:[project](https://emoca.is.tue.mpg.de/)
* 换头
  * [Few-Shot Head Swapping in the Wild](https://arxiv.org/abs/2204.13100)<br>:open_mouth:oral:star:[code](https://github.com/jmliu88/heser):house:[project](https://jmliu88.github.io/HeSer/):tv:[video](https://youtu.be/bf2S74eFgt8):newspaper:[解读](https://zhuanlan.zhihu.com/p/506416975)
* 人像畸变矫正
  * [Semi-Supervised Wide-Angle Portraits Correction by Multi-Scale Transformer](https://arxiv.org/abs/2109.08024)<br>:star:[code](https://github.com/megvii-research/Portraits_Correction):newspaper:[解读](https://mp.weixin.qq.com/s/kZue3ds348UXQI86xrwudQ)
* 3D人脸建模
  * [Physically-guided Disentangled Implicit Rendering for 3D Face Modeling](https://drive.google.com/file/d/1s-6glim2XEKNGmrRJKrXyKFJ1X58B7GA/view)<br>:newspaper:[解读](https://mp.weixin.qq.com/s/FYn0S46OA6xraH9ofNIctw)
* 人脸修复
  * [Blind Face Restoration via Integrating Face Shape and Generative Priors](https://drive.google.com/file/d/18f2Spu3Qx1gskjdGh1Edx1ZlWjlC8xGB/view?usp=sharing)<br>:newspaper:[解读](https://mp.weixin.qq.com/s/FYn0S46OA6xraH9ofNIctw)
  * [Rethinking Deep Face Restoration](https://openaccess.thecvf.com/content/CVPR2022/papers/Zhao_Rethinking_Deep_Face_Restoration_CVPR_2022_paper.pdf)
  * [RestoreFormer: High-Quality Blind Face Restoration From Undegraded Key-Value Pairs](https://arxiv.org/abs/2201.06374)<br>:star:[code](https://github.com/wzhouxiff/RestoreFormer.git)
  * [Learning to Restore 3D Face from In-the-Wild Degraded Images](https://openaccess.thecvf.com/content/CVPR2022/papers/Zhang_Learning_To_Restore_3D_Face_From_In-the-Wild_Degraded_Images_CVPR_2022_paper.pdf)
* 人脸对齐
  * [Sparse Local Patch Transformer for Robust Face Alignment and Landmarks Inherent Relation Learning](https://arxiv.org/abs/2203.06541)<br>:star:[code](https://github.com/Jiahao-UTS/SLPT-master)  
  * [Occlusion-Robust Face Alignment Using a Viewpoint-Invariant Hierarchical Network Architecture](https://openaccess.thecvf.com/content/CVPR2022/papers/Zhu_Occlusion-Robust_Face_Alignment_Using_a_Viewpoint-Invariant_Hierarchical_Network_Architecture_CVPR_2022_paper.pdf)<br>:star:[code](https://github.com/zhuccly/GlomFace-Face-Alignment)
* 语音驱动的3D脸部动画
  * [FaceFormer: Speech-Driven 3D Facial Animation with Transformers](https://arxiv.org/abs/2112.05329)<br>:star:[code](https://github.com/EvelynFan/FaceFormer):house:[project](https://evelynfan.github.io/audio2face/)
* 舌头三维重建
  * [3D Human Tongue Reconstruction From Single "In-the-Wild" Images](https://arxiv.org/abs/2106.12302)<br>:star:[code](https://github.com/steliosploumpis/tongue)
* 伪造图像检测
  * [Robust Image Forgery Detection Over Online Social Network Shared Images](https://openaccess.thecvf.com/content/CVPR2022/papers/Wu_Robust_Image_Forgery_Detection_Over_Online_Social_Network_Shared_Images_CVPR_2022_paper.pdf)<br>:star:[code](https://github.com/HighwayWu/ImageForensicsOSN)
* 人脸解析
  * [Decoupled Multi-Task Learning With Cyclical Self-Regulation for Face Parsing](https://arxiv.org/abs/2203.14448)<br>:star:[code](https://github.com/deepinsight/insightface/tree/master/parsing/dml_csr)
* 人脸表情
  * [Robust Egocentric Photo-Realistic Facial Expression Transfer for Virtual Reality](https://arxiv.org/abs/2104.04794)
* 人脸检测
  * [MogFace: Towards a Deeper Appreciation on Face Detection](https://arxiv.org/abs/2103.11139)<br>:star:[code](https://github.com/damo-cv/MogFace)
* 人脸重现
  * [Dual-Generator Face Reenactment](https://openaccess.thecvf.com/content/CVPR2022/papers/Hsu_Dual-Generator_Face_Reenactment_CVPR_2022_paper.pdfV)<br>:star:[code](https://github.com/AvLab-CV/Dual_Generator_Face_Reenactment)
* 说话人脸生成
  * [Talking Face Generation With Multilingual TTS](https://arxiv.org/abs/2205.06421)<br>:house:[project](https://huggingface.co/spaces/CVPR/ml-talking-face)
  * [Expressive Talking Head Generation With Granular Audio-Visual Control](https://openaccess.thecvf.com/content/CVPR2022/papers/Liang_Expressive_Talking_Head_Generation_With_Granular_Audio-Visual_Control_CVPR_2022_paper.pdf)
* 人脸关键点
  * [Towards Accurate Facial Landmark Detection via Cascaded Transformers](https://openaccess.thecvf.com/content/CVPR2022/papers/Li_Towards_Accurate_Facial_Landmark_Detection_via_Cascaded_Transformers_CVPR_2022_paper.pdf)
* 人脸变形
  * [FaceVerse: A Fine-Grained and Detail-Controllable 3D Face Morphable Model From a Hybrid Dataset](https://arxiv.org/abs/2203.14057)<br>:star:[code](https://github.com/LizhenWangT/FaceVerse)
* 3D人脸表情合成
  * [Sparse to Dense Dynamic 3D Facial Expression Generation](https://arxiv.org/abs/2105.07463)<br>:star:[code](https://github.com/CRISTAL-3DSAM/Sparse2Dense)


<a name="10"/>

## 10.3D(三维视觉)
* [Disentangled3D: Learning a 3D Generative Model with Disentangled Geometry and Appearance from Monocular Images](https://arxiv.org/abs/2203.15926)
* [Depth-Guided Sparse Structure-from-Motion for Movies and TV Shows](https://arxiv.org/abs/2204.02509)<br>:star:[code](https://github.com/amazon-research/small-baseline-camera-tracking)
* [3D-SPS: Single-Stage 3D Visual Grounding via Referred Point Progressive Selection](https://arxiv.org/abs/2204.06272)<br>:open_mouth:oral:star:[code](https://github.com/fjhzhixi/3D-SPS):newspaper:[解读](https://zhuanlan.zhihu.com/p/498244289)
* [Physical Simulation Layer for Accurate 3D Modeling](https://openaccess.thecvf.com/content/CVPR2022/papers/Mezghanni_Physical_Simulation_Layer_for_Accurate_3D_Modeling_CVPR_2022_paper.pdf)
* [φ-SfT: Shape-from-Template with a Physics-Based Deformation Model](https://openaccess.thecvf.com/content/CVPR2022/papers/Kairanda_f-SfT_Shape-From-Template_With_a_Physics-Based_Deformation_Model_CVPR_2022_paper.pdf)<br>:house:[project](https://4dqv.mpi-inf.mpg.de/phi-SfT/)
* [ICON: Implicit Clothed Humans Obtained From Normals](https://arxiv.org/abs/2112.09127)<br>:star:[code](https://github.com/YuliangXiu/ICON):house:[project](https://icon.is.tue.mpg.de/)
* [Representing 3D Shapes With Probabilistic Directed Distance Fields](https://openaccess.thecvf.com/content/CVPR2022/papers/Aumentado-Armstrong_Representing_3D_Shapes_With_Probabilistic_Directed_Distance_Fields_CVPR_2022_paper.pdf)
* [Improving Neural Implicit Surfaces Geometry With Patch Warping](https://arxiv.org/abs/2112.09648)<br>:star:[code](https://github.com/fdarmon/NeuralWarp)
* [LOLNerf: Learn From One Look](https://arxiv.org/abs/2111.09996)<br>:house:[project](https://ubc-vision.github.io/lolnerf/)
* Stereo Merging
  * [PSMNet: Position-aware Stereo Merging Network for Room Layout Estimation](https://arxiv.org/abs/2203.15965)
  * [GraftNet: Towards Domain Generalized Stereo Matching with a Broad-Spectrum and Task-Oriented Feature](https://arxiv.org/abs/2204.00179)<br>:star:[code](https://github.com/SpadeLiu/Graft-PSMNet)
  * [Degradation-agnostic Correspondence from Resolution-asymmetric Stereo](https://arxiv.org/abs/2204.01429)
  * [Practical Stereo Matching via Cascaded Recurrent Network with Adaptive Correlation](https://arxiv.org/abs/2203.11483)<br>:open_mouth:oral:star:[code](https://github.com/megvii-research/CREStereo):newspaper:[解读](https://mp.weixin.qq.com/s/kZue3ds348UXQI86xrwudQ)
* stereo matching
  * [Chitransformer: Towards Reliable Stereo From Cues](https://arxiv.org/abs/2203.04554)<br>:star:[code](https://github.com/ISL-CV/ChiTransformer)
  * [Uniform Subdivision of Omnidirectional Camera Space for Efficient Spherical Stereo Matching](https://openaccess.thecvf.com/content/CVPR2022/papers/Kang_Uniform_Subdivision_of_Omnidirectional_Camera_Space_for_Efficient_Spherical_Stereo_CVPR_2022_paper.pdf)
  * [FoggyStereo: Stereo Matching With Fog Volume Representation](https://openaccess.thecvf.com/content/CVPR2022/papers/Yao_FoggyStereo_Stereo_Matching_With_Fog_Volume_Representation_CVPR_2022_paper.pdf)
  * [ITSA: An Information-Theoretic Approach to Automatic Shortcut Avoidance and Domain Generalization in Stereo Matching Networks](https://arxiv.org/abs/2201.02263)<br>:star:[code](https://github.com/waychin-weiqin/ITSA)
* 深度估计
  * [OmniFusion: 360 Monocular Depth Estimation via Geometry-Aware Fusion](https://arxiv.org/abs/2203.00838)<br>:open_mouth:oral:star:[code](https://github.com/yuyanli0831/OmniFusion)
  * [NeW CRFs: Neural Window Fully-connected CRFs for Monocular Depth Estimation](https://arxiv.org/abs/2203.01502)<br>:star:[code](https://github.com/aliyun/NeWCRFs):house:[project](https://weihaosky.github.io/newcrfs/)
  * 🐦️[Toward Practical Self-Supervised Monocular Indoor Depth Estimation](https://arxiv.org/abs/2112.02306)
  * [P3Depth: Monocular Depth Estimation with a Piecewise Planarity Prior](https://arxiv.org/abs/2204.02091)<br>:star:[code](https://github.com/SysCV/P3Depth)
  * [HiMODE: A Hybrid Monocular Omnidirectional Depth Estimation Model](https://arxiv.org/abs/2204.05007)
  * [Multi-Frame Self-Supervised Depth with Transformers](https://arxiv.org/abs/2204.07616)
  * [Layered Depth Refinement with Mask Guidance](https://arxiv.org/abs/2206.03048)<br>:house:[project](https://sooyekim.github.io/MaskDepth/)
  * [360MonoDepth: High-Resolution 360deg Monocular Depth Estimation](https://openaccess.thecvf.com/content/CVPR2022/papers/Rey-Area_360MonoDepth_High-Resolution_360deg_Monocular_Depth_Estimation_CVPR_2022_paper.pdf)<br>:star:[code](https://github.com/manurare/360monodepth):house:[project](https://manurare.github.io/360monodepth/)
  * [Towards Multimodal Depth Estimation from Light Fields](https://arxiv.org/abs/2203.16542)
  * [Multi-Frame Self-Supervised Depth with Transformers](https://openaccess.thecvf.com/content/CVPR2022/papers/Guizilini_Multi-Frame_Self-Supervised_Depth_With_Transformers_CVPR_2022_paper.pdf)
  * [Exploiting Pseudo Labels in a Self-Supervised Learning Framework for Improved Monocular Depth Estimation](https://openaccess.thecvf.com/content/CVPR2022/papers/Petrovai_Exploiting_Pseudo_Labels_in_a_Self-Supervised_Learning_Framework_for_Improved_CVPR_2022_paper.pdf)
  * [Rethinking Depth Estimation for Multi-View Stereo: A Unified Representation](https://arxiv.org/abs/2201.01501)<br>:star:[code](https://github.com/prstrive/UniMVSNet)
  * [Multi-View Depth Estimation by Fusing Single-View Depth Probability With Multi-View Geometry](Multi-View Depth Estimation by Fusing Single-View Depth Probability With Multi-View Geometry)<br>:open_mouth:oral:star:[code](https://github.com/baegwangbin/MaGNet)
  * [Toward Practical Monocular Indoor Depth Estimation](https://arxiv.org/abs/2112.02306)
  * [Single-Stage 3D Geometry-Preserving Depth Estimation Model Training on Dataset Mixtures With Uncalibrated Stereo Data](https://openaccess.thecvf.com/content/CVPR2022/papers/Patakin_Single-Stage_3D_Geometry-Preserving_Depth_Estimation_Model_Training_on_Dataset_Mixtures_CVPR_2022_paper.pdf)
  * [Stereo Depth From Events Cameras: Concentrate and Focus on the Future](https://openaccess.thecvf.com/content/CVPR2022/papers/Nam_Stereo_Depth_From_Events_Cameras_Concentrate_and_Focus_on_the_CVPR_2022_paper.pdf)<br>:star:[code](https://github.com/yonseivnl/se-cff)
  * [Depth Estimation by Combining Binocular Stereo and Monocular Structured-Light](https://arxiv.org/abs/2203.10493)<br>:star:[code](https://github.com/YuhuaXu/MonoStereoFusion)
* 房间布局
  * [LGT-Net: Indoor Panoramic Room Layout Estimation with Geometry-Aware Transformer Network](https://arxiv.org/abs/2203.01824)<br>:star:[code](https://github.com/zhigangjiang/LGT-Net):newspaper:[粗解](https://zhuanlan.zhihu.com/p/475067096)
* MVS
  * MVPS
    * [Uncertainty-Aware Deep Multi-View Photometric Stereo](https://arxiv.org/abs/2202.13071)
  * [RayMVSNet: Learning Ray-based 1D Implicit Fields for Accurate Multi-View Stereo](https://arxiv.org/abs/2204.01320)
  * [TransMVSNet: Global Context-aware Multi-view Stereo Network with Transformers](https://arxiv.org/abs/2111.14600)<br>:star:[code](https://github.com/MegviiRobot/TransMVSNet):newspaper:[解读](https://mp.weixin.qq.com/s/kZue3ds348UXQI86xrwudQ)
  * [Non-parametric Depth Distribution Modelling based Depth Inference for Multi-view Stereo](https://arxiv.org/abs/2205.03783)<br>:star:[code](https://github.com/NVlabs/NP-CVP-MVSNet)
  * [IterMVS: Iterative Probability Estimation for Efficient Multi-View Stereo](https://arxiv.org/abs/2112.05126)<br>:star:[code](https://github.com/FangjinhuaWang/IterMVS)
* 三维重建
  * [PlaneMVS: 3D Plane Reconstruction from Multi-View Stereo](https://arxiv.org/abs/2203.12082)
  * [Self-supervised Neural Articulated Shape and Appearance Models](https://arxiv.org/abs/2205.08525)<br>:house:[project](https://weify627.github.io/nasam/)
  * [BNV-Fusion: Dense 3D Reconstruction using Bi-level Neural Volume Fusion](https://arxiv.org/abs/2204.01139)
  * [Topologically-Aware Deformation Fields for Single-View 3D Reconstruction](https://arxiv.org/abs/2205.06267)<br>:star:[code](https://github.com/google/nerfies):house:[project](https://shivamduggal4.github.io/tars-3D/)
  * [Pre-train, Self-train, Distill: A simple recipe for Supersizing 3D Reconstruction](https://arxiv.org/abs/2204.03642)<br>:star:[code](https://github.com/shubhtuls/ss3d/):house:[project](https://shubhtuls.github.io/ss3d/):newspaper:[解读](https://zhuanlan.zhihu.com/p/494913545)
  * [What's in your hands? 3D Reconstruction of Generic Objects in Hands](https://arxiv.org/abs/2204.07153)<br>:star:[code](https://github.com/JudyYe/ihoi):house:[project](https://judyye.github.io/ihoi/):tv:[video](https://youtu.be/-hHlkWwENiI):newspaper:[解读](https://zhuanlan.zhihu.com/p/498883232)
  * [Surface Reconstruction from Point Clouds by Learning Predictive Context Priors](https://arxiv.org/abs/2204.11015)<br>:star:[code](https://github.com/mabaorui/PredictableContextPrior)
  * [FvOR: Robust Joint Shape and Pose Optimization for Few-view Object Reconstruction](https://arxiv.org/abs/2205.07763)<br>:star:[code](https://github.com/zhenpeiyang/FvOR/)<br>:newspaper:[解读](https://zhuanlan.zhihu.com/p/515672497)
  * [KeyTr: Keypoint Transporter for 3D Reconstruction of Deformable Objects in Videos](https://openaccess.thecvf.com/content/CVPR2022/papers/Novotny_KeyTr_Keypoint_Transporter_for_3D_Reconstruction_of_Deformable_Objects_in_CVPR_2022_paper.pdf)
  * [SPAMs: Structured Implicit Parametric Models](https://arxiv.org/abs/2201.08141)<br>:house:[project](https://pablopalafox.github.io/spams/):tv:[video](https://www.youtube.com/watch?v=ChdjHNGgrzI)
  * [Enhancing Face Recognition With Self-Supervised 3D Reconstruction](https://openaccess.thecvf.com/content/CVPR2022/papers/He_Enhancing_Face_Recognition_With_Self-Supervised_3D_Reconstruction_CVPR_2022_paper.pdf)
  * [Neural Fields As Learnable Kernels for 3D Reconstruction](https://arxiv.org/abs/2111.13674)<br>:house:[project](https://nv-tlabs.github.io/nkf/)
  * 三维场景重建
    * [Neural 3D Scene Reconstruction with the Manhattan-world Assumption](https://arxiv.org/abs/2205.02836)<br>:open_mouth:oral:star:[code](https://github.com/zju3dv/manhattan_sdf):house:[project](https://zju3dv.github.io/manhattan_sdf/):tv:[video](https://youtu.be/oEE7mK0YQtc):newspaper:[解读](https://zhuanlan.zhihu.com/p/510007610)
    * [StyleMesh: Style Transfer for Indoor 3D Scene Reconstructions](https://openaccess.thecvf.com/content/CVPR2022/papers/Hollein_StyleMesh_Style_Transfer_for_Indoor_3D_Scene_Reconstructions_CVPR_2022_paper.pdf)<br>:star:[code](https://github.com/lukasHoel/stylemesh):house:[project](https://lukashoel.github.io/stylemesh/):tv:[video](https://www.youtube.com/watch?v=ZqgiTLcNcks)
    * [PhotoScene: Photorealistic Material and Lighting Transfer for Indoor Scenes](https://openaccess.thecvf.com/content/CVPR2022/papers/Yeh_PhotoScene_Photorealistic_Material_and_Lighting_Transfer_for_Indoor_Scenes_CVPR_2022_paper.pdf)<br>:star:[code](https://github.com/ViLab-UCSD/photoscene)
    * [Look Outside the Room: Synthesizing a Consistent Long-Term 3D Scene Video From a Single Image](https://arxiv.org/abs/2203.09457)<br>:star:[code](https://github.com/xrenaa/Look-Outside-Room):house:[project](https://xrenaa.github.io/look-outside-room/)
  * 手物重建
    * [Collaborative Learning for Hand and Object Reconstruction with Attention-guided Graph Convolution](https://arxiv.org/abs/2204.13062)
  * 三维服装网格重建
    * [Registering Explicit to Implicit: Towards High-Fidelity Garment mesh Reconstruction from Single Images](https://arxiv.org/abs/2203.15007)<br>:house:[project](https://kv2000.github.io/2022/03/28/reef/)
    * [Photorealistic Monocular 3D Reconstruction of Humans Wearing Clothing](https://arxiv.org/abs/2204.08906)<br>:house:[project](https://phorhum.github.io/)
  * 三维网格重建
    * [Neural Template: Topology-aware Reconstruction and Disentangled Generation of 3D Meshes](https://arxiv.org/abs/2206.04942)<br>:star:[code](https://github.com/edward1997104/Neural-Template):newspaper:[解读](https://zhuanlan.zhihu.com/p/528028523)
 * 三维形状重建
    * [3D Shape Reconstruction from 2D Images with Disentangled Attribute Flow](https://arxiv.org/abs/2203.15190)
    * [GIFS: Neural Implicit Function for General Shape Representation](https://arxiv.org/abs/2204.07126)<br>:house:[project](https://jianglongye.com/gifs/)
* 三维服装变形
  * [SNUG: Self-Supervised Neural Dynamic Garments](https://arxiv.org/abs/2204.02219)<br>:open_mouth:oral:star:[code](https://github.com/isantesteban/snug)
* 纹理迁移与合成
  * [AUV-Net: Learning Aligned UV Maps for Texture Transfer and Synthesis](https://arxiv.org/abs/2204.03105)<br>:star:[code](https://github.com/nv-tlabs/AUV-NET):house:[project](https://nv-tlabs.github.io/AUV-NET/):tv:[video](https://www.youtube.com/watch?v=UTzH8WB-Xl0)
* 形状匹配
  * [A Scalable Combinatorial Solver for Elastic Geometrically Consistent 3D Shape Matching](https://arxiv.org/abs/2204.12805)<br>:star:[code](https://github.com/paul0noah/sm-comb)
  * [Deep Orientation-Aware Functional Maps: Tackling Symmetry Issues in Shape Matching](https://arxiv.org/abs/2204.13453)<br>:star:[code](https://github.com/nicolasdonati/DUO-FM)
* 表面重建
  * [Critical Regularizations for Neural Surface Reconstruction in the Wild](https://arxiv.org/abs/2206.03087)
  * [POCO: Point Convolution for Surface Reconstruction](https://arxiv.org/abs/2201.01831)<br>:star:[code](https://github.com/valeoai/POCO)  
* 多视图网格重建
  * [Multi-View Mesh Reconstruction With Neural Deferred Shading](https://openaccess.thecvf.com/content/CVPR2022/papers/Worchel_Multi-View_Mesh_Reconstruction_With_Neural_Deferred_Shading_CVPR_2022_paper.pdf) 
* 3D形状分析
  * [Medial Spectral Coordinates for 3D Shape Analysis](https://arxiv.org/abs/2111.13295)
  * [Learning Deep Implicit Functions for 3D Shapes with Dynamic Code Clouds](https://arxiv.org/abs/2203.14048)<br>:star:[code](https://github.com/lity20/DCCDIF)
* 三维补全
  * [AutoSDF: Shape Priors for 3D Completion, Reconstruction and Generation](https://arxiv.org/abs/2203.09516)<br>:star:[code](https://github.com/yccyenchicheng/AutoSDF/):house:[project](https://yccyenchicheng.github.io/AutoSDF/)
* 图像重建
  * [Image Based Reconstruction of Liquids from 2D Surface Detections](https://arxiv.org/abs/2111.11491)<br>:star:[code](https://github.com/ucsdarclab/liquid_reconstruction)
* PS
  * [Fast Light-Weight Near-Field Photometric Stereo](https://arxiv.org/abs/2203.16515)
* 预测三维物体形状
  * [Learning 3D Object Shape and Layout Without 3D Supervision](https://openaccess.thecvf.com/content/CVPR2022/papers/Gkioxari_Learning_3D_Object_Shape_and_Layout_Without_3D_Supervision_CVPR_2022_paper.pdf)<br>:house:[project](https://gkioxari.github.io/usl/)
* 三维形状
  * [3D Shape Variational Autoencoder Latent Disentanglement via Mini-Batch Feature Swapping for Bodies and Faces](https://arxiv.org/abs/2111.12448)<br>:star:[code](https://github.com/simofoti/3DVAE-SwapDisentangled)
* 神经三维内容生成
  * [StyleSDF: High-Resolution 3D-Consistent Image and Geometry Generation](https://arxiv.org/abs/2112.11427):house:[project](https://github.com/royorel/StyleSDF)
* 深度补全
  * [RGB-Depth Fusion GAN for Indoor Depth Completion](https://arxiv.org/abs/2203.10856)
  * [GuideFormer: Transformers for Image Guided Depth Completion](https://openaccess.thecvf.com/content/CVPR2022/papers/Rho_GuideFormer_Transformers_for_Image_Guided_Depth_Completion_CVPR_2022_paper.pdf)
* 线段重建
  * [ELSR: Efficient Line Segment Reconstruction With Planes and Points Guidance](https://openaccess.thecvf.com/content/CVPR2022/papers/Wei_ELSR_Efficient_Line_Segment_Reconstruction_With_Planes_and_Points_Guidance_CVPR_2022_paper.pdf)

<a name="9"/>

## 9.Human Pose Estimation(人体姿态估计)
* [COAP: Compositional Articulated Occupancy of People](https://arxiv.org/abs/2204.06184)<br>:star:[code](https://github.com/markomih/COAP):house:[project](https://neuralbodies.github.io/COAP/index.html):tv:[video](https://www.youtube.com/watch?v=qU0q5h6IldU):newspaper:[解读](https://zhuanlan.zhihu.com/p/498244289)
* [Context-Aware Sequence Alignment using 4D Skeletal Augmentation](https://arxiv.org/abs/2204.12223)<br>:open_mouth:oral:star:[code](https://github.com/taeinkwon/casa):house:[project](https://taeinkwon.com/projects/casa/)
* [Generalizable Human Pose Triangulation](https://openaccess.thecvf.com/content/CVPR2022/papers/Bartol_Generalizable_Human_Pose_Triangulation_CVPR_2022_paper.pdf)
* [Location-Free Human Pose Estimation](https://arxiv.org/abs/2205.12619)<br>:newspaper:[解读](https://mp.weixin.qq.com/s/FYn0S46OA6xraH9ofNIctw)
* 多人姿态估计
  * [Learning Local-Global Contextual Adaptation for Multi-Person Pose Estimation](https://arxiv.org/abs/2109.03622)<br>:star:[code](https://github.com/cherubicXN/logocap)
  * [End-to-End Multi-Person Pose Estimation With Transformers](https://openaccess.thecvf.com/content/CVPR2022/papers/Shi_End-to-End_Multi-Person_Pose_Estimation_With_Transformers_CVPR_2022_paper.pdf)<br>:star:[code](https://github.com/hikvision-research/opera)
  * [Contextual Instance Decoupling for Robust Multi-Person Pose Estimation](https://openaccess.thecvf.com/content/CVPR2022/papers/Wang_Contextual_Instance_Decoupling_for_Robust_Multi-Person_Pose_Estimation_CVPR_2022_paper.pdf)<br>:star:[code](https://github.com/kennethwdk/CID)
* 基于视频的HPE
  * [Temporal Feature Alignment and Mutual Information Maximization for Video-Based Human Pose Estimation](https://arxiv.org/abs/2203.15227)<br>:open_mouth:oral:star:[code](https://github.com/Pose-Group/FAMI-Pose)
* 3D pose
  * [MixSTE: Seq2seq Mixed Spatio-Temporal Encoder for 3D Human Pose Estimation in Video](https://arxiv.org/abs/2203.00859)
  * [PoseTriplet: Co-evolving 3D Human Pose Estimation, Imitation, and Hallucination under Self-supervision](https://arxiv.org/abs/2203.15625)<br>:open_mouth:oral:star:[code](https://github.com/Garfield-kh/PoseTriplet)
  * [Uncertainty-Aware Adaptation for Self-Supervised 3D Human Pose Estimation](https://arxiv.org/abs/2203.15293)<br>:house:[project](https://sites.google.com/view/mrp-net)
  * [Single-Stage Is Enough: Multi-Person Absolute 3D Pose Estimation](https://openaccess.thecvf.com/content/CVPR2022/papers/Jin_Single-Stage_Is_Enough_Multi-Person_Absolute_3D_Pose_Estimation_CVPR_2022_paper.pdf)
  * [Distribution-Aware Single-Stage Models for Multi-Person 3D Pose Estimation](https://arxiv.org/abs/2203.07697)<br>:newspaper:[精准高效估计多人3D姿态，美图&北航分布感知式单阶段模型](https://mp.weixin.qq.com/s/eFfHnoXmkJUm5HZIVEdYRQ)
  * [Forecasting Characteristic 3D Poses of Human Actions](https://openaccess.thecvf.com/content/CVPR2022/papers/Diller_Forecasting_Characteristic_3D_Poses_of_Human_Actions_CVPR_2022_paper.pdf)<br>:tv:[video](https://www.youtube.com/watch?v=kVhn8OWMgME)
  * [Ray3D: Ray-Based 3D Human Pose Estimation for Monocular Absolute 3D Localization](https://arxiv.org/abs/2203.11471)<br>:star:[code](https://github.com/YxZhxn/Ray3D)
  * [Estimating Egocentric 3D Human Pose in the Wild With External Weak Supervision](https://arxiv.org/abs/2201.07929)<br>:house:[project](https://people.mpi-inf.mpg.de/~jianwang/projects/egopw/)
  * [ElePose: Unsupervised 3D Human Pose Estimation by Predicting Camera Elevation and Learning Normalizing Flows on 2D Poses](https://arxiv.org/abs/2112.07088)<br>:star:[code](https://github.com/bastianwandt/ElePose)
  * [MHFormer: Multi-Hypothesis Transformer for 3D Human Pose Estimation](https://arxiv.org/abs/2111.12707)<br>:star:[code](https://github.com/Vegetebird/MHFormer)
  * [PoseKernelLifter: Metric Lifting of 3D Human Pose Using Sound](https://arxiv.org/abs/2112.00216)
* 4D 人体捕获  
  * [H4D: Human 4D Modeling by Learning Neural Compositional Representation](https://arxiv.org/abs/2203.01247)<br>:star:[code](https://github.com/BoyanJIANG/H4D):house:[project](https://boyanjiang.github.io/H4D/)
* 手势生成
  * [Learning Hierarchical Cross-Modal Association for Co-Speech Gesture Generation](https://arxiv.org/pdf/2203.13161.pdf)<br>:star:[code](https://github.com/alvinliu0/HA2G):house:[project](https://alvinliu0.github.io/projects/HA2G)
* 3D手网格估计
  * [HandOccNet: Occlusion-Robust 3D Hand Mesh Estimation Network](https://arxiv.org/abs/2203.14564)<br>:star:[code](https://github.com/namepllet/HandOccNet)
* 3D形状生成
  * [Towards Implicit Text-Guided 3D Shape Generation](https://arxiv.org/abs/2203.14622)<br>:star:[code](https://github.com/liuzhengzhe/Towards-Implicit)
  * 3D狗的形状
    * [BARC: Learning to Regress 3D Dog Shape from Images by Exploiting Breed Information](https://arxiv.org/abs/2203.15536)<br>:house:[project](https://barc.is.tue.mpg.de)
* 运动捕捉
  * [Neural MoCon: Neural Motion Control for Physically Plausible Human Motion Capture](https://arxiv.org/abs/2203.14065)<br>:house:[project](https://www.yangangwang.com/papers/HBZ-NM-2022-03.html)
  * [A Low-Cost & Real-Time Motion Capture System](https://openaccess.thecvf.com/content/CVPR2022/papers/Chatzitofis_A_Low-Cost__Real-Time_Motion_Capture_System_CVPR_2022_paper.pdf)<br>:tv:[video](https://www.youtube.com/watch?v=mTFfUu0pM5o)
  * [LiDARCap: Long-Range Marker-Less 3D Human Motion Capture With LiDAR Point Clouds](https://arxiv.org/abs/2203.14698)
* 手臂-手部动态估计
  * [Spatial-Temporal Parallel Transformer for Arm-Hand Dynamic Estimation](https://arxiv.org/abs/2203.16202)
* 3D手重建
  * [LISA: Learning Implicit Shape and Appearance of Hands](https://arxiv.org/abs/2204.01695)<br>:house:[project](https://www.iri.upc.edu/people/ecorona/lisa/)
* 3D人体形状
  * [OSSO: Obtaining Skeletal Shape from Outside](https://arxiv.org/abs/2204.10129)<br>:star:[code](https://github.com/MarilynKeller/OSSO):house:[project](https://osso.is.tue.mpg.de/):tv:[video](https://osso.is.tue.mpg.de/#Video):newspaper:[解读](https://zhuanlan.zhihu.com/p/502894478)
* Dense correspondence 
  * [BodyMap: Learning Full-Body Dense Correspondence Map](https://arxiv.org/abs/2205.09111)<br>:house:[project](https://nsarafianos.github.io/bodymap)
* 3D人体运动重建
  * [Differentiable Dynamics for Articulated 3d Human Motion Reconstruction](https://arxiv.org/abs/2205.12256)
* 三维人体姿态重建
  * [Trajectory Optimization for Physics-Based Reconstruction of 3d Human Pose from Monocular Video](https://arxiv.org/abs/2205.12292)
  * [Putting People in their Place: Monocular Regression of 3D People in Depth](https://arxiv.org/abs/2112.08274)<br>:star:[code](https://github.com/LiyaoTang/contrastBoundary):newspaper:[解读](https://mp.weixin.qq.com/s/lSY1is6Fmm6A0Db0Jxo4qg)
* 人体重建
  * [SelfRecon: Self Reconstruction Your Digital Avatar From Monocular Video](https://arxiv.org/abs/2201.12792)<br>:open_mouth:oral:star:[code](https://github.com/jby1993/SelfReconCode)
* 人体网格恢复
  * [Human Mesh Recovery From Multiple Shots](https://arxiv.org/abs/2012.09843)<br>:star:[code](https://github.com/geopavlakos/multishot):house:[project](https://geopavlakos.github.io/multishot/)
  * [Occluded Human Mesh Recovery](https://arxiv.org/abs/2203.13349)<br>:house:[project](https://rawalkhirodkar.github.io/ochmr/)
  * [GLAMR: Global Occlusion-Aware Human Mesh Recovery With Dynamic Cameras](https://arxiv.org/abs/2112.01524)<br>:open_mouth:oral:star:[code](https://github.com/NVlabs/GLAMR):house:[project](https://nvlabs.github.io/GLAMR/)
* 人体运动描述
  * [Programmatic Concept Learning for Human Motion Description and Synthesis](https://openaccess.thecvf.com/content/CVPR2022/papers/Kulal_Programmatic_Concept_Learning_for_Human_Motion_Description_and_Synthesis_CVPR_2022_paper.pdf)<br>:house:[project](https://sumith1896.github.io/motion-concepts/)
* 三维人体动作
  * [Generating Diverse and Natural 3D Human Motions From Text](https://openaccess.thecvf.com/content/CVPR2022/papers/Guo_Generating_Diverse_and_Natural_3D_Human_Motions_From_Text_CVPR_2022_paper.pdf)<br>:star:[code](https://github.com/EricGuo5513/text-to-motion):house:[project](https://ericguo5513.github.io/text-to-motion/)
* 三维人体合成
  * [Surface-Aligned Neural Radiance Fields for Controllable 3D Human Synthesis](https://arxiv.org/abs/2201.01683)<br>:star:[code](https://github.com/pfnet-research/surface-aligned-nerf):house:[project](https://pfnet-research.github.io/surface-aligned-nerf/)
* 音频驱动的手势重演
  * [Audio-driven Neural Gesture Reenactment with Video Motion Graphs](https://openaccess.thecvf.com/content/CVPR2022/papers/Zhou_Audio-Driven_Neural_Gesture_Reenactment_With_Video_Motion_Graphs_CVPR_2022_paper.pdf)<br>:star:[code](https://github.com/yzhou359/vid-reenact)
* HSC
  * [Capturing and Inferring Dense Full-Body Human-Scene Contact](https://openaccess.thecvf.com/content/CVPR2022/papers/Huang_Capturing_and_Inferring_Dense_Full-Body_Human-Scene_Contact_CVPR_2022_paper.pdf)<br>:star:[code](https://github.com/paulchhuang/bstro):house:[project](https://rich.is.tue.mpg.de/):tv:[video](https://youtu.be/IbFc12L5Kc4)
* 3D人体运动合成
  * [Towards Diverse and Natural Scene-Aware 3D Human Motion Synthesis](https://arxiv.org/abs/2205.13001)
* 人体重建
  * [DoubleField: Bridging the Neural Surface and Radiance Fields for High-Fidelity Human Reconstruction and Rendering](https://arxiv.org/abs/2106.03798)<br>:house:[project](http://www.liuyebin.com/dbfield/dbfield.html)
  * [SMPL-A: Modeling Person-Specific Deformable Anatomy](https://openaccess.thecvf.com/content/CVPR2022/papers/Guo_SMPL-A_Modeling_Person-Specific_Deformable_Anatomy_CVPR_2022_paper.pdf)
* 3D手部姿势
  * [Mining Multi-View Information: A Strong Self-Supervised Framework for Depth-Based 3D Hand Pose and Mesh Estimation](https://openaccess.thecvf.com/content/CVPR2022/papers/Ren_Mining_Multi-View_Information_A_Strong_Self-Supervised_Framework_for_Depth-Based_3D_CVPR_2022_paper.pdf)<br>:star:[code](https://github.com/RenFeiTemp/MMI)
* 形状重建
  * [Topology-Preserving Shape Reconstruction and Registration via Neural Diffeomorphic Flow](https://arxiv.org/abs/2203.08652)<br>:star:[code](https://github.com/Siwensun/Neural_Diffeomorphic_Flow--NDF)


<a name="8"/>

## 8.Action Detection(人体动作检测与识别)
* 动作检测
  * [Colar: Effective and Efficient Online Action Detection by Consulting Exemplars](https://arxiv.org/abs/2203.01057)
  * [Learnable Irrelevant Modality Dropout for Multimodal Action Recognition on Modality-Specific Annotated Videos](https://arxiv.org/abs/2203.03014)
  * [End-to-End Semi-Supervised Learning for Video Action Detection](https://arxiv.org/abs/2203.04251)
  * [SPAct: Self-supervised Privacy Preservation for Action Recognition](https://arxiv.org/abs/2203.15205)<br>:star:[code](https://github.com/DAVEISHAN/SPAct)
  * [Temporal Alignment Networks for Long-term Video](https://arxiv.org/abs/2204.02968)<br>:open_mouth:oral:star:[code](https://github.com/TengdaHan/TemporalAlignNet):house:[project](https://www.robots.ox.ac.uk/~vgg/research/tan/):newspaper:[粗解](https://zhuanlan.zhihu.com/p/494237617)
  * [SOS! Self-supervised Learning Over Sets Of Handled Objects In Egocentric Action Recognition](https://arxiv.org/abs/2204.04796)
  * [GateHUB: Gated History Unit With Background Suppression for Online Action Detection](https://openaccess.thecvf.com/content/CVPR2022/papers/Chen_GateHUB_Gated_History_Unit_With_Background_Suppression_for_Online_Action_CVPR_2022_paper.pdf) 
  * [MS-TCT: Multi-Scale Temporal ConvTransformer for Action Detection](https://openaccess.thecvf.com/content/CVPR2022/papers/Dai_MS-TCT_Multi-Scale_Temporal_ConvTransformer_for_Action_Detection_CVPR_2022_paper.pdf)<br>:star:[code](https://github.com/dairui01/MS-TCT)
  * [Look for the Change: Learning Object States and State-Modifying Actions From Untrimmed Web Videos](https://openaccess.thecvf.com/content/CVPR2022/papers/Soucek_Look_for_the_Change_Learning_Object_States_and_State-Modifying_Actions_CVPR_2022_paper.pdf)<br>:house:[project](https://data.ciirc.cvut.cz/public/projects/2022LookForTheChange/)
  * [Uncertainty-Guided Probabilistic Transformer for Complex Action Recognition](https://openaccess.thecvf.com/content/CVPR2022/papers/Guo_Uncertainty-Guided_Probabilistic_Transformer_for_Complex_Action_Recognition_CVPR_2022_paper.pdf)
  * [Learning From Temporal Gradient for Semi-Supervised Action Recognition](https://arxiv.org/abs/2111.13241)<br>:star:[code](https://github.com/lambert-x/video-semisup)
  * [DirecFormer: A Directed Attention in Transformer Approach to Robust Action Recognition](https://arxiv.org/abs/2203.10233)<br>:star:[code](https://github.com/uark-cviu/DirecFormer)
  * [Interact Before Align: Leveraging Cross-Modal Knowledge for Domain Adaptive Action Recognition](https://openaccess.thecvf.com/content/CVPR2022/papers/Yang_Interact_Before_Align_Leveraging_Cross-Modal_Knowledge_for_Domain_Adaptive_Action_CVPR_2022_paper.pdf)
  * [Object-Relation Reasoning Graph for Action Recognition](https://openaccess.thecvf.com/content/CVPR2022/papers/Ou_Object-Relation_Reasoning_Graph_for_Action_Recognition_CVPR_2022_paper.pdf)
  * [Revisiting Skeleton-Based Action Recognition](https://arxiv.org/abs/2104.13586)<br>:open_mouth:oral:star:[code](https://github.com/kennymckormick/pyskl)
  * [InfoGCN: Representation Learning for Human Skeleton-Based Action Recognition](https://openaccess.thecvf.com/content/CVPR2022/papers/Chi_InfoGCN_Representation_Learning_for_Human_Skeleton-Based_Action_Recognition_CVPR_2022_paper.pdf)
  * 零样本动作识别
    * [Cross-modal Representation Learning for Zero-shot Action Recognition](https://arxiv.org/abs/2205.01657)<br>:star:[code](https://github.com/microsoft/ResT)<br>零样本动作识别：跨模态表示学习
  * 小样本动作识别
    * [Hybrid Relation Guided Set Matching for Few-shot Action Recognition](https://arxiv.org/abs/2204.13423)<br>:star:[code](https://github.com/alibaba-mmai-research/HyRSM):newspaper:[解读](https://zhuanlan.zhihu.com/p/507053208)
    * [Motion-Modulated Temporal Fragment Alignment Network for Few-Shot Action Recognition](https://openaccess.thecvf.com/content/CVPR2022/papers/Wu_Motion-Modulated_Temporal_Fragment_Alignment_Network_for_Few-Shot_Action_Recognition_CVPR_2022_paper.pdf)
  * 时序动作检测
    * [An Empirical Study of End-to-End Temporal Action Detection](https://arxiv.org/abs/2204.02932)<br>:star:[code](https://github.com/xlliu7/E2E-TAD):newspaper:[粗解](https://zhuanlan.zhihu.com/p/494237617)
    * [RCL: Recurrent Continuous Localization for Temporal Action Detection](https://arxiv.org/abs/2203.07112)
* 时序动作定位
  * [Weakly Supervised Temporal Action Localization via Representative Snippet Knowledge Propagation](https://arxiv.org/abs/2203.02925)<br>:star:[code](https://github.com/LeonHLJ/RSKP):newspaper:[粗解](https://zhuanlan.zhihu.com/p/477624433)
  * [Unsupervised Pre-training for Temporal Action Localization Tasks](https://arxiv.org/abs/2203.13609)<br>:star:[code](https://github.com/zhang-can/UP-TAL)
  * [ASM-Loc: Action-aware Segment Modeling for Weakly-Supervised Temporal Action Localization](https://arxiv.org/abs/2203.15187)<br>:star:[code](https://github.com/boheumd/ASM-Loc)
  * [Fine-grained Temporal Contrastive Learning for Weakly-supervised Temporal Action Localization](https://arxiv.org/abs/2203.16800)<br>:star:[code](https://github.com/MengyuanChen21/CVPR2022-FTCL)
  * [Structured Attention Composition for Temporal Action Localization](https://arxiv.org/abs/2205.09956)<br>:star:[code](https://github.com/VividLe/Online-Action-Detection)
  * [Learning To Refactor Action and Co-Occurrence Features for Temporal Action Localization](https://openaccess.thecvf.com/content/CVPR2022/papers/Xia_Learning_To_Refactor_Action_and_Co-Occurrence_Features_for_Temporal_Action_CVPR_2022_paper.pdf)
* 重复动作计数
  * [TransRAC: Encoding Multi-scale Temporal Correlation with Transformers for Repetitive Action Counting](https://arxiv.org/abs/2204.01018)<br>:open_mouth:oral:star:[code](https://github.com/SvipRepetitionCounting/TransRAC):house:[project](https://svip-lab.github.io/dataset/RepCount_dataset.html)
* 组动作识别
  * [Dual-AI: Dual-path Action Interaction Learning for Group Activity Recognition](https://arxiv.org/abs/2204.02148)<br>:open_mouth:oral
  * [Detector-Free Weakly Supervised Group Activity Recognition](https://arxiv.org/abs/2204.02139)<br>:star:[code](https://github.com/dk-kim/DFWSGAR):house:[project](https://cvlab.postech.ac.kr/research/DFWSGAR/)
* 动作质量评估
  * [FineDiving: A Fine-grained Dataset for Procedure-aware Action Quality Assessment](https://arxiv.org/abs/2204.03646)<br>:open_mouth:oral:star:[code](https://github.com/xujinglin/FineDiving):house:[project](https://sites.google.com/view/finediving):newspaper:[解读](https://zhuanlan.zhihu.com/p/494913545)
* 活动识别
  * [Audio-Adaptive Activity Recognition Across Video Domains](https://arxiv.org/abs/2203.14240)<br>:star:[code](https://github.com/xiaobai1217/DomainAdaptation):house:[project](https://xiaobai1217.github.io/DomainAdaptation/)
  * 群体活动识别
    * [Dual-AI: Dual-path Actor Interaction Learning for Group Activity Recognition](https://openaccess.thecvf.com/content/CVPR2022/papers/Han_Dual-AI_Dual-Path_Actor_Interaction_Learning_for_Group_Activity_Recognition_CVPR_2022_paper.pdf)<br>:house:[project](https://mingfei.info/Dual-AI/)

<a name="7"/>

## 7.Point Cloud(点云)
* [Shape-invariant 3D Adversarial Point Clouds](https://arxiv.org/abs/2203.04041)<br>:star:[code](https://github.com/shikiw/SI-Adv)
* [AziNorm: Exploiting the Radial Symmetry of Point Cloud for Azimuth-Normalized 3D Perception](https://arxiv.org/pdf/2203.13090.pdf)<br>:star:[code](https://github.com/hustvl/AziNorm)
* [REGTR: End-to-end Point Cloud Correspondences with Transformers](https://arxiv.org/abs/2203.14517)<br>:star:[code](https://github.com/yewzijian/RegTR)
* [Equivariant Point Cloud Analysis via Learning Orientations for Message Passing](https://arxiv.org/abs/2203.14486)<br>:star:[code](https://github.com/luost26/Equivariant-OrientedMP)
* [Text2Pos: Text-to-Point-Cloud Cross-Modal Localization](https://arxiv.org/abs/2203.15125)<br>:star:[code](https://github.com/mako443/Text2Pos-CVPR2022):house:[project](https://text2pos.github.io/)
* [Deformation and Correspondence Aware Unsupervised Synthetic-to-Real Scene Flow Estimation for Point Clouds](https://arxiv.org/abs/2203.16895)<br>:star:[code](https://github.com/leolyj/DCA-SRSFE)
* [Self-Supervised Arbitrary-Scale Point Clouds Upsampling via Implicit Neural Representation](https://arxiv.org/abs/2204.08196)<br>:star:[code](https://github.com/xnowbzhao/sapcu):newspaper:[解读](https://zhuanlan.zhihu.com/p/501027339)
* [3DeformRS: Certifying Spatial Deformations on Point Clouds](https://arxiv.org/abs/2204.05687)<br>:star:[code](https://github.com/gaperezsa/3DeformRS)
* [Reconstructing Surfaces for Sparse Point Clouds with On-Surface Priors](https://arxiv.org/abs/2204.10603)<br>:star:[code](https://github.com/mabaorui/OnSurfacePrior):newspaper:[解读](https://zhuanlan.zhihu.com/p/504499515)
* [Density-preserving Deep Point Cloud Compression](https://arxiv.org/abs/2204.12684)<br>:star:[code](https://github.com/yunhe20/D-PCC):house:[project](https://yunhe20.github.io/D-PCC/):newspaper:[解读](https://zhuanlan.zhihu.com/p/506416975)
* [Surface Representation for Point Clouds](https://arxiv.org/abs/2205.05740)<br>:open_mouth:oral:star:[code](https://github.com/hancyran/RepSurf)<br>:newspaper:[解读1](https://zhuanlan.zhihu.com/p/513702674)<br>:newspaper:[解读2](https://mp.weixin.qq.com/s/FYn0S46OA6xraH9ofNIctw)
* [Neural Points: Point Cloud Representation With Neural Fields for Arbitrary Upsampling](https://arxiv.org/abs/2112.04148)<br>:star:[code](https://github.com/WanquanF/NeuralPoints)
* [Point Cloud Pre-Training With Natural 3D Structures](https://openaccess.thecvf.com/content/CVPR2022/papers/Yamada_Point_Cloud_Pre-Training_With_Natural_3D_Structures_CVPR_2022_paper.pdf)<br>:star:[code](https://github.com/ryosuke-yamada/3dfractaldb):house:[project](https://ryosuke-yamada.github.io/PointCloud-FractalDataBase/)
* [Not All Points Are Equal: Learning Highly Efficient Point-Based Detectors for 3D LiDAR Point Clouds](https://arxiv.org/abs/2203.11139)<br>:star:[code](https://github.com/yifanzhang713/IA-SSD)
* [Point2Cyl: Reverse Engineering 3D Objects from Point Clouds to Extrusion Cylinders](https://arxiv.org/abs/2112.09329)
* [RigidFlow: Self-Supervised Scene Flow Learning on Point Clouds by Local Rigidity Prior](https://openaccess.thecvf.com/content/CVPR2022/papers/Li_RigidFlow_Self-Supervised_Scene_Flow_Learning_on_Point_Clouds_by_Local_CVPR_2022_paper.pdf)
* [PatchFormer: An Efficient Point Transformer With Patch Attention](https://arxiv.org/abs/2111.00207)
* [PhyIR: Physics-Based Inverse Rendering for Panoramic Indoor Images](https://openaccess.thecvf.com/content/CVPR2022/papers/Yu_Finding_Good_Configurations_of_Planar_Primitives_in_Unorganized_Point_Clouds_CVPR_2022_paper.pdf)
* [Point Cloud Color Constancy](https://arxiv.org/abs/2111.11280)<br>:star:[code](https://github.com/xyxingx/Point-Cloud-Color-Constancy)
* [Multimodal Colored Point Cloud to Image Alignment](https://arxiv.org/abs/2110.03249)
* [Domain Adaptation on Point Clouds via Geometry-Aware Implicits](https://arxiv.org/abs/2112.09343)<br>:star:[code](https://github.com/Jhonve/ImplicitPCDA)
* 3D 点云
  * [Point-BERT: Pre-Training 3D Point Cloud Transformers With Masked Point Modeling](https://openaccess.thecvf.com/content/CVPR2022/papers/Yu_Point-BERT_Pre-Training_3D_Point_Cloud_Transformers_With_Masked_Point_Modeling_CVPR_2022_paper.pdf)<br>:star:[code](https://github.com/lulutang0608/Point-BERT)
  * [CrossPoint: Self-Supervised Cross-Modal Contrastive Learning for 3D Point Cloud Understanding](https://arxiv.org/abs/2203.00680)<br>:star:[code](https://github.com/MohamedAfham/CrossPoint):newspaper:[粗解](https://zhuanlan.zhihu.com/p/474565863)<br>CrossPoint，一个用于 3D 点云表征学习的简单自监督学习框架。虽然该方法是在合成的三维物体数据集上训练的，但在下游任务中的实验结果，如三维物体分类和三维物体部分分割，在合成和真实世界的数据集中都证明了该方法在学习可迁移表征方面的有效性。
  * [IDEA-Net: Dynamic 3D Point Cloud Interpolation via Deep Embedding Alignment](https://openaccess.thecvf.com/content/CVPR2022/papers/Zeng_IDEA-Net_Dynamic_3D_Point_Cloud_Interpolation_via_Deep_Embedding_Alignment_CVPR_2022_paper.pdf)<br>:star:[code](https://github.com/ZENGYIMING-EAMON/IDEA-Net)
  * [A Unified Query-based Paradigm for Point Cloud Understanding](https://arxiv.org/abs/2203.01252)<br>:star:[code](https://github.com/dvlab-research/DeepVision3D)
  * [WarpingGAN: Warping Multiple Uniform Priors for Adversarial 3D Point Cloud Generation](https://arxiv.org/abs/2203.12917)<br>:star:[code](https://github.com/yztang4/WarpingGAN)
  * [3DJCG: A Unified Framework for Joint Dense Captioning and Visual Grounding on 3D Point Clouds](https://openaccess.thecvf.com/content/CVPR2022/papers/Cai_3DJCG_A_Unified_Framework_for_Joint_Dense_Captioning_and_Visual_CVPR_2022_paper.pdf)
  * [Robust Structured Declarative Classifiers for 3D Point Clouds: Defending Adversarial Attacks With Implicit Gradients](https://arxiv.org/abs/2203.15245)<br>:house:[project](https://zhang-vislab.github.io/)
  * [Why Discard if You Can Recycle?: A Recycling Max Pooling Module for 3D Point Cloud Analysis](https://openaccess.thecvf.com/content/CVPR2022/papers/Chen_Why_Discard_if_You_Can_Recycle_A_Recycling_Max_Pooling_CVPR_2022_paper.pdf)
* 3D点云分割
    * [Stratified Transformer for 3D Point Cloud Segmentation](https://arxiv.org/abs/2203.14508)<br>:star:[code](https://github.com/dvlab-research/Stratified-Transformer)
* 点云分类
  * [ART-Point: Improving Rotation Robustness of Point Cloud Classifiers via Adversarial Rotation](https://arxiv.org/abs/2203.03888)<br>:star:[code](https://github.com/robinwang1/ART-Point):newspaper:[粗解](https://zhuanlan.zhihu.com/p/478070143)[:notebook:](https://mp.weixin.qq.com/s/lSY1is6Fmm6A0Db0Jxo4qg)
* 点云配准
  * [SC^2-PCR: A Second Order Spatial Compatibility for Efficient and Robust Point Cloud Registration](https://arxiv.org/abs/2203.14453)<br>:star:[code](https://github.com/ZhiChen902/SC2-PCR)<br>:newspaper:[二阶相似性测度，让传统配准方法取得比深度学习更好的性能，并达到深度学习的速度](https://mp.weixin.qq.com/s/pOVgC4nvE4YCxe3hyLmkGA)
  * [Multi-Instance Point Cloud Registration by Efficient Correspondence Clustering](https://arxiv.org/abs/2111.14582)<br>:star:[code](https://github.com/SJTU-ViSYS/multi-instant-reg)
  * [Deterministic Point Cloud Registration via Novel Transformation Decomposition](https://openaccess.thecvf.com/content/CVPR2022/papers/Chen_Deterministic_Point_Cloud_Registration_via_Novel_Transformation_Decomposition_CVPR_2022_paper.pdf)<br>:newspaper:[解读](https://mp.weixin.qq.com/s/FYn0S46OA6xraH9ofNIctw)
  * [SC2-PCR: A Second Order Spatial Compatibility for Efficient and Robust Point Cloud Registration](https://openaccess.thecvf.com/content/CVPR2022/papers/Chen_SC2-PCR_A_Second_Order_Spatial_Compatibility_for_Efficient_and_Robust_CVPR_2022_paper.pdf)<br>:star:[code](https://github.com/ZhiChen902/SC2-PCR)
* 点云补全
  * [Learning a Structured Latent Space for Unsupervised Point Cloud Completion](https://arxiv.org/abs/2203.15580) 
  * [Learning Local Displacements for Point Cloud Completion](https://arxiv.org/abs/2203.16600) 
  * [LAKe-Net: Topology-Aware Point Cloud Completionby Localizing Aligned Keypoints](https://arxiv.org/abs/2203.16771)<br>:newspaper:[粗解](https://news.sjtu.edu.cn/jdzh/20220330/169456.html)
* 点云分割
  * [Contrastive Boundary Learning for Point Cloud Segmentation](https://arxiv.org/abs/2203.05272)<br>:star:[code](https://github.com/LiyaoTang/contrastBoundary):newspaper:[解读](https://mp.weixin.qq.com/s/lSY1is6Fmm6A0Db0Jxo4qg)
  * [SemAffiNet: Semantic-Affine Transformation for Point Cloud Segmentation](https://arxiv.org/abs/2205.13490)<br>:star:[code](https://github.com/wangzy22/SemAffiNet):newspaper:[解读](https://zhuanlan.zhihu.com/p/520636472)
  * [An MIL-Derived Transformer for Weakly Supervised Point Cloud Segmentation](https://openaccess.thecvf.com/content/CVPR2022/papers/Yang_An_MIL-Derived_Transformer_for_Weakly_Supervised_Point_Cloud_Segmentation_CVPR_2022_paper.pdf)<br>:star:[code](https://github.com/jimmy15923/wspss_mil_transformer)
  * [Pyramid Architecture for Multi-Scale Processing in Point Cloud Segmentation](https://openaccess.thecvf.com/content/CVPR2022/papers/Nie_Pyramid_Architecture_for_Multi-Scale_Processing_in_Point_Cloud_Segmentation_CVPR_2022_paper.pdf)<br>:star:[code](https://github.com/ginobilinie/kp_pyramid)
* 点云匹配
  * [Lepard: Learning Partial Point Cloud Matching in Rigid and Deformable Scenes](https://arxiv.org/abs/2111.12591)<br>:star:[code](https://github.com/rabbityl/lepard)
* 场景流估计
  * [RCP: Recurrent Closest Point for Scene Flow Estimation on 3D Point Clouds](https://arxiv.org/abs/2205.11028)

<a name="6"/>

## 6.Object Tracking(目标跟踪)
* [TCTrack: Temporal Contexts for Aerial Tracking](https://arxiv.org/abs/2203.01885)<br>:star:[code](https://github.com/vision4robotics/TCTrack):newspaper:[粗解](https://zhuanlan.zhihu.com/p/475067096)<br>:newspaper:[TCTrack: 用于空中跟踪的时序信息框架](https://mp.weixin.qq.com/s/4UgyAwg1MJJH9Aajl35q9w)
* [Correlation-Aware Deep Tracking](https://arxiv.org/abs/2203.01666)
* [Global Tracking Transformers](https://arxiv.org/pdf/2203.13250.pdf)<br>:star:[code](https://github.com/xingyizhou/GTR)
* [Unified Transformer Tracker for Object Tracking](https://arxiv.org/abs/2203.15175)<br>:star:[code](https://github.com/Flowerfan/Trackron)
* [Global Tracking via Ensemble of Local Trackers](https://arxiv.org/abs/2203.16092)
* [Unsupervised Learning of Accurate Siamese Tracking](https://arxiv.org/abs/2204.01475)<br>:star:[code](https://github.com/FlorinShum/ULAST)
* [Transformer Tracking with Cyclic Shifting Window Attention](https://arxiv.org/abs/2205.03806)<br>:star:[code](https://github.com/SkyeSong38/CSWinTT)<br>Transformer 跟踪：循环为一窗口注意力模型。该算法在五个数据集VOT2020, UAV123, LaSOT, TrackingNet, GOT-10k上均实现了新的SOTA.
* [Tracking People by Predicting 3D Appearance, Location and Pose](https://openaccess.thecvf.com/content/CVPR2022/papers/Rajasegaran_Tracking_People_by_Predicting_3D_Appearance_Location_and_Pose_CVPR_2022_paper.pdf)<br>:open_mouth:oral:star:[code](https://github.com/brjathu/PHALP):house:[project](http://people.eecs.berkeley.edu/~jathushan/PHALP/)
* [Cannot See the Forest for the Trees: Aggregating Multiple Viewpoints to Better Classify Objects in Videos](https://arxiv.org/abs/2206.02116)<br>:star:[code](https://github.com/sukjunhwang/set_classifier)
* [Opening Up Open World Tracking](https://arxiv.org/abs/2104.11221)<br>:open_mouth:oral:star:[code](https://github.com/YangLiu14/Open-World-Tracking):house:[project](https://openworldtracking.github.io/) 
* [Transforming Model Prediction for Tracking](https://arxiv.org/abs/2203.11192)<br>:star:[code](https://github.com/visionml/pytracking)
* [PyMiceTracking: An Open-Source Toolbox for Real-Time Behavioral Neuroscience Experiments](https://openaccess.thecvf.com/content/CVPR2022/papers/Menezes_PyMiceTracking_An_Open-Source_Toolbox_for_Real-Time_Behavioral_Neuroscience_Experiments_CVPR_2022_paper.pdf)<br>:star:[code](https://github.com/xarmison/proj-pca)
* [Spiking Transformers for Event-Based Single Object Tracking](https://openaccess.thecvf.com/content/CVPR2022/papers/Zhang_Spiking_Transformers_for_Event-Based_Single_Object_Tracking_CVPR_2022_paper.pdf)<br>:star:[code](https://github.com/JeeKing/CVPR2022_STNet)
* 3D 目标跟踪
  * [Beyond 3D Siamese Tracking: A Motion-Centric Paradigm for 3D Single Object Tracking in Point Clouds](https://arxiv.org/abs/2203.01730)<br>:star:[code](https://github.com/Ghostish/Open3DSOT):newspaper:[粗解](https://zhuanlan.zhihu.com/p/475067096)
  * [Iterative Corresponding Geometry: Fusing Region and Depth for Highly Efficient 3D Tracking of Textureless Objects](https://arxiv.org/abs/2203.05334)<br>:star:[code](https://github.com/DLR-RM/3DObjectTracking)
  * [BCOT: A Markerless High-Precision 3D Object Tracking Benchmark](https://arxiv.org/abs/2203.13437)<br>:star:[code](https://ar3dv.github.io/BCOT-Benchmark/)
* 多目标跟踪
  * [Learning of Global Objective for Network Flow in Multi-Object Tracking](https://arxiv.org/abs/2203.16210)
  * [MeMOT: Multi-Object Tracking with Memory](https://arxiv.org/abs/2203.16761)<br>:open_mouth:oral
  * [Multi-Object Tracking Meets Moving UAV](https://openaccess.thecvf.com/content/CVPR2022/papers/Liu_Multi-Object_Tracking_Meets_Moving_UAV_CVPR_2022_paper.pdf)
  * [Adiabatic Quantum Computing for Multi Object Tracking](http://arxiv.org/abs/2202.08837)
  * [Towards Discriminative Representation: Multi-View Trajectory Contrastive Learning for Online Multi-Object Tracking](https://arxiv.org/abs/2203.14208)
  * [LMGP: Lifted Multicut Meets Geometry Projections for Multi-Camera Multi-Object Tracking](https://arxiv.org/abs/2111.11892)<br>:star:[code](https://github.com/nhmduy/LMGP)
* RGB-T跟踪
  * [Visible-Thermal UAV Tracking: A Large-Scale Benchmark and New Baseline](https://arxiv.org/abs/2204.04120)<br>:house:[project](https://zhang-pengyu.github.io/DUT-VTUAV/):newspaper:[解读](https://zhuanlan.zhihu.com/p/496386916)
* 视觉跟踪
  * [Ranking-Based Siamese Visual Tracking](https://arxiv.org/abs/2205.11761)<br>:star:[code](https://github.com/sansanfree/RBO):newspaper:[解读](https://zhuanlan.zhihu.com/p/519556254) 
* 夜间跟踪
  * [Unsupervised Domain Adaptation for Nighttime Aerial Tracking](https://arxiv.org/abs/2203.10541)<br>:star:[code](https://github.com/vision4robotics/UDAT)

<a name="5"/>

## 5.Object Detection(目标检测)
* [DN-DETR: Accelerate DETR Training by Introducing Query DeNoising](https://arxiv.org/abs/2203.01305)<br>:star:[code](https://github.com/FengLi-ust/DN-DETR):newspaper:[粗解](https://zhuanlan.zhihu.com/p/475100003/)
* [Overcoming Catastrophic Forgetting in Incremental Object Detection via Elastic Response Distillation](https://arxiv.org/abs/2204.02136)<br>:star:[code](https://github.com/Hi-FT/ERD)
* [ESCNet: Gaze Target Detection with the Understanding of 3D Scenes](https://openaccess.thecvf.com/content/CVPR2022/papers/Bao_ESCNet_Gaze_Target_Detection_With_the_Understanding_of_3D_Scenes_CVPR_2022_paper.pdf)<br>:star:[code](https://github.com/bjj9/ESCNet)
* [Segment and Complete: Defending Object Detectors Against Adversarial Patch Attacks With Robust Patch Detection](https://arxiv.org/abs/2112.04532)<br>:star:[code](https://github.com/joellliu/SegmentAndComplete)
* [Interactron: Embodied Adaptive Object Detection](https://arxiv.org/abs/2202.00660)<br>:star:[code](https://github.com/allenai/interactron)
* [Beyond Bounding Box: Multimodal Knowledge Learning for Object Detection](https://arxiv.org/abs/2205.04072)<br>以往目标检测往往以目标包围框作为标注训练，作者引入语言提示信息，提炼语言知识到目标检测模型中，获得了1.6~2.1%的性能增益。
* [Dynamic Sparse R-CNN](https://arxiv.org/abs/2205.02101)
* [Unknown-Aware Object Detection: Learning What You Don't Know from Videos in the Wild](https://arxiv.org/abs/2203.03800)<br>:star:[code](https://github.com/deeplearning-wisc/stud):newspaper:[粗解](https://zhuanlan.zhihu.com/p/478070143)
* [Focal and Global Knowledge Distillation for Detectors](https://arxiv.org/abs/2111.11837)<br>:star:[code](https://github.com/yzd-v/FGD):newspaper:[解读](https://zhuanlan.zhihu.com/p/477707304)<br>关于目标检测的知识蒸馏工作，只需要30行代码就可以在 anchor-base, anchor-free 的单阶段、两阶段各种检测器上稳定涨点，现在代码已经开源。
* [Group R-CNN for Weakly Semi-supervised Object Detection with Points](https://arxiv.org/abs/2205.05920)<br>:star:[code](https://github.com/jshilong/GroupRCNN)<br>:newspaper:[解读](https://zhuanlan.zhihu.com/p/513702674)
* [Real-time Object Detection for Streaming Perception](https://arxiv.org/abs/2203.12338)<br>:star:[code](https://github.com/yancie-yjr/StreamYOLO):newspaper:[解读](https://mp.weixin.qq.com/s/kZue3ds348UXQI86xrwudQ)
* [Ev-TTA: Test-Time Adaptation for Event-Based Object Recognition](https://arxiv.org/abs/2203.12247)
* [Learning to Prompt for Open-Vocabulary Object Detection with Vision-Language Model](https://arxiv.org/abs/2203.14940)<br>:star:[code](https://github.com/dyabel/detpro)
* [Optimal Correction Cost for Object Detection Evaluation](https://arxiv.org/abs/2203.14438)
* [Expanding Low-Density Latent Regions for Open-Set Object Detection](https://arxiv.org/abs/2203.14911)<br>:star:[code](https://github.com/csuhan/opendet2)<br>:newspaper:[解读](https://mp.weixin.qq.com/s/FYn0S46OA6xraH9ofNIctw)
* [SIOD: Single Instance Annotated Per Category Per Image for Object Detection](https://arxiv.org/abs/2203.15353)<br>:star:[code](https://github.com/solicucu/SIOD)<br>:newspaper:[解读](https://mp.weixin.qq.com/s/FYn0S46OA6xraH9ofNIctw)
* [Task-specific Inconsistency Alignment for Domain Adaptive Object Detection](https://arxiv.org/abs/2203.15345)<br>:star:[code](https://github.com/MCG-NJU/TIA)
* [Zero-Query Transfer Attacks on Context-Aware Object Detectors](https://arxiv.org/abs/2203.15230)
* [AdaMixer: A Fast-Converging Query-Based Object Detector](https://arxiv.org/abs/2203.16507)<br>:open_mouth:oral:star:[code](https://github.com/MCG-NJU/AdaMixer)
* [Learning to Detect Mobile Objects from LiDAR Scans Without Labels](https://arxiv.org/abs/2203.15882)<br>:star:[code](https://github.com/YurongYou/MODEST)
* [Forecasting from LiDAR via Future Object Detection](https://arxiv.org/abs/2203.16297)<br>:star:[code](https://github.com/neeharperi/FutureDet)
* [Target-aware Dual Adversarial Learning and a Multi-scenario Multi-Modality Benchmark to Fuse Infrared and Visible for Object Detection](https://arxiv.org/abs/2203.16220)<br>:open_mouth:oral:star:[code](https://github.com/dlut-dimt/TarDAL)
* [Multi-Granularity Alignment Domain Adaptation for Object Detection](https://arxiv.org/abs/2203.16897)
* [Proper Reuse of Image Classification Features Improves Object Detection](https://arxiv.org/abs/2204.00484)<br>:star:[code](https://github.com/tensorflow/models/blob/master/official/projects/backbone_reuse/README.md)
* [R(Det)^2: Randomized Decision Routing for Object Detection](https://arxiv.org/abs/2204.00794)
* [Towards Robust Adaptive Object Detection under Noisy Annotations](https://arxiv.org/abs/2204.02620)<br>:star:[code](https://github.com/CityU-AIM-Group/NLTE)
* [Entropy-based Active Learning for Object Detection with Progressive Diversity Constraint](https://arxiv.org/abs/2204.07965)
* [Target-Relevant Knowledge Preservation for Multi-Source Domain Adaptive Object Detection](https://arxiv.org/abs/2204.07964)
* [Interactive Segmentation and Visualization for Tiny Objects in Multi-megapixel Images](https://arxiv.org/abs/2204.10356)<br>:star:[code](https://github.com/cy-xu/cosmic-conn)
* [Cross Domain Object Detection by Target-Perceived Dual Branch Distillation](https://arxiv.org/abs/2205.01291)<br>:star:[code](https://github.com/Feobi1999/TDD)<br>跨域目标检测：目标感知双分支蒸馏
* [Progressive End-to-End Object Detection in Crowded Scenes](https://arxiv.org/abs/2203.07669)<br>:star:[code](https://github.com/megvii-research/Iter-E2EDET)<br>:newspaper:[解读](https://mp.weixin.qq.com/s/kZue3ds348UXQI86xrwudQ)
* [HCSC: Hierarchical Contrastive Selective Coding](https://arxiv.org/abs/2202.00455)<br>:star:[code](https://github.com/gyfastas/HCSC)<br>:newspaper:[CNN自监督预训练新SOTA：上交、Mila、字节联合提出具有层级结构的图像表征自学习新框架](https://mp.weixin.qq.com/s/XQodgVDIl40otl64wpHutg)
* [Recurrent Glimpse-based Decoder for Detection with Transformer](https://arxiv.org/abs/2112.04632)<br>:open_mouth:oral:star:[code](https://github.com/zhechen/Deformable-DETR-REGO)<br>:newspaper:[解读](https://mp.weixin.qq.com/s/lSY1is6Fmm6A0Db0Jxo4qg)
* [Continual Object Detection via Prototypical Task Correlation Guided Gating Mechanism](https://arxiv.org/abs/2205.03055)<br>:star:[code](https://github.com/dkxocl/ROSSETA)
* [Balanced and Hierarchical Relation Learning for One-Shot Object Detection](https://openaccess.thecvf.com/content/CVPR2022/papers/Yang_Balanced_and_Hierarchical_Relation_Learning_for_One-Shot_Object_Detection_CVPR_2022_paper.pdf)<br>:star:[code](https://github.com/hero-y/BHRL)
* [Accelerating DETR Convergence via Semantic-Aligned Matching](https://arxiv.org/abs/2203.06883)<br>:star:[code](https://github.com/ZhangGongjie/SAM-DETR) 
* [DETReg: Unsupervised Pretraining With Region Priors for Object Detection](https://arxiv.org/abs/2106.04550)<br>:star:[code](https://github.com/amirbar/DETReg):house:[project](https://www.amirbar.net/detreg/)
* [Source-Free Object Detection by Learning To Overlook Domain Style](https://openaccess.thecvf.com/content/CVPR2022/papers/Li_Source-Free_Object_Detection_by_Learning_To_Overlook_Domain_Style_CVPR_2022_paper.pdf)
* [DESTR: Object Detection With Split Transformer](https://openaccess.thecvf.com/content/CVPR2022/papers/He_DESTR_Object_Detection_With_Split_Transformer_CVPR_2022_paper.pdf)
* [SmartAdapt: Multi-Branch Object Detection Framework for Videos on Mobiles](https://openaccess.thecvf.com/content/CVPR2022/papers/Xu_SmartAdapt_Multi-Branch_Object_Detection_Framework_for_Videos_on_Mobiles_CVPR_2022_paper.pdf)
* [Explore Spatio-Temporal Aggregation for Insubstantial Object Detection: Benchmark Dataset and Baseline](https://openaccess.thecvf.com/content/CVPR2022/papers/Zhou_Explore_Spatio-Temporal_Aggregation_for_Insubstantial_Object_Detection_Benchmark_Dataset_and_CVPR_2022_paper.pdf)<br>:star:[code](https://github.com/CalayZhou/IOD-Video)
* [Exploring Endogenous Shift for Cross-Domain Detection: A Large-Scale Benchmark and Perturbation Suppression Network](https://openaccess.thecvf.com/content/CVPR2022/papers/Tao_Exploring_Endogenous_Shift_for_Cross-Domain_Detection_A_Large-Scale_Benchmark_and_CVPR_2022_paper.pdf)
* [Not All Labels Are Equal: Rationalizing the Labeling Costs for Training Object Detection](https://arxiv.org/abs/2106.11921)<br>:star:[code](https://github.com/NVlabs/AL-SSL)
* [Training Object Detectors From Scratch: An Empirical Study in the Era of Vision Transformer](https://openaccess.thecvf.com/content/CVPR2022/papers/Hong_Training_Object_Detectors_From_Scratch_An_Empirical_Study_in_the_CVPR_2022_paper.pdf)
* [Sequential Voting With Relational Box Fields for Active Object Detection](https://arxiv.org/abs/2110.11524)<br>:star:[code](https://github.com/fuqichen1998/SequentialVotingDet):house:[project](https://fuqichen1998.github.io/SequentialVotingDet/)
* [Simple Multi-dataset Detection](https://openaccess.thecvf.com/content/CVPR2022/papers/Zhou_Simple_Multi-Dataset_Detection_CVPR_2022_paper.pdf)<br>:star:[code](https://github.com/xingyizhou/UniDet)
* 小目标检测
  * [QueryDet: Cascaded Sparse Query for Accelerating High-Resolution Small Object Detection](https://arxiv.org/abs/2103.09136)<br>:star:[code](https://github.com/ChenhongyiYang/QueryDet-PyTorch)
  * [Interactive Multi-Class Tiny-Object Detection](https://arxiv.org/abs/2203.15266)<br>:star:[code](https://github.com/ChungYi347/Interactive-Multi-Class-Tiny-Object-Detection)
  * [ISNet: Shape Matters for Infrared Small Target Detection](https://openaccess.thecvf.com/content/CVPR2022/papers/Zhang_ISNet_Shape_Matters_for_Infrared_Small_Target_Detection_CVPR_2022_paper.pdf)<br>:star:[code](https://github.com/RuiZhang97/ISNe)<br>:newspaper:[解读](https://mp.weixin.qq.com/s/lSY1is6Fmm6A0Db0Jxo4qg)
* 小样本目标检测  
  * [Sylph: A Hypernetwork Framework for Incremental Few-shot Object Detection](https://arxiv.org/abs/2203.13903)
  * [Few-Shot Object Detection with Fully Cross-Transformer](https://arxiv.org/abs/2203.15021)
  * [Kernelized Few-Shot Object Detection With Efficient Integral Aggregation](https://openaccess.thecvf.com/content/CVPR2022/papers/Zhang_Kernelized_Few-Shot_Object_Detection_With_Efficient_Integral_Aggregation_CVPR_2022_paper.pdf)<br>:star:[code](https://github.com/ZS123-lang/KFSOD)
* 目标定位
  * [Weakly Supervised Object Localization as Domain Adaption](https://arxiv.org/abs/2203.01714)<br>:star:[code](https://github.com/zh460045050/DA-WSOL_CVPR2022):newspaper:[粗解](https://zhuanlan.zhihu.com/p/475067096)
  * [Bridging the Gap between Classification and Localization for Weakly Supervised Object Localization](https://arxiv.org/abs/2204.00220)
  * [Object Localization under Single Coarse Point Supervision](https://arxiv.org/abs/2203.09338)<br>:star:[code](https://github.com/ucas-vg/PointTinyBenchmark/)<br>:newspaper:[解读](https://mp.weixin.qq.com/s/lSY1is6Fmm6A0Db0Jxo4qg)
  * [CREAM: Weakly Supervised Object Localization via Class RE-Activation Mapping](https://arxiv.org/abs/2205.13922)<br>:star:[code](https://github.com/Jazzcharles/CREAM)
* 3D目标检测
  * [A Versatile Multi-View Framework for LiDAR-based 3D Object Detection with Guidance from Panoptic Segmentation](https://arxiv.org/abs/2203.02133)
  * [Voxel Set Transformer: A Set-to-Set Approach to 3D Object Detection From Point Clouds](https://arxiv.org/abs/2203.10314)<br>:star:[code](https://github.com/skyhehe123/VoxSeT)
  * [Pseudo-Stereo for Monocular 3D Object Detection in Autonomous Driving](https://arxiv.org/abs/2203.02112)<br>:star:[code](https://github.com/revisitq/Pseudo-Stereo-3D):newspaper:[粗解](https://zhuanlan.zhihu.com/p/476923554)
  * [Rope3D: TheRoadside Perception Dataset for Autonomous Driving and Monocular 3D Object Detection Task](https://arxiv.org/abs/2203.13608)<br>:house:[project](https://thudair.baai.ac.cn/rope)
  * [Point2Seq: Detecting 3D Objects as Sequences](https://arxiv.org/abs/2203.13394)<br>:star:[code](https://github.com/ocNflag/point2seq)
  * [MonoDETR: Depth-aware Transformer for Monocular 3D Object Detection](https://arxiv.org/abs/2203.13310)<br>:star:[code](https://github.com/ZrrSkywalker/MonoDETR)
  * [Canonical Voting: Towards Robust Oriented Bounding Box Detection in 3D Scenes](https://arxiv.org/abs/2011.12001)<br>:star:[code](https://github.com/qq456cvb/CanonicalVoting)<br>:newspaper:[粗解](https://news.sjtu.edu.cn/jdzh/20220330/169456.html)
  * [Exploring Geometric Consistency for Monocular 3D Object Detection](http://arxiv.org/abs/2104.05858)
  * [LiDAR Snowfall Simulation for Robust 3D Object Detection](https://arxiv.org/abs/2203.15118)<br>:open_mouth:oral:star:[code](https://github.com/SysCV/LiDAR_snow_sim)
  * [CAT-Det: Contrastively Augmented Transformer for Multi-modal 3D Object Detection](https://arxiv.org/abs/2204.00325)
  * [Homography Loss for Monocular 3D Object Detection](https://arxiv.org/abs/2204.00754)
  * [HyperDet3D: Learning a Scene-conditioned 3D Object Detector](https://arxiv.org/abs/2204.05599)
  * [DAIR-V2X: A Large-Scale Dataset for Vehicle-Infrastructure Cooperative 3D Object Detection](https://arxiv.org/abs/2204.05575)<br>:star:[code](https://github.com/AIR-THU/DAIR-V2X)
  * [OccAM's Laser: Occlusion-based Attribution Maps for 3D Object Detectors on LiDAR Data](https://arxiv.org/abs/2204.06577)<br>:star:[code](https://github.com/dschinagl/occam) 
  * [Focal Sparse Convolutional Networks for 3D Object Detection](https://arxiv.org/abs/2204.12463)<br>:open_mouth:oral:star:[code](https://github.com/open-mmlab/OpenPCDet):newspaper:[解读](https://zhuanlan.zhihu.com/p/505802169)[:notebook:](https://mp.weixin.qq.com/s/kZue3ds348UXQI86xrwudQ)
  * [Rotationally Equivariant 3D Object Detection](https://arxiv.org/abs/2204.13630)<br>:house:[project](https://kovenyu.com/eon/)
  * [Bridged Transformer for Vision and Point Cloud 3D Object Detection](https://fengxianghe.github.io/paper/wang2022bridged.pdf)<br>:newspaper:[解读](https://mp.weixin.qq.com/s/lSY1is6Fmm6A0Db0Jxo4qg)
  * [Sparse Fuse Dense: Towards High Quality 3D Detection with Depth Completion](https://arxiv.org/abs/2203.09780)<br>:newspaper:[解读](https://mp.weixin.qq.com/s/Ao_kZlAW_AqPcMdXg2Zerg)
  * [VISTA: Boosting 3D Object Detection via Dual Cross-VIew SpaTial Attention](https://arxiv.org/abs/2203.09704)<br>:star:[code](https://github.com/Gorilla-Lab-SCUT/VISTA)<br>:newspaper:[华南理工提出VISTA：双跨视角空间注意力机制实现3D目标检测SOTA，即插即用](https://mp.weixin.qq.com/s/QJmqRk0tqZd-4Io1TBM-9g)
  * [Diversity Matters: Fully Exploiting Depth Clues for Reliable Monocular 3D Object Detection](https://arxiv.org/abs/2205.09373)<br>:open_mouth:oral
  * [MonoDTR: Monocular 3D Object Detection With Depth-Aware Transformer](https://arxiv.org/abs/2203.10981)<br>:star:[code](https://github.com/kuanchihhuang/MonoDTR)
  * [Voxel Field Fusion for 3D Object Detection](https://arxiv.org/abs/2205.15938)<br>:star:[code](https://github.com/dvlab-research/VFF)<br>:newspaper:[解读](https://mp.weixin.qq.com/s/kZue3ds348UXQI86xrwudQ)
  * [DisARM: Displacement Aware Relation Module for 3D Detection](https://arxiv.org/abs/2203.01152)<br>:star:[code](https://github.com/YaraDuan/DisARM)
  * [Back to Reality: Weakly-supervised 3D Object Detection with Shape-guided Label Enhancement](https://arxiv.org/abs/2203.05238)<br>:star:[code](https://github.com/wyf-ACCEPT/BackToReality)
  * [Embracing Single Stride 3D Object Detector With Sparse Transformer](https://arxiv.org/abs/2112.06375)<br>:star:[code](https://github.com/TuSimple/SST)
  * [3D-VField: Adversarial Augmentation of Point Clouds for Domain Generalization in 3D Object Detection](https://openaccess.thecvf.com/content/CVPR2022/papers/Lehner_3D-VField_Adversarial_Augmentation_of_Point_Clouds_for_Domain_Generalization_in_CVPR_2022_paper.pdf)<br>:house:[project](https://crashd-cars.github.io/)
  * [Dimension Embeddings for Monocular 3D Object Detection](https://openaccess.thecvf.com/content/CVPR2022/papers/Zhang_Dimension_Embeddings_for_Monocular_3D_Object_Detection_CVPR_2022_paper.pdf)
  * [MonoJSG: Joint Semantic and Geometric Cost Volume for Monocular 3D Object Detection](https://arxiv.org/abs/2203.08563)<br>:star:[code](https://github.com/lianqing11/MonoJSG)
  * [RBGNet: Ray-Based Grouping for 3D Object Detection](https://arxiv.org/abs/2204.02251)<br>:star:[code](https://github.com/Haiyang-W/RBGNet)
* 伪装目标检测
  * [Zoom In and Out: A Mixed-scale Triplet Network for Camouflaged Object Detection](https://arxiv.org/abs/2203.02688)<br>:star:[code](https://github.com/lartpang/ZoomNet)
  * [Detecting Camouflaged Object in Frequency Domain](https://openaccess.thecvf.com/content/CVPR2022/papers/Zhong_Detecting_Camouflaged_Object_in_Frequency_Domain_CVPR_2022_paper.pdf)
  * [Implicit Motion Handling for Video Camouflaged Object Detection](https://arxiv.org/abs/2203.07363)<br>:house:[project](https://xueliancheng.github.io/SLT-Net-project/)
* 全监督目标检测
  * [Omni-DETR: Omni-Supervised Object Detection with Transformers](https://arxiv.org/abs/2203.16089)<br>:star:[code](https://github.com/amazon-research/omni-detr)  
* 半监督目标检测
  * [Dense Learning based Semi-Supervised Object Detection](https://arxiv.org/abs/2204.07300)<br>:star:[code](https://github.com/chenbinghui1/DSL):newspaper:[解读](https://zhuanlan.zhihu.com/p/500351469) 
  * [Label Matching Semi-Supervised Object Detection](https://openaccess.thecvf.com/content/CVPR2022/papers/Chen_Label_Matching_Semi-Supervised_Object_Detection_CVPR_2022_paper.pdf)<br>:star:[code](https://github.com/HIK-LAB/SSOD)
  * [Semi-Supervised Object Detection via Multi-Instance Alignment With Global Class Prototypes](https://openaccess.thecvf.com/content/CVPR2022/papers/Li_Semi-Supervised_Object_Detection_via_Multi-Instance_Alignment_With_Global_Class_Prototypes_CVPR_2022_paper.pdf)
  * [Active Teacher for Semi-Supervised Object Detection](https://openaccess.thecvf.com/content/CVPR2022/papers/Mi_Active_Teacher_for_Semi-Supervised_Object_Detection_CVPR_2022_paper.pdf)<br>:star:[code](https://github.com/HunterJ-Lin/ActiveTeacher)
* 弱监督目标检测
  * [Salvage of Supervision in Weakly Supervised Object Detection](https://arxiv.org/abs/2106.04073)
  * [Background Activation Suppression for Weakly Supervised Object Localization](https://arxiv.org/abs/2112.00580)<br>:star:[code](https://github.com/wpy1999/BAS)
  * [H2FA R-CNN: Holistic and Hierarchical Feature Alignment for Cross-Domain Weakly Supervised Object Detection](https://openaccess.thecvf.com/content/CVPR2022/papers/Xu_H2FA_R-CNN_Holistic_and_Hierarchical_Feature_Alignment_for_Cross-Domain_Weakly_CVPR_2022_paper.pdf)<br>:star:[code](https://github.com/XuYunqiu/H2FA_R-CNN)
  * 显著目标检测
  * [Pyramid Grafting Network for One-Stage High Resolution Saliency Detection](https://arxiv.org/abs/2204.05041)<br>:star:[code](https://github.com/iCVTEAM/PGNet):newspaper:[解读](https://zhuanlan.zhihu.com/p/497009845)<br>:newspaper:[超高分辨率显著目标检测，新颖高效的错层嫁接架构PGNet（CVPR2022）](https://mp.weixin.qq.com/s/4QyMkoz1aLKQ4pkUsMDVpA)
  * [Learning from Pixel-Level Noisy Label : A New Perspective for Light Field Saliency Detection](https://arxiv.org/abs/2204.13456)<br>:star:[code](https://github.com/OLobbCode/NoiseLF):newspaper:[解读](https://zhuanlan.zhihu.com/p/507053208)
  * [Bi-directional Object-context Prioritization Learning for Saliency Ranking](https://arxiv.org/abs/2203.09416)<br>:star:[code](https://github.com/GrassBro/OCOR)
  * [Multi-Source Uncertainty Mining for Deep Unsupervised Saliency Detection](https://openaccess.thecvf.com/content/CVPR2022/papers/Wang_Multi-Source_Uncertainty_Mining_for_Deep_Unsupervised_Saliency_Detection_CVPR_2022_paper.pdf)
  * [Learning From Pixel-Level Noisy Label: A New Perspective for Light Field Saliency Detection](https://arxiv.org/abs/2204.13456)<br>:star:[code](https://github.com/OLobbCode/NoiseLF)
* 密集目标检测
  * [Revisiting AP Loss for Dense Object Detection: Adaptive Ranking Pair Selection](https://openaccess.thecvf.com/content/CVPR2022/papers/Xu_Revisiting_AP_Loss_for_Dense_Object_Detection_Adaptive_Ranking_Pair_CVPR_2022_paper.pdf)<br>:star:[code](https://github.com/Xudangliatiger/APE-Loss)
* Co-Salient目标检测
  * [Democracy Does Matter: Comprehensive Feature Mining for Co-Salient Object Detection](https://arxiv.org/abs/2203.05787)<br>:star:[code](https://github.com/siyueyu/DCFM)
  * [Can You Spot the Chameleon? Adversarially Camouflaging Images From Co-Salient Object Detection](https://openaccess.thecvf.com/content/CVPR2022/papers/Gao_Can_You_Spot_the_Chameleon_Adversarially_Camouflaging_Images_From_Co-Salient_CVPR_2022_paper.pdf)<br>:star:[code](https://github.com/tsingqguo/jadena)
* 长尾目标检测
  * [C2AM Loss: Chasing a Better Decision Boundary for Long-Tail Object Detection](https://openaccess.thecvf.com/content/CVPR2022/papers/Wang_C2AM_Loss_Chasing_a_Better_Decision_Boundary_for_Long-Tail_Object_CVPR_2022_paper.pdf)
  * [Equalized Focal Loss for Dense Long-Tailed Object Detection](https://arxiv.org/abs/2201.02593)<br>:star:[code](https://github.com/ModelTC/United-Perception)
* 旋转目标检测
  * [OSKDet: Orientation-Sensitive Keypoint Localization for Rotated Object Detection](https://openaccess.thecvf.com/content/CVPR2022/papers/Lu_OSKDet_Orientation-Sensitive_Keypoint_Localization_for_Rotated_Object_Detection_CVPR_2022_paper.pdf)
* 关键点检测
  * [Self-Supervised Equivariant Learning for Oriented Keypoint Detection](https://arxiv.org/abs/2204.08613)<br>:star:[code](https://github.com/bluedream1121/REKD):house:[project](http://cvlab.postech.ac.kr/research/REKD/)
  * [UKPGAN: A General Self-Supervised Keypoint Detector](https://arxiv.org/abs/2011.11974)<br>:star:[code](https://github.com/qq456cvb/UKPGAN)<br>:newspaper:[粗解](https://news.sjtu.edu.cn/jdzh/20220330/169456.html)
  * [Contour-Hugging Heatmaps for Landmark Detection](https://openaccess.thecvf.com/content/CVPR2022/papers/McCouat_Contour-Hugging_Heatmaps_for_Landmark_Detection_CVPR_2022_paper.pdf)<br>:star:[code](https://github.com/jfm15/ContourHuggingHeatmaps)
  * 关键点发现
    * [Self-Supervised Keypoint Discovery in Behavioral Videos](https://arxiv.org/abs/2112.05121)<br>:star:[code](https://github.com/neuroethology/BKinD):house:[project](https://sites.google.com/view/b-kind)
* object discovery
  * [Discovering Objects that Can Move](https://arxiv.org/abs/2203.10159)
* Affordance grounding
  * [Learning Affordance Grounding from Exocentric Images](https://arxiv.org/abs/2203.09905)<br>:star:[code](https://github.com/lhc1224/Cross-View-AG):newspaper:[解读](https://mp.weixin.qq.com/s/lSY1is6Fmm6A0Db0Jxo4qg)
  * [Self-Supervised Transformers for Unsupervised Object Discovery using Normalized Cut](https://arxiv.org/abs/2202.11539)<br>:star:[code](https://github.com/YangtaoWANG95/TokenCut):house:[project](https://www.m-psi.fr/Papers/TokenCut2022/)
* 图像对齐
  * [Unsupervised Homography Estimation with Coplanarity-Aware GAN](https://arxiv.org/abs/2205.03821)<br>:star:[code](https://github.com/megvii-research/HomoGAN):newspaper:[解读](https://mp.weixin.qq.com/s/kZue3ds348UXQI86xrwudQ)
* 物体属性识别
  * [Disentangling Visual Embeddings for Attributes and Objects](https://arxiv.org/abs/2205.08536)<br>:open_mouth:oral:star:[code](https://github.com/nirat1606/OADis)
* 消影点检测
  * [Deep vanishing point detection: Geometric priors make dataset variations vanish](https://arxiv.org/abs/2203.08586)<br>:star:[code](https://github.com/yanconglin/VanishingPoint_HoughTransform_GaussianSphere)  
* 红外探测线
  * [Infrared Invisible Clothing: Hiding From Infrared Detectors at Multiple Angles in Real World](https://arxiv.org/abs/2205.05909)<br>:open_mouth:oral
* OOD
  * [Deep Hybrid Models for Out-of-Distribution Detection](https://openaccess.thecvf.com/content/CVPR2022/papers/Cao_Deep_Hybrid_Models_for_Out-of-Distribution_Detection_CVPR_2022_paper.pdf)
  * [Rethinking Reconstruction Autoencoder-Based Out-of-Distribution Detection](https://arxiv.org/abs/2203.02194)
  * [Evading the Simplicity Bias: Training a Diverse Set of Models Discovers Solutions with Superior OOD Generalization](https://arxiv.org/abs/2105.05612)<br>:sunflower:[dataset](https://github.com/dteney/collages-dataset)
  * [PixMix: Dreamlike Pictures Comprehensively Improve Safety Measures](https://arxiv.org/abs/2112.05135)<br>:star:[code](https://github.com/andyzoujm/pixmix)
  * [The Two Dimensions of Worst-Case Training and Their Integrated Effect for Out-of-Domain Generalization](https://openaccess.thecvf.com/content/CVPR2022/papers/Huang_The_Two_Dimensions_of_Worst-Case_Training_and_Their_Integrated_Effect_CVPR_2022_paper.pdf)
  * [Out-of-Distribution Generalization With Causal Invariant Transformations](https://arxiv.org/abs/2203.11528)
  * [ViM: Out-Of-Distribution with Virtual-logit Matching](https://arxiv.org/abs/2203.10807)<br>:star:[code](https://github.com/haoqiwang/vim)
* 开放世界目标检测
  * [OW-DETR: Open-world Detection Transformer](https://openaccess.thecvf.com/content/CVPR2022/papers/Gupta_OW-DETR_Open-World_Detection_Transformer_CVPR_2022_paper.pdf)<br>:star:[code](https://github.com/akshitac8/OW-DETR)

<a name="4"/>

## 4.Image Captioning(图像字幕)
* [X-Trans2Cap: Cross-Modal Knowledge Transfer using Transformer for 3D Dense Captioning](https://arxiv.org/abs/2203.00843)<br>:star:[code](https://github.com/CurryYuan/X-Trans2Cap)
* [Quantifying Societal Bias Amplification in Image Captioning](https://arxiv.org/abs/2203.15395)
* [NICGSlowDown: Evaluating the Efficiency Robustness of Neural Image Caption Generation Models](https://arxiv.org/abs/2203.15859)
* [It is Okay to Not Be Okay: Overcoming Emotional Bias in Affective Image Captioning by Contrastive Data Collection](https://arxiv.org/abs/2204.07660)<br>:star:[code](https://github.com/Vision-CAIR/artemis-v2):house:[project](https://www.artemisdataset-v2.org/)
* [Beyond a Pre-Trained Object Detector: Cross-Modal Textual and Visual Context for Image Captioning](https://arxiv.org/abs/2205.04363) 
* [DIFNet: Boosting Visual Information Flow for Image Captioning](https://openaccess.thecvf.com/content/CVPR2022/papers/Wu_DIFNet_Boosting_Visual_Information_Flow_for_Image_Captioning_CVPR_2022_paper.pdf)<br>:star:[code](https://github.com/mrwu-mac/DIFNet)<br>:newspaper:[解读](https://mp.weixin.qq.com/s/FYn0S46OA6xraH9ofNIctw)
* [VisualGPT: Data-Efficient Adaptation of Pretrained Language Models for Image Captioning](https://arxiv.org/abs/2102.10407)<br>:star:[code](https://github.com/Vision-CAIR/VisualGPT)
* [Comprehending and Ordering Semantics for Image Captioning](https://openaccess.thecvf.com/content/CVPR2022/papers/Li_Comprehending_and_Ordering_Semantics_for_Image_Captioning_CVPR_2022_paper.pdf)<br>:star:[code](https://github.com/YehLi/xmodaler/tree/master/configs/image_caption/cosnet)<br>:newspaper:[解读](https://mp.weixin.qq.com/s/lSY1is6Fmm6A0Db0Jxo4qg)
* [DeeCap: Dynamic Early Exiting for Efficient Image Captioning](https://openaccess.thecvf.com/content/CVPR2022/papers/Fei_DeeCap_Dynamic_Early_Exiting_for_Efficient_Image_Captioning_CVPR_2022_paper.pdf)<br>:star:[code](https://github.com/feizc/DeeCap)
* Novel Object Captioning 
  * [NOC-REK: Novel Object Captioning with Retrieved Vocabulary from External Knowledge](https://arxiv.org/abs/2203.14499)
 
<a name="3"/>

## 3.Image Progress(图像处理)
* 图像恢复
  * [Attentive Fine-Grained Structured Sparsity for Image Restoration](https://arxiv.org/abs/2204.12266)<br>:star:[code](https://github.com/JungHunOh/SLS_CVPR2022):newspaper:[解读](https://zhuanlan.zhihu.com/p/505802169)
  * [Uformer: A General U-Shaped Transformer for Image Restoration](https://openaccess.thecvf.com/content/CVPR2022/papers/Wang_Uformer_A_General_U-Shaped_Transformer_for_Image_Restoration_CVPR_2022_paper.pdf)<br>:star:[code](https://github.com/ZhendongWang6/Uformer)
  * [Burst Image Restoration and Enhancement](https://arxiv.org/abs/2110.03680)<br>:open_mouth:oral:star:[code](https://github.com/akshaydudhane16/BIPNet)
  * [BNUDC: A Two-Branched Deep Neural Network for Restoring Images From Under-Display Cameras](https://openaccess.thecvf.com/content/CVPR2022/papers/Koh_BNUDC_A_Two-Branched_Deep_Neural_Network_for_Restoring_Images_From_CVPR_2022_paper.pdf)
  * [Restormer: Efficient Transformer for High-Resolution Image Restoration](https://arxiv.org/abs/2111.09881)<br>:open_mouth:oral:star:[code](https://github.com/swz30/Restormer)
  * [TransWeather: Transformer-Based Restoration of Images Degraded by Adverse Weather Conditions](https://arxiv.org/abs/2111.14813)<br>:star:[code](https://github.com/jeya-maria-jose/TransWeather)
  * [Deep Generalized Unfolding Networks for Image Restoration](https://arxiv.org/abs/2204.13348)<br>:star:[code](https://github.com/MC-E/Deep-Generalized-Unfolding-Networks-for-Image-Restoration)
* 图像修复
  * [Incremental Transformer Structure Enhanced Image Inpainting with Masking Positional Encoding](https://arxiv.org/abs/2203.00867)<br>:star:[code](https://github.com/DQiaole/ZITS_inpainting):newspaper:[粗解](https://zhuanlan.zhihu.com/p/475100003/)
  * [MAT: Mask-Aware Transformer for Large Hole Image Inpainting](https://arxiv.org/abs/2203.15270)<br>:star:[code](https://github.com/fenglinglwb/MAT)
  * [Reduce Information Loss in Transformers for Pluralistic Image Inpainting](https://arxiv.org/abs/2205.05076)<br>:star:[code](https://github.com/liuqk3/PUT)
  * [UniCoRN: A Unified Conditional Image Repainting Network](https://openaccess.thecvf.com/content/CVPR2022/papers/Sun_UniCoRN_A_Unified_Conditional_Image_Repainting_Network_CVPR_2022_paper.pdf)
  * [Dual-Path Image Inpainting With Auxiliary GAN Inversion](https://openaccess.thecvf.com/content/CVPR2022/papers/Wang_Dual-Path_Image_Inpainting_With_Auxiliary_GAN_Inversion_CVPR_2022_paper.pdf)
* 图像拼接
  * [Deep Rectangling for Image Stitching: A Learning Baseline](https://arxiv.org/abs/2203.03831)<br>:open_mouth:oral:star:[code](https://github.com/nie-lang/DeepRectangling):newspaper:[粗解](https://zhuanlan.zhihu.com/p/478070143)
  * [utomatic Color Image Stitching Using Quaternion Rank-1 Alignment](https://openaccess.thecvf.com/content/CVPR2022/papers/Li_Automatic_Color_Image_Stitching_Using_Quaternion_Rank-1_Alignment_CVPR_2022_paper.pdf)
  * [Geometric Structure Preserving Warp for Natural Image Stitching](https://openaccess.thecvf.com/content/CVPR2022/papers/Du_Geometric_Structure_Preserving_Warp_for_Natural_Image_Stitching_CVPR_2022_paper.pdf)<br>:star:[code](https://github.com/flowerDuo/GES-GSP-Stitching)
* 运动去模糊
  * [Unifying Motion Deblurring and Frame Interpolation with Events](https://arxiv.org/abs/2203.12178)<br>:star:[code](https://github.com/XiangZ-0/EVDI)
* image outpainting
  * [Diverse Plausible 360-Degree Image Outpainting for Efficient 3DCG Background Creation](https://arxiv.org/abs/2203.14668)<br>:house:[project](https://akmtn.github.io/omni-dreamer/)
* 图像美学评估
  * [Personalized Image Aesthetics Assessment with Rich Attributes](https://arxiv.org/abs/2203.16754)<br>:house:[project](https://cv-datasets.institutecv.com/#/data-sets)
* 图像质量评估
  * [Incorporating Semi-Supervised and Positive-Unlabeled Learning for Boosting Full Reference Image Quality Assessment](https://arxiv.org/abs/2204.08763)<br>:star:[code](https://github.com/happycaoyue/JSPL):newspaper:[解读](https://zhuanlan.zhihu.com/p/501585273) 
* 图像去雨
  * [Towards Robust Rain Removal Against Adversarial Attacks: A Comprehensive Benchmark Analysis and Beyond](https://arxiv.org/abs/2203.16931)<br>:star:[code](https://github.com/yuyi-sd/Robust_Rain_Removal)
  * [Dreaming To Prune Image Deraining Networks](https://openaccess.thecvf.com/content/CVPR2022/papers/Zou_Dreaming_To_Prune_Image_Deraining_Networks_CVPR_2022_paper.pdf)
* 图像去模糊
  * [Learning to Deblur using Light Field Generated and Real Defocus Images](https://arxiv.org/abs/2204.00367)<br>:star:[code](https://github.com/lingyanruan/DRBNet):house:[project](http://lyruan.com/Projects/DRBNet/)
  * [Pixel Screening Based Intermediate Correction for Blind Deblurring](https://openaccess.thecvf.com/content/CVPR2022/papers/Zhang_Pixel_Screening_Based_Intermediate_Correction_for_Blind_Deblurring_CVPR_2022_paper.pdf)
  * [Deblurring via Stochastic Refinement](https://arxiv.org/abs/2112.02475) 
  * [XYDeblur: Divide and Conquer for Single Image Deblurring](https://openaccess.thecvf.com/content/CVPR2022/papers/Ji_XYDeblur_Divide_and_Conquer_for_Single_Image_Deblurring_CVPR_2022_paper.pdf)
  * [Towards Multi-Domain Single Image Dehazing via Test-Time Training](https://openaccess.thecvf.com/content/CVPR2022/papers/Liu_Towards_Multi-Domain_Single_Image_Dehazing_via_Test-Time_Training_CVPR_2022_paper.pdf)
* 图像压缩
  * [SASIC: Stereo Image Compression With Latent Shifts and Stereo Attention](https://openaccess.thecvf.com/content/CVPR2022/papers/Wodlinger_SASIC_Stereo_Image_Compression_With_Latent_Shifts_and_Stereo_Attention_CVPR_2022_paper.pdf)<br>:star:[code](https://github.com/mwoedlinger/sasic)
  * [Global Sensing and Measurements Reuse for Image Compressed Sensing](https://openaccess.thecvf.com/content/CVPR2022/papers/Fan_Global_Sensing_and_Measurements_Reuse_for_Image_Compressed_Sensing_CVPR_2022_paper.pdf)<br>:star:[code](https://github.com/fze0012/MR-CCSNet)
  * [DPICT: Deep Progressive Image Compression Using Trit-Planes](https://arxiv.org/abs/2112.06334)<br>:open_mouth:oral:star:[code](https://github.com/jaehanlee-mcl/DPICT)
  * [Joint Global and Local Hierarchical Priors for Learned Image Compression](https://arxiv.org/abs/2112.04487)<br>:star:[code](https://github.com/naver-ai/informer)
  * [Neural Data-Dependent Transform for Learned Image Compression](https://arxiv.org/abs/2203.04963)<br>:star:[code](https://github.com/Dezhao-Wang/Neural-Syntax-Code):house:[project](https://dezhao-wang.github.io/Neural-Syntax-Website/)
  * [LC-FDNet: Learned Lossless Image Compression With Frequency Decomposition Network](https://openaccess.thecvf.com/content/CVPR2022/papers/Rhee_LC-FDNet_Learned_Lossless_Image_Compression_With_Frequency_Decomposition_Network_CVPR_2022_paper.pdf)<br>:star:[code](https://github.com/myideaisgood/LC-FDNet)
  * [ELIC: Efficient Learned Image Compression With Unevenly Grouped Space-Channel Contextual Adaptive Coding](https://arxiv.org/abs/2203.10886)<br>:open_mouth:oral
  * [Deep Stereo Image Compression via Bi-Directional Coding](https://openaccess.thecvf.com/content/CVPR2022/papers/Lei_Deep_Stereo_Image_Compression_via_Bi-Directional_Coding_CVPR_2022_paper.pdf)
* 图像去噪
  * [CVF-SID: Cyclic multi-Variate Function for Self-Supervised Image Denoising by Disentangling Noise from Image](https://arxiv.org/pdf/2203.13009.pdf)<br>:star:[code](https://github.com/Reyhanehne/CVF-SID_PyTorch)
  * [NAN: Noise-Aware NeRFs for Burst-Denoising](https://arxiv.org/abs/2204.04668) 
  * [Blind2Unblind: Self-Supervised Image Denoising With Visible Blind Spots](https://arxiv.org/abs/2203.06967)<br>:star:[code](https://github.com/demonsjin/Blind2Unblind)
  * [AP-BSN: Self-Supervised Denoising for Real-World Images via Asymmetric PD and Blind-Spot Network](https://openaccess.thecvf.com/content/CVPR2022/papers/Lee_AP-BSN_Self-Supervised_Denoising_for_Real-World_Images_via_Asymmetric_PD_and_CVPR_2022_paper.pdf)<br>:star:[code](https://github.com/wooseoklee4/AP-BSN)
  * [RePaint: Inpainting Using Denoising Diffusion Probabilistic Models](https://arxiv.org/abs/2201.09865)<br>:star:[code](https://github.com/andreas128/RePaint)
* 图像去雾
  * [Image Dehazing Transformer with Transmission-Aware 3D Position Embedding](https://openaccess.thecvf.com/content/CVPR2022/papers/Guo_Image_Dehazing_Transformer_With_Transmission-Aware_3D_Position_Embedding_CVPR_2022_paper.pdf)<br>:house:[project](https://li-chongyi.github.io/Proj_DeHamer.html)
* De-rendering
  * [Learning sRGB-to-Raw-RGB De-rendering with Content-Aware Metadata](https://arxiv.org/abs/2206.01813)<br>:star:[code](https://github.com/SamsungLabs/content-aware-metadata):newspaper:[解读](https://zhuanlan.zhihu.com/p/525331776) 
  * [De-Rendering 3D Objects in the Wild](https://arxiv.org/abs/2201.02279)<br>:star:[code](https://github.com/Brummi/derender3d)
  * [IDR: Self-Supervised Image Denoising via Iterative Data Refinement](https://arxiv.org/abs/2111.14358)<br>:star:[code](https://github.com/zhangyi-3/IDR)
  * [RADU: Ray-Aligned Depth Update Convolutions for ToF Data Denoising](https://arxiv.org/abs/2111.15513)<br>:star:[code](https://github.com/schellmi42/RADU)
  * [Self-augmented Unpaired Image Dehazing via Density and Depth Decomposition](https://openaccess.thecvf.com/content/CVPR2022/papers/Yang_Self-Augmented_Unpaired_Image_Dehazing_via_Density_and_Depth_Decomposition_CVPR_2022_paper.pdf)<br>:star:[code](https://github.com/YaN9-Y/D4)<br>:newspaper:[解读](https://mp.weixin.qq.com/s/lSY1is6Fmm6A0Db0Jxo4qg)
* 图像增强
  * [Toward Fast, Flexible, and Robust Low-Light Image Enhancement](https://arxiv.org/abs/2204.10137)<br>:open_mouth:oral:star:[code](https://github.com/vis-opt-group/SCI):newspaper:[解读](https://zhuanlan.zhihu.com/p/502894478)<br>:newspaper:[SCI：快速、灵活与稳健的低光照图像增强方法（CVPR 2022 Oral）](https://mp.weixin.qq.com/s/1edHkevclvnDNoVNRPitTg)
  * [AdaInt: Learning Adaptive Intervals for 3D Lookup Tables on Real-time Image Enhancement](https://arxiv.org/abs/2204.13983)<br>:star:[code](https://github.com/ImCharlesY/AdaInt)
  * [Directional Self-supervised Learning for Heavy Image Augmentations](https://arxiv.org/abs/2110.13555)<br>:star:[code](https://github.com/Yif-Yang/DSSL)<br>:newspaper:[解读](https://mp.weixin.qq.com/s/lSY1is6Fmm6A0Db0Jxo4qg)
  * [Abandoning the Bayer-Filter To See in the Dark](https://arxiv.org/abs/2203.04042)<br>:star:[code](https://github.com/TCL-AILab/Abandon_Bayer-Filter_See_in_the_Dark)
  * [URetinex-Net: Retinex-Based Deep Unfolding Network for Low-Light Image Enhancement](https://openaccess.thecvf.com/content/CVPR2022/papers/Wu_URetinex-Net_Retinex-Based_Deep_Unfolding_Network_for_Low-Light_Image_Enhancement_CVPR_2022_paper.pdf)<br>:star:[code](https://github.com/AndersonYong/URetinex-Net)
  * [GIQE: Generic Image Quality Enhancement via Nth Order Iterative Degradation](https://openaccess.thecvf.com/content/CVPR2022/papers/Shyam_GIQE_Generic_Image_Quality_Enhancement_via_Nth_Order_Iterative_Degradation_CVPR_2022_paper.pdf)
* 图像和谐化
  * [SCS-Co: Self-Consistent Style Contrastive Learning for Image Harmonization](https://arxiv.org/abs/2204.13962)<br>:star:[code](https://github.com/YCHang686/SCS-Co-CVPR2022) 
  * [High-Resolution Image Harmonization via Collaborative Dual Transformations](https://arxiv.org/abs/2109.06671)<br>:star:[code](https://github.com/bcmi/CDTNet-High-Resolution-Image-Harmonization)
* 图像超级补全
  * [Scene Graph Expansion for Semantics-Guided Image Outpainting](https://arxiv.org/abs/2205.02958)<br>该文解决了一个非常有意思的问题，通过对图像场景图的扩展，对图像边缘以外的内容进行语义引导的内容生成，可帮助设计师快速绘就自然和谐的图像扩展内容。
* 语义图像匹配
  * [TransforMatcher: Match-to-Match Attention for Semantic Correspondence](https://arxiv.org/abs/2205.11634)<br>:star:[code](https://github.com/wookiekim/transformatcher):house:[project](http://cvlab.postech.ac.kr/research/TransforMatcher/)<br>:newspaper:[解读](https://zhuanlan.zhihu.com/p/519556254)
* 图像修饰
  * [ABPN: Adaptive Blend Pyramid Network for Real-Time Local Retouching of Ultra High-Resolution Photo](https://openaccess.thecvf.com/content/CVPR2022/papers/Lei_ABPN_Adaptive_Blend_Pyramid_Network_for_Real-Time_Local_Retouching_of_CVPR_2022_paper.pdf)<br>:star:[code](https://github.com/youngLBW/CRHD-3K)
* 图像着色
  * [Style-Structure Disentangled Features and Normalizing Flows for Diverse Icon Colorization](https://openaccess.thecvf.com/content/CVPR2022/papers/Li_Style-Structure_Disentangled_Features_and_Normalizing_Flows_for_Diverse_Icon_Colorization_CVPR_2022_paper.pdf)
* 图像校正
  * [EvUnroll: Neuromorphic Events Based Rolling Shutter Image Correction](https://openaccess.thecvf.com/content/CVPR2022/papers/Zhou_EvUnroll_Neuromorphic_Events_Based_Rolling_Shutter_Image_Correction_CVPR_2022_paper.pdf)<br>:star:[code](https://github.com/zxyemo/EvUnroll)
* 图像分解
  * [PIE-Net: Photometric Invariant Edge Guided Network for Intrinsic Image Decomposition](https://openaccess.thecvf.com/content/CVPR2022/papers/Das_PIE-Net_Photometric_Invariant_Edge_Guided_Network_for_Intrinsic_Image_Decomposition_CVPR_2022_paper.pdf)<br>:star:[code](https://github.com/Morpheus3000/PIE-Net):house:[project](https://ivi.fnwi.uva.nl/cv/pienet/)
* 图像重建
  * [Mask-guided Spectral-wise Transformer for Efficient Hyperspectral Image Reconstruction](https://arxiv.org/abs/2111.07910)<br>:star:[code](https://github.com/caiyuanhao1998/MST/)
  * [A Differentiable Two-Stage Alignment Scheme for Burst Image Reconstruction With Large Shift](https://arxiv.org/abs/2203.09294)<br>:star:[code](https://github.com/GuoShi28/2StageAlign)
* 图像配准
  * [A Variational Bayesian Method for Similarity Learning in Non-Rigid Image Registration](https://openaccess.thecvf.com/content/CVPR2022/papers/Grzech_A_Variational_Bayesian_Method_for_Similarity_Learning_in_Non-Rigid_Image_CVPR_2022_paper.pdf)<br>:star:[code](https://github.com/dgrzech/learnsim)
  * [NODEO: A Neural Ordinary Differential Equation Based Optimization Framework for Deformable Image Registration](http://arxiv.org/abs/2108.03443)
  * [RFNet: Unsupervised Network for Mutually Reinforcing Multi-Modal Image Registration and Fusion](https://openaccess.thecvf.com/content/CVPR2022/papers/Xu_RFNet_Unsupervised_Network_for_Mutually_Reinforcing_Multi-Modal_Image_Registration_and_CVPR_2022_paper.pdf)
  * [Aladdin: Joint Atlas Building and Diffeomorphic Registration Learning With Pairwise Alignment](https://arxiv.org/abs/2202.03563)<br>:star:[code](https://github.com/uncbiag/Aladdin)
* 图像编辑
  * [Brain-Supervised Image Editing](https://openaccess.thecvf.com/content/CVPR2022/papers/Davis_Brain-Supervised_Image_Editing_CVPR_2022_paper.pdf)
* 图像缩放
  * [Towards Bidirectional Arbitrary Image Rescaling: Joint Optimization and Cycle Idempotence](https://arxiv.org/abs/2203.00911)
* 图像色彩编辑
  * [SpaceEdit: Learning a Unified Editing Space for Open-Domain Image Color Editing](https://openaccess.thecvf.com/content/CVPR2022/papers/Shi_SpaceEdit_Learning_a_Unified_Editing_Space_for_Open-Domain_Image_Color_CVPR_2022_paper.pdf)<br>:star:[code](https://github.com/jshi31/T2ONet):house:[project](https://jshi31.github.io/SpaceEdit/)
* 图像拼图
  * [SoftCollage: A Differentiable Probabilistic Tree Generator for Image Collage](https://openaccess.thecvf.com/content/CVPR2022/papers/Yu_SoftCollage_A_Differentiable_Probabilistic_Tree_Generator_for_Image_Collage_CVPR_2022_paper.pdf)<br>:star:[code](https://github.com/ChineseYjh/SoftCollage)
* 图像裁剪
  * [Rethinking Image Cropping: Exploring Diverse Compositions From Global Views](https://openaccess.thecvf.com/content/CVPR2022/papers/Jia_Rethinking_Image_Cropping_Exploring_Diverse_Compositions_From_Global_Views_CVPR_2022_paper.pdf)
* 图像补全
  * [Bridging Global Context Interactions for High-Fidelity Image Completion](https://arxiv.org/abs/2104.00845)<br>:star:[code](https://github.com/lyndonzheng/TFill)
* 基于文本指导的图像操作
  * [DiffusionCLIP: Text-Guided Diffusion Models for Robust Image Manipulation](https://arxiv.org/abs/2110.02711)<br>:star:[code](https://github.com/gwang-kim/DiffusionCLIP)
* Image Dewarping
  * [Revisiting Document Image Dewarping by Grid Regularization](https://arxiv.org/abs/2203.16850)
<a name="2"/>

## 2.Image Segmentation(图像分割)
* [FocalClick: Towards Practical Interactive Image Segmentation](https://arxiv.org/abs/2204.02574)<br>:star:[code](https://github.com/XavierCHEN34/ClickSEG):newspaper:[粗解](https://zhuanlan.zhihu.com/p/494237617)
* [Multimodal Material Segmentation](https://openaccess.thecvf.com/content/CVPR2022/papers/Liang_Multimodal_Material_Segmentation_CVPR_2022_paper.pdf)
* [Semantic-Aware Domain Generalized Segmentation](https://arxiv.org/abs/2204.00822)<br>:open_mouth:oral:star:[code](https://github.com/leolyj/SAN-SAW)
* [ReSTR: Convolution-free Referring Image Segmentation Using Transformers](https://arxiv.org/abs/2203.16768)<br>:star:[code](https://github.com/southflame/restr):house:[project](http://cvlab.postech.ac.kr/research/restr/)
* [CRIS: CLIP-Driven Referring Image Segmentation](https://arxiv.org/abs/2111.15174)
* [Panoptic Neural Fields: A Semantic Object-Aware Neural Scene Representation](https://arxiv.org/abs/2205.04334)<br>:house:[project](https://abhijitkundu.info/projects/pnf/)<br>全景神经场：谷歌新提出的语义级目标感知的神经场景表示模型。该表示模型可以有效地用于新视图合成、2D 全景分割、3D 场景编辑和多视图深度预测等多项任务。相信这又会是一个引领潮流的新方向。
* [FocusCut: Diving Into a Focus View in Interactive Segmentation](https://openaccess.thecvf.com/content/CVPR2022/papers/Lin_FocusCut_Diving_Into_a_Focus_View_in_Interactive_Segmentation_CVPR_2022_paper.pdf)<br>:house:[project](http://www.lin-zheng.com/focuscut/)
* [Hyperbolic Image Segmentation](https://openaccess.thecvf.com/content/CVPR2022/papers/Atigh_Hyperbolic_Image_Segmentation_CVPR_2022_paper.pdf)<br>:star:[code](https://github.com/MinaGhadimiAtigh/HyperbolicImageSegmentation)
* [Clustering Plotted Data by Image Segmentation](https://arxiv.org/abs/2110.05187)<br>:star:[code](https://github.com/tareknaous/visual-clustering)
* [Generalizable Cross-Modality Medical Image Segmentation via Style Augmentation and Dual Normalization](https://arxiv.org/abs/2112.11177)<br>:star:[code](https://github.com/zzzqzhou/Dual-Normalization)
* [Image Segmentation Using Text and Image Prompts](https://openaccess.thecvf.com/content/CVPR2022/papers/Luddecke_Image_Segmentation_Using_Text_and_Image_Prompts_CVPR_2022_paper.pdf)<br>:star:[code](https://github.com/timojl/clipseg)
* [ISDNet: Integrating Shallow and Deep Networks for Efficient Ultra-high Resolution Segmentation](https://openaccess.thecvf.com/content/CVPR2022/papers/Guo_ISDNet_Integrating_Shallow_and_Deep_Networks_for_Efficient_Ultra-High_Resolution_CVPR_2022_paper.pdf)<br>:star:[code](https://github.com/cedricgsh/ISDNet)<br>:newspaper:[解读](https://mp.weixin.qq.com/s/FYn0S46OA6xraH9ofNIctw)
* [Adaptive Early-Learning Correction for Segmentation From Noisy Annotations](https://arxiv.org/abs/2110.03740)<br>:star:[code](https://github.com/Kangningthu/ADELE)
* 实例分割
  * [E2EC: An End-to-End Contour-based Method for High-Quality High-Speed Instance Segmentation](https://arxiv.org/abs/2203.04074)<br>:star:[code](https://github.com/zhang-tao-whu/e2ec):newspaper:[粗解](https://zhuanlan.zhihu.com/p/478070143)
  * [Open-Vocabulary Instance Segmentation via Robust Cross-Modal Pseudo-Labeling](https://arxiv.org/abs/2111.12698)<br>:star:[code](https://github.com/hbdat/cvpr22_cross_modal_pseudo_labeling)
  * [Sparse Instance Activation for Real-Time Instance Segmentation](https://arxiv.org/abs/2203.12827)<br>:star:[code](https://github.com/hustvl/SparseInst)   
  * [SharpContour: A Contour-based Boundary Refinement Approach for Efficient and Accurate Instance Segmentation](https://arxiv.org/abs/2203.13312)<br>:house:[project](https://xyzhang17.github.io/SharpContour/)   
  * [Open-World Instance Segmentation: Exploiting Pseudo Ground Truth From Learned Pairwise Affinity](https://arxiv.org/abs/2204.06107)<br>:star:[code](https://github.com/facebookresearch/Generic-Grouping):house:[project](https://sites.google.com/view/generic-grouping/)
  * [DArch: Dental Arch Prior-assisted 3D Tooth Instance Segmentation](https://arxiv.org/abs/2204.11911)
  * [Relieving Long-tailed Instance Segmentation via Pairwise Class Balance](https://arxiv.org/abs/2201.02784)<br>:star:[code](https://github.com/megvii-research/PCB):newspaper:[解读](https://mp.weixin.qq.com/s/kZue3ds348UXQI86xrwudQ)
  * [ContrastMask: Contrastive Learning to Segment Every Thing](https://arxiv.org/abs/2203.09775)<br>:newspaper:[解读](https://mp.weixin.qq.com/s/FYn0S46OA6xraH9ofNIctw)<br>基于像素级对比学习的不完全监督实例分割算法
  * [GASP, a Generalized Framework for Agglomerative Clustering of Signed Graphs and Its Application to Instance Segmentation](https://openaccess.thecvf.com/content/CVPR2022/papers/Bailoni_GASP_a_Generalized_Framework_for_Agglomerative_Clustering_of_Signed_Graphs_CVPR_2022_paper.pdf)<br>:star:[code](https://github.com/abailoni/GASP)
  * [TWIST: Two-Way Inter-Label Self-Training for Semi-Supervised 3D Instance Segmentation](https://openaccess.thecvf.com/content/CVPR2022/papers/Chu_TWIST_Two-Way_Inter-Label_Self-Training_for_Semi-Supervised_3D_Instance_Segmentation_CVPR_2022_paper.pdf)<br>:star:[code](https://github.com/dvlab-research/TWIST)
  * [Pointly-Supervised Instance Segmentation](https://arxiv.org/abs/2104.06404)<br>:open_mouth:oral:star:[code](https://github.com/facebookresearch/detectron2/tree/main/projects/PointSup):house:[project](https://bowenc0221.github.io/point-sup/)
  * [Instance Segmentation With Mask-Supervised Polygonal Boundary Transformers](https://openaccess.thecvf.com/content/CVPR2022/papers/Lazarow_Instance_Segmentation_With_Mask-Supervised_Polygonal_Boundary_Transformers_CVPR_2022_paper.pdf)<br>:star:[code](https://github.com/mlpc-ucsd/BoundaryFormer)
  * 半监督实例分割
    * [Noisy Boundaries: Lemon or Lemonade for Semi-supervised Instance Segmentation?](https://arxiv.org/abs/2203.13427)
  * 3D 实例分割
    * [SoftGroup for 3D Instance Segmentation on Point Clouds](https://arxiv.org/abs/2203.01509)<br>:star:[code](https://github.com/thangvubk/SoftGroup):newspaper:[粗解](https://zhuanlan.zhihu.com/p/475067096)
  * 🐦️[FreeSOLO: Learning to Segment Objects without Annotations](https://arxiv.org/abs/2202.12181)<br>:star:[code](https://github.com/NVlabs/FreeSOLO)
  * 小样本分割
    * [iFS-RCNN: An Incremental Few-Shot Instance Segmenter](https://openaccess.thecvf.com/content/CVPR2022/papers/Nguyen_iFS-RCNN_An_Incremental_Few-Shot_Instance_Segmenter_CVPR_2022_paper.pdf)
* 语义分割
  * [Generalized Few-Shot Semantic Segmentation](https://arxiv.org/abs/2010.05210)<br>:star:[code]([https://github.com/XinyuLyu/FGPL](https://github.com/dvlab-research/GFS-Seg)
  * [Novel Class Discovery in Semantic Segmentation](https://arxiv.org/abs/2112.01900)<br>:star:[code](https://github.com/HeliosZhao/NCDSS):house:[project](https://ncdss.github.io/)
  * [Multi-Scale High-Resolution Vision Transformer for Semantic Segmentation](https://arxiv.org/abs/2111.01236)<br>:star:[code](https://github.com/facebookresearch/HRViT)
  * [Semi-Supervised Video Semantic Segmentation With Inter-Frame Feature Reconstruction](https://openaccess.thecvf.com/content/CVPR2022/papers/Zhuang_Semi-Supervised_Video_Semantic_Segmentation_With_Inter-Frame_Feature_Reconstruction_CVPR_2022_paper.pdf)<br>:star:[code](https://github.com/jfzhuang/IFR)
  * [Pin the Memory: Learning to Generalize Semantic Segmentation](https://arxiv.org/abs/2204.03609)<br>:star:[code](https://github.com/Genie-Kim/PintheMemory):newspaper:[解读](https://zhuanlan.zhihu.com/p/494913545)
  * [Representation Compensation Networks for Continual Semantic Segmentation](https://arxiv.org/abs/2203.05402)<br>:star:[code](https://github.com/zhangchbin/RCIL)
  * [Tree Energy Loss: Towards Sparsely Annotated Semantic Segmentation](https://arxiv.org/abs/2203.10739)<br>:star:[code](https://github.com/megvii-research/TreeEnergyLoss):newspaper:[解读](https://mp.weixin.qq.com/s/kZue3ds348UXQI86xrwudQ)
  * [GroupViT: Semantic Segmentation Emerges from Text Supervision](https://arxiv.org/abs/2202.11094)<br>:star:[code](https://github.com/NVlabs/GroupViT):house:[project](https://jerryxu.net/GroupViT/):tv:[video](https://youtu.be/DtJsWIUTW-Y)<br>:newspaper:[做语义分割不用任何像素标签，UCSD、英伟达在ViT中加入分组模块](https://mp.weixin.qq.com/s/P1vHiMVS93vo_KRUi6GUKw)
  * [Bending Reality: Distortion-aware Transformers for Adapting to Panoramic Semantic Segmentation](https://arxiv.org/abs/2203.01452)<br>:star:[code](https://github.com/jamycheung/Trans4PASS):newspaper:[粗解](https://zhuanlan.zhihu.com/p/475067096)
  * [Deep Hierarchical Semantic Segmentation](https://arxiv.org/abs/2203.14335)<br>:star:[code](https://github.com/0liliulei/HieraSeg)
  * [Semantic Segmentation by Early Region Proxy](https://arxiv.org/abs/2203.14043)<br>:star:[code](https://github.com/YiF-Zhang/RegionProxy):newspaper:[粗解](https://news.sjtu.edu.cn/jdzh/20220330/169456.html)
  * [SimT: Handling Open-set Noise for Domain Adaptive Semantic Segmentation](https://arxiv.org/abs/2203.15202)<br>:star:[code](https://github.com/CityU-AIM-Group/SimT)
  * [Rethinking Semantic Segmentation: A Prototype View](https://arxiv.org/abs/2203.15102)<br>:open_mouth:oral:star:[code](https://github.com/tfzhou/ProtoSeg)
  * [On the Road to Online Adaptation for Semantic Image Segmentation](https://arxiv.org/abs/2203.16195)<br>:star:[code](https://github.com/naver/oasis)
  * [Threshold Matters in WSSS: Manipulating the Activation for the Robust and Accurate Segmentation Model Against Thresholds](https://arxiv.org/abs/2203.16045)<br>:star:[code](https://github.com/gaviotas/AMN)
  * [NightLab: A Dual-level Architecture with Hardness Detection for Segmentation at Night](https://arxiv.org/abs/2204.05538)<br>:star:[code](https://github.com/xdeng7/NightLab):newspaper:[解读](https://zhuanlan.zhihu.com/p/497769133)
  * [TopFormer: Token Pyramid Transformer for Mobile Semantic Segmentation](https://arxiv.org/abs/2204.05525)
  * [Cross-Image Relational Knowledge Distillation for Semantic Segmentation](https://arxiv.org/abs/2204.06986)<br>:star:[code](https://github.com/winycg/CIRKD):newspaper:[解读](https://zhuanlan.zhihu.com/p/498883232)
  * [Dynamic Prototype Convolution Network for Few-Shot Semantic Segmentation](https://arxiv.org/abs/2204.10638)
  * [Unsupervised Hierarchical Semantic Segmentation with Multiview Cosegmentation and Clustering Transformers](https://arxiv.org/abs/2204.11432)<br>:star:[code](https://github.com/twke18/HSG)
  * [Self-Supervised Learning of Object Parts for Semantic Segmentation](https://arxiv.org/abs/2204.13101)<br>:star:[code](https://github.com/MkuuWaUjinga/leopart)
  * [Cross-view Transformers for real-time Map-view Semantic Segmentation](https://arxiv.org/abs/2205.02833)<br>:open_mouth:oral:star:[code](https://github.com/bradyz/cross_view_transformers)
  * [Deep Spectral Methods: A Surprisingly Strong Baseline for Unsupervised Semantic Segmentation and Localization](https://arxiv.org/abs/2205.07839)<br>:house:[project](https://lukemelas.github.io/deep-spectral-segmentation/)
  * [Point-to-Voxel Knowledge Distillation for LiDAR Semantic Segmentation](https://arxiv.org/abs/2206.02099)<br>:star:[code](https://github.com/cardwing/Codes-for-PVKD):newspaper:[解读](https://zhuanlan.zhihu.com/p/525331776)
  * [Real-Time, Accurate, and Consistent Video Semantic Segmentation via Unsupervised Adaptation and Cross-Unit Deployment on Mobile Device](https://openaccess.thecvf.com/content/CVPR2022/papers/Park_Real-Time_Accurate_and_Consistent_Video_Semantic_Segmentation_via_Unsupervised_Adaptation_CVPR_2022_paper.pdf)
  * [Partial Class Activation Attention for Semantic Segmentation](https://openaccess.thecvf.com/content/CVPR2022/papers/Liu_Partial_Class_Activation_Attention_for_Semantic_Segmentation_CVPR_2022_paper.pdf)<br>:star:[code](https://github.com/lsa1997/PCAA)
  * [Incremental Learning in Semantic Segmentation From Image Labels](https://arxiv.org/abs/2112.01882)<br>:star:[code](https://github.com/fcdl94/WILSON)
  * [HybridCR: Weakly-Supervised 3D Point Cloud Semantic Segmentation via Hybrid Contrastive Regularization](https://openaccess.thecvf.com/content/CVPR2022/papers/Li_HybridCR_Weakly-Supervised_3D_Point_Cloud_Semantic_Segmentation_via_Hybrid_Contrastive_CVPR_2022_paper.pdf)<br>:newspaper:[解读](https://mp.weixin.qq.com/s/FYn0S46OA6xraH9ofNIctw)
  * 3D分割
    * [MM-TTA: Multi-Modal Test-Time Adaptation for 3D Semantic Segmentation](https://arxiv.org/abs/2204.12667)<br>:house:[project](https://www.nec-labs.com/~mas/MM-TTA/)
    * [Learning Multi-View Aggregation In the Wild for Large-Scale 3D Semantic Segmentation](https://arxiv.org/abs/2204.07548)<br>:open_mouth:oral:star:[code](https://github.com/drprojects/DeepViewAgg):newspaper:[解读](https://zhuanlan.zhihu.com/p/500351469)
  * 弱监督语义分割
    * [Class Re-Activation Maps for Weakly-Supervised Semantic Segmentation](https://arxiv.org/abs/2203.00962)<br>:star:[code](https://github.com/zhaozhengChen/ReCAM):newspaper:[粗解](https://zhuanlan.zhihu.com/p/475100003/)
    * [Self-supervised Image-specific Prototype Exploration for Weakly Supervised Semantic Segmentation](https://arxiv.org/abs/2203.02909)<br>:star:[code](https://github.com/chenqi1126/SIPE)
    * [Contrastive learning of Class-agnostic Activation Map for Weakly Supervised Object Localization and Semantic Segmentation](https://arxiv.org/abs/2203.13505)<br>:star:[code](https://github.com/CVI-SZU/CCAM)
    * [Cross Language Image Matching for Weakly Supervised Semantic Segmentation](https://arxiv.org/abs/2203.02668)<br>:star:[code](https://github.com/CVI-SZU/CLIMS)
    * [Multi-class Token Transformer for Weakly Supervised Semantic Segmentation](https://arxiv.org/abs/2203.02891)<br>:star:[code](https://github.com/xulianuwa/MCTformer)
    * [Learning Affinity from Attention: End-to-End Weakly-Supervised Semantic Segmentation with Transformers](https://arxiv.org/abs/2203.02664)<br>:star:[code](https://github.com/rulixiang/afa):newspaper:[解读](https://mp.weixin.qq.com/s/lSY1is6Fmm6A0Db0Jxo4qg)
    * [Weakly Supervised Semantic Segmentation using Out-of-Distribution Data](https://arxiv.org/abs/2203.03860)<br>:star:[code](https://github.com/naver-ai/w-ood):newspaper:[粗解](https://zhuanlan.zhihu.com/p/478070143)
    * [L2G: A Simple Local-to-Global Knowledge Transfer Framework for Weakly Supervised Semantic Segmentation](https://arxiv.org/abs/2204.03206)<br>:star:[code](https://github.com/PengtaoJiang/L2G)
    * [Weakly Supervised Semantic Segmentation by Pixel-to-Prototype Contrast](https://arxiv.org/abs/2110.07110)
    * [CLIMS: Cross Language Image Matching for Weakly Supervised Semantic Segmentation](https://openaccess.thecvf.com/content/CVPR2022/papers/Xie_CLIMS_Cross_Language_Image_Matching_for_Weakly_Supervised_Semantic_Segmentation_CVPR_2022_paper.pdf)<br>:star:[code](https://github.com/CVI-SZU/CLIMS)
    * [Regional Semantic Contrast and Aggregation for Weakly Supervised Semantic Segmentation](https://arxiv.org/abs/2203.09653)<br>:star:[code](https://github.com/maeve07/RCA)
  * 无监督语义分割
    * [Cross-Domain Correlation Distillation for Unsupervised Domain Adaptation in Nighttime Semantic Segmentation](https://arxiv.org/abs/2205.00858)<br>:star:[code](https://github.com/ghuan99/CCDistill)
  * 半监督语义分割
    * [Semi-Supervised Semantic Segmentation Using Unreliable Pseudo-Labels](https://arxiv.org/abs/2203.03884)
    * [Semi-supervised Semantic Segmentation with Error Localization Network](https://arxiv.org/abs/2204.02078)<br>:star:[code](https://github.com/kinux98/SSL_ELN):house:[project](http://cvlab.postech.ac.kr/research/ELN/):newspaper:[粗解](https://zhuanlan.zhihu.com/p/493615566)
    * [UCC: Uncertainty guided Cross-head Co-training for Semi-Supervised Semantic Segmentation](https://arxiv.org/abs/2205.10334)
    * [Perturbed and Strict Mean Teachers for Semi-Supervised Semantic Segmentation](https://arxiv.org/abs/2111.12903)<br>:star:[code](https://github.com/yyliu01/PS-MT)
  * 域适应语义分割
    * [Towards Fewer Annotations: Active Learning via Region Impurity and Prediction Uncertainty for Domain Adaptive Semantic Segmentation](https://arxiv.org/pdf/2111.12940.pdf)<br>:star:[code](https://github.com/BIT-DA/RIPU)
  * 域泛化语义分割
    * [WildNet: Learning Domain Generalized Semantic Segmentation from the Wild](https://arxiv.org/abs/2204.01446)<br>:star:[code](https://github.com/suhyeonlee/WildNet)
  * 小样本语义分割
    * [Learning Non-target Knowledge for Few-shot Semantic Segmentation](https://arxiv.org/abs/2205.04903)<br>:star:[code](https://github.com/LIUYUANWEI98/NERTNet)<br>:newspaper:[解读](https://zhuanlan.zhihu.com/p/512521624)
  * 跨域语义分割
    * [Undoing the Damage of Label Shift for Cross-Domain Semantic Segmentation](https://arxiv.org/abs/2204.05546)<br>:star:[code](https://github.com/manmanjun/Undoing_UDA)
* 动作分割
  * [Weakly-Supervised Online Action Segmentation in Multi-View Instructional Videos](https://arxiv.org/abs/2203.13309)
  * [Fast and Unsupervised Action Boundary Detection for Action Segmentation](https://openaccess.thecvf.com/content/CVPR2022/papers/Du_Fast_and_Unsupervised_Action_Boundary_Detection_for_Action_Segmentation_CVPR_2022_paper.pdf)
* 场景解析
  * [FLOAT: Factorized Learning of Object Attributes for Improved Multi-object Multi-part Scene Parsing](https://arxiv.org/abs/2203.16168)<br>:star:[code](https://floatseg.github.io)
  * [Cerberus Transformer: Joint Semantic, Affordance and Attribute Parsing](https://arxiv.org/abs/2111.12608)<br>:star:[code](https://github.com/OPEN-AIR-SUN/Cerberus)
* 雾景分割
  * [FIFO: Learning Fog-invariant Features for Foggy Scene Segmentation](https://arxiv.org/abs/2204.01587)<br>:open_mouth:oral:star:[code](https://github.com/Sohyun-lee96/FIFO):house:[project](http://cvlab.postech.ac.kr/research/FIFO/)
* 全景分割
  * [Panoptic, Instance and Semantic Relations: A Relational Context Encoder to Enhance Panoptic Segmentation](https://arxiv.org/abs/2204.05370)
  * [Joint Forecasting of Panoptic Segmentations with Difference Attention](https://arxiv.org/abs/2204.07157)<br>:star:[code](https://github.com/cgraber/psf-diffattn):newspaper:[解读](https://zhuanlan.zhihu.com/p/498883232)
  * [PanopticDepth: A Unified Framework for Depth-aware Panoptic Segmentation](https://arxiv.org/abs/2206.00468)<br>:star:[code](https://github.com/NaiyuGao/PanopticDepth):newspaper:[解读](https://zhuanlan.zhihu.com/p/523390359)
  * [Amodal Panoptic Segmentation](https://openaccess.thecvf.com/content/CVPR2022/papers/Mohan_Amodal_Panoptic_Segmentation_CVPR_2022_paper.pdf)<br>:house:[project](http://amodal-panoptic.cs.uni-freiburg.de/)
  * [Panoptic-PHNet: Towards Real-Time and High-Precision LiDAR Panoptic Segmentation via Clustering Pseudo Heatmap](https://openaccess.thecvf.com/content/CVPR2022/papers/Li_Panoptic-PHNet_Towards_Real-Time_and_High-Precision_LiDAR_Panoptic_Segmentation_via_Clustering_CVPR_2022_paper.pdf)
  * [CMT-DeepLab: Clustering Mask Transformers for Panoptic Segmentation](https://openaccess.thecvf.com/content/CVPR2022/papers/Yu_CMT-DeepLab_Clustering_Mask_Transformers_for_Panoptic_Segmentation_CVPR_2022_paper.pdf)
  * 视频全景分割
    * [Slot-VPS: Object-Centric Representation Learning for Video Panoptic Segmentation](https://openaccess.thecvf.com/content/CVPR2022/papers/Zhou_Slot-VPS_Object-Centric_Representation_Learning_for_Video_Panoptic_Segmentation_CVPR_2022_paper.pdf)<br>:star:[code](https://github.com/SAITPublic/SlotVPS)
* 抠图
  * [Human Instance Matting via Mutual Guidance and Multi-Instance Refinement](https://arxiv.org/abs/2205.10767)<br>:open_mouth:oral:star:[code](https://github.com/nowsyn/InstMatt)
  * [MatteFormer: Transformer-Based Image Matting via Prior-Tokens](https://arxiv.org/abs/2203.15662)<br>:star:[code](https://github.com/webtoon/matteformer)
* 玻璃分割
  * [Glass Segmentation Using Intensity and Spectral Polarization Cues](https://openaccess.thecvf.com/content/CVPR2022/papers/Mei_Glass_Segmentation_Using_Intensity_and_Spectral_Polarization_Cues_CVPR_2022_paper.pdf)<br>:house:[project](https://mhaiyang.github.io/CVPR2022_PGSNet/)
* Amodal Segmentation
  * [Amodal Segmentation through Out-of-Task and Out-of-Distribution Generalization with a Bayesian Model](https://arxiv.org/abs/2010.13175)<br>:star:[code](https://github.com/YihongSun/Bayesian-Amodal)
* 场景理解
  * [Both Style and Fog Matter: Cumulative Domain Adaptation for Semantic Foggy Scene Understanding](https://arxiv.org/abs/2112.00484)
  * [ScanQA: 3D Question Answering for Spatial Scene Understanding](https://arxiv.org/abs/2112.10482)<br>:star:[code](https://github.com/ATR-DBI/ScanQA)
* 人体解析
  * [CDGNet: Class Distribution Guided Network for Human Parsing](https://arxiv.org/abs/2111.14173)<br>:star:[code](https://github.com/tjpulkl/CDGNet)
* Part Segmentation
  * [Learning Part Segmentation through Unsupervised Domain Adaptation from Synthetic Vehicles](https://arxiv.org/abs/2103.14098)<br>:house:[project](https://qliu24.github.io/udapart/)

<a name="1"/>

## 1.其它
* [Learning to Anticipate Future with Dynamic Context Removal](https://arxiv.org/abs/2204.02587)<br>:star:[code](https://github.com/AllenXuuu/DCR):newspaper:[粗解](https://news.sjtu.edu.cn/jdzh/20220330/169456.html)
* [Learning Optimal K-space Acquisition and Reconstruction using Physics-Informed Neural Networks](https://arxiv.org/abs/2204.02480)
* [Instance-wise Occlusion and Depth Orders in Natural Scenes](https://arxiv.org/abs/2111.14562)<br>:star:[code](https://arxiv.org/abs/2111.14562)
* [IFOR: Iterative Flow Minimization for Robotic Object Rearrangement](https://arxiv.org/abs/2202.00732)<br>:house:[project](https://imankgoyal.github.io/ifor.html)
* [PINA: Learning a Personalized Implicit Neural Avatar from a Single RGB-D Video Sequence](https://arxiv.org/abs/2203.01754)<br>:star:[code](https://github.com/zj-dong/pina):house:[project](https://zj-dong.github.io/pina/):tv:[video](https://www.youtube.com/watch?v=oGpKUuD54Qk):newspaper:[粗解](https://zhuanlan.zhihu.com/p/475067096)
* [LiT: Zero-Shot Transfer with Locked-image text Tuning](https://arxiv.org/abs/2111.07991)
* [CAFE: Learning to Condense Dataset by Aligning Features](https://arxiv.org/abs/2203.01531)<br>:star:[code](https://github.com/kaiwang960112/CAFE):newspaper:[粗解](https://zhuanlan.zhihu.com/p/475067096)
* [Enhancing Adversarial Robustness for Deep Metric Learning](https://arxiv.org/abs/2203.01439)
* [BatchFormer: Learning to Explore Sample Relationships for Robust Representation Learning](https://arxiv.org/abs/2203.01522)<br>:star:[code](https://github.com/zhihou7/BatchFormer):newspaper:[粗解](https://zhuanlan.zhihu.com/p/475067096)[:notebook:](https://mp.weixin.qq.com/s/lSY1is6Fmm6A0Db0Jxo4qg)
* [ACVNet: Attention Concatenation Volume for Accurate and Efficient Stereo Matching](https://arxiv.org/abs/2203.02146)<br>:star:[code](https://github.com/gangweiX/ACVNet):newspaper:[粗解](https://zhuanlan.zhihu.com/p/476923554)
* [Polarity Sampling: Quality and Diversity Control of Pre-Trained Generative Networks via Singular Values](https://arxiv.org/abs/2203.01993)<br>:star:[code](https://colab.research.google.com/drive/1Y4_pp5miLXCeGHkzg7wptTRviHiyViWB?usp=sharing)
* [Do Explanations Explain? Model Knows Best](https://arxiv.org/abs/2203.02269)<br>:star:[code](https://github.com/CAMP-eXplain-AI/Do-Explanations-Explain)
* [HDNet: High-resolution Dual-domain Learning for Spectral Compressive Imaging](https://arxiv.org/abs/2203.02149)<br>:star:[code](https://github.com/Huxiaowan/HDNet)
* [E-CIR: Event-Enhanced Continuous Intensity Recovery](https://arxiv.org/abs/2203.01935)<br>:star:[code](https://github.com/chensong1995/E-CIR)
* 🐦️[Transferability Estimation using Bhattacharyya Class Separability](https://arxiv.org/abs/2111.12780)
* [Interpretable part-whole hierarchies and conceptual-semantic relationships in neural networks](https://arxiv.org/abs/2203.03282)<br>:star:[code](https://github.com/mmlab-cv/Agglomerator)
* [GlideNet: Global, Local and Intrinsic based Dense Embedding NETwork for Multi-category Attributes Prediction](https://arxiv.org/abs/2203.03079)<br>:star:[code](https://github.com/kareem-metwaly/glidenet)
* [Differentially Private Federated Learning with Local Regularization and Sparsification](https://arxiv.org/abs/2203.03106)
* [Towards Efficient and Scalable Sharpness-Aware Minimization](https://arxiv.org/abs/2203.02714)<br>:star:[code](https://github.com/yong-6/LookSAM)
* [DeltaCNN: End-to-End CNN Inference of Sparse Frame Differences in Videos](https://arxiv.org/abs/2203.03996)
* [Probabilistic Warp Consistency for Weakly-Supervised Semantic Correspondences](https://arxiv.org/abs/2203.04279)<br>:star:[code](https://github.com/PruneTruong/DenseMatching):newspaper:[粗解](https://zhuanlan.zhihu.com/p/478070143)
* [Dynamic Dual-Output Diffusion Models](https://arxiv.org/abs/2203.04304)
* [Moving Window Regression: A Novel Approach to Ordinal Regression](https://arxiv.org/pdf/2203.13122.pdf)
* [Egocentric Prediction of Action Target in 3D](https://arxiv.org/pdf/2203.13116.pdf)
* [Compositional Temporal Grounding
with Structured Variational Cross-Graph Correspondence Learning](https://arxiv.org/pdf/2203.13049.pdf)<br>:star:[code](https://github.com/YYJMJC/Compositional-Temporal-Grounding)
* [Hierarchical Nearest Neighbor Graph Embedding for Efficient Dimensionality Reduction](https://arxiv.org/abs/2203.12997)<br>:star:[code](https://github.com/koulakis/h-nne)
* [Neural Reflectance for Shape Recovery with Shadow Handling](https://arxiv.org/abs/2203.12909)<br>:star:[code](https://github.com/junxuan-li/Neural-Reflectance-PS)
* [DyRep: Bootstrapping Training with Dynamic Re-parameterization](https://arxiv.org/abs/2203.12868)<br>:star:[code](https://github.com/hunto/DyRep)
* [Enhancing Classifier Conservativeness and Robustness by Polynomiality](https://arxiv.org/abs/2203.12693)
* [Versatile Multi-Modal Pre-Training for Human-Centric Perception](https://arxiv.org/abs/2203.13815)<br>:star:[code](https://github.com/hongfz16/HCMoCo)
* [Attributable Visual Similarity Learning](https://arxiv.org/abs/2203.14932)<br>:star:[code](https://github.com/zbr17/AVSL)
* [Optimizing Elimination Templates by Greedy Parameter Search](https://arxiv.org/abs/2203.14901)
* [Partially Does It: Towards Scene-Level FG-SBIR with Partial Input](https://arxiv.org/abs/2203.14804)
* [Bi-level Doubly Variational Learning for Energy-based Latent Variable Models](https://arxiv.org/abs/2203.14702)
* [Brain-inspired Multilayer Perceptron with Spiking Neurons](https://arxiv.org/abs/2203.14679)
* [ARCS: Accurate Rotation and Correspondence Search](https://arxiv.org/abs/2203.14493)<br>:star:[code](https://github.com/liangzu/ARCS)
* [iPLAN: Interactive and Procedural Layout Planning](https://arxiv.org/abs/2203.14412)
* [HINT: Hierarchical Neuron Concept Explainer](https://arxiv.org/abs/2203.14196)<br>:star:[code](https://github.com/AntonotnaWang/HINT)
* [Visual Abductive Reasoning](https://arxiv.org/abs/2203.14040)<br>:star:[code](https://github.com/leonnnop/VAR)
* [A Stitch in Time Saves Nine: A Train-Time Regularizing Loss for Improved Neural Network Calibration](https://arxiv.org/abs/2203.13834)<br>:star:[code](https://github.com/mdca-loss/MDCA-Calibration)
* [Learning Structured Gaussians to Approximate Deep Ensembles](https://arxiv.org/abs/2203.15485)
* [Self-Supervised Image Representation Learning with Geometric Set Consistency](https://arxiv.org/abs/2203.15361)
* [Balanced Multimodal Learning via On-the-fly Gradient Modulation](https://arxiv.org/abs/2203.15332)<br>:open_mouth:oral:star:[code](https://github.com/GeWu-Lab/OGM-GE_CVPR2022)
* [CNN Filter DB: An Empirical Investigation of Trained Convolutional Filters](https://arxiv.org/abs/2203.15331)<br>:star:[code](https://github.com/paulgavrikov/cnn-filter-db)
* [Eigencontours: Novel Contour Descriptors Based on Low-Rank Approximation](https://arxiv.org/abs/2203.15259)<br>:open_mouth:oral
* [Pop-Out Motion: 3D-Aware Image Deformation via Learning the Shape Laplacian](https://arxiv.org/abs/2203.15235)
* [Long-term Visual Map Sparsification with Heterogeneous GNN](https://arxiv.org/abs/2203.15182)
* [Clean Implicit 3D Structure from Noisy 2D STEM Images](https://arxiv.org/abs/2203.15434)
* [Equivariance Allows Handling Multiple Nuisance Variables When Analyzing Pooled Neuroimaging Datasets](https://arxiv.org/abs/2203.15234)
* [CaDeX: Learning Canonical Deformation Coordinate Space for Dynamic Surface Representation via Neural Homeomorphism](https://arxiv.org/abs/2203.16529)<br>:star:[code](https://github.com/JiahuiLei/CaDeX):house:[project](https://www.cis.upenn.edu/~leijh/projects/cadex/)
* [Fast Light-Weight Near-Field Photometric Stereo](https://arxiv.org/abs/2203.16515)
* [Fast, Accurate and Memory-Efficient Partial Permutation Synchronization](https://arxiv.org/abs/2203.16505)
* [Multi-Robot Active Mapping via Neural Bipartite Graph Matching](https://arxiv.org/abs/2203.16319)
* [Learning Program Representations for Food Images and Cooking Recipes](https://arxiv.org/abs/2203.16071)<br>:open_mouth:oral:star:[code](https://github.com/dimipapa/cookingprograms):house:[project](http://cookingprograms.csail.mit.edu/)
* [Iterative Deep Homography Estimation](https://arxiv.org/abs/2203.15982)<br>:star:[code](https://github.com/imdumpl78/IHN)
* [Practical Learned Lossless JPEG Recompression with Multi-Level Cross-Channel Entropy Model in the DCT Domain](https://arxiv.org/abs/2203.16357)
* [Generating High Fidelity Data from Low-density Regions using Diffusion Models](https://arxiv.org/abs/2203.17260)
* [Continuous Scene Representations for Embodied AI](https://arxiv.org/abs/2203.17251)<br>:star:[code](https://github.com/allenai/CSR):house:[project](https://prior.allenai.org/projects/csr)
* [It's All In the Teacher: Zero-Shot Quantization Brought Closer to the Teacher](https://arxiv.org/abs/2203.17008)
* [End-to-End Trajectory Distribution Prediction Based on Occupancy Grid Maps](https://arxiv.org/abs/2203.16910)
* [Reflection and Rotation Symmetry Detection via Equivariant Learning](https://arxiv.org/abs/2203.16787)<br>:star:[code](https://github.com/ahyunSeo/EquiSym):house:[project](http://cvlab.postech.ac.kr/research/EquiSym/)
* [Exploiting Explainable Metrics for Augmented SGD](https://arxiv.org/abs/2203.16723)<br>:star:[code](https://github.com/mahdihosseini/RMSGD)
* [On the Importance of Asymmetry for Siamese Representation Learning](https://arxiv.org/abs/2204.00613)<br>:star:[code](https://github.com/facebookresearch/asym-siam)
* [Unimodal-Concentrated Loss: Fully Adaptive Label Distribution Learning for Ordinal Regression](https://arxiv.org/abs/2204.00309)
* [Perception Prioritized Training of Diffusion Models](https://arxiv.org/abs/2204.00227)<br>:star:[code](https://github.com/jychoi118/P2-weighting)
* [LASER: LAtent SpacE Rendering for 2D Visual Localization](https://arxiv.org/abs/2204.00157)<br>:open_mouth:oral
* [Efficient Maximal Coding Rate Reduction by Variational Forms](https://arxiv.org/abs/2204.00077)
* [Exemplar-bsaed Pattern Synthesis with Implicit Periodic Field Network](https://arxiv.org/abs/2204.01671)
* [Progressive Minimal Path Method with Embedded CNN](https://arxiv.org/abs/2204.00944)
* [Online Convolutional Re-parameterization](https://arxiv.org/abs/2204.00826)<br>:star:[code](https://github.com/JUGGHM/OREPA_CVPR2022)
* [Consistency driven Sequential Transformers Attention Model for Partially Observable Scenes](https://arxiv.org/abs/2204.00656)<br>:star:[code](https://github.com/samrudhdhirangrej/STAM-Sequential-Transformers-Attention-Model)
* [Leveraging Equivariant Features for Absolute Pose Regression](https://arxiv.org/abs/2204.02163)
* [Neural Convolutional Surfaces](https://arxiv.org/abs/2204.02289)<br>:house:[project](http://geometry.cs.ucl.ac.uk/projects/2022/cnnmaps/)
* [GLASS: Geometric Latent Augmentation for Shape Spaces](https://arxiv.org/pdf/2108.03225.pdf)<br>:star:[code](https://github.com/sanjeevmk/glass/):house:[project](https://sanjeevmk.github.io/glass_webpage/)
* [Total Variation Optimization Layers for Computer Vision](https://arxiv.org/abs/2204.03643)
* [Identifying Ambiguous Similarity Conditions via Semantic Matching](https://arxiv.org/abs/2204.04053)<br>:star:[code](https://github.com/shiy19/DiscoverNet):newspaper:[解读](https://zhuanlan.zhihu.com/p/496386916)
* [TemporalUV: Capturing Loose Clothing with Temporally Coherent UV Coordinates](https://arxiv.org/abs/2204.03671)
* [Gravitationally Lensed Black Hole Emission Tomography](https://arxiv.org/abs/2204.03715)<br>:star:[code](https://github.com/aviadlevis/bhnerf):house:[project](http://imaging.cms.caltech.edu/bhnerf/):tv:[video](https://youtu.be/eFPmShxhtg0)
* [Robust and Accurate Superquadric Recovery: a Probabilistic Approach](https://arxiv.org/abs/2111.14517)<br>:star:[code](https://github.com/bmlklwx/EMS-superquadric_fitting)
* [Projective Manifold Gradient Layer for Deep Rotation Regression](https://arxiv.org/abs/2110.11657)<br>:star:[code](https://github.com/JYChen18/RPMG)
* [Habitat-Web: Learning Embodied Object-Search Strategies from Human Demonstrations at Scale](https://arxiv.org/abs/2204.03514)<br>:star:[code](https://github.com/Ram81/habitat-web)
* [Single-Photon Structured Light](https://arxiv.org/abs/2204.05300)
* [Explaining Deep Convolutional Neural Networks via Latent Visual-Semantic Filter Attention](https://arxiv.org/abs/2204.04601)<br>:open_mouth:oral:star:[code](https://github.com/YuYang0901/LaViSE)
* [Defensive Patches for Robust Recognition in the Physical World](https://arxiv.org/abs/2204.06213)<br>:star:[code](https://github.com/nlsde-safety-team/DefensivePatch):newspaper:[解读](https://zhuanlan.zhihu.com/p/498244289)
* [Event-aided Direct Sparse Odometry](https://arxiv.org/abs/2204.07640)<br>:open_mouth:oral:star:[code](https://github.com/uzh-rpg/eds-buildconf):house:[project](https://rpg.ifi.uzh.ch/eds.html):tv:[video](https://www.youtube.com/watch?v=ymnAWgfYj6Y&feature=youtu.be)
* [Deep Unlearning via Randomized Conditionally Independent Hessians](https://arxiv.org/abs/2204.07655)<br>:star:[code](https://github.com/vsingh-group/LCODEC-deep-unlearning)
* [Learning to Imagine: Diversify Memory for Incremental Learning using Unlabeled Data](https://arxiv.org/abs/2204.08932)
* [Towards Data-Free Model Stealing in a Hard Label Setting](https://arxiv.org/abs/2204.11022)<br>:star:[code](https://github.com/val-iisc/Hard-Label-Model-Stealing):house:[project](https://sites.google.com/view/dfms-hl)
* [Proto2Proto: Can you recognize the car, the way I do?](https://arxiv.org/abs/2204.11830)<br>:star:[code](https://github.com/archmaester/proto2proto)
 * [Balanced MSE for Imbalanced Visual Regression](https://arxiv.org/abs/2203.16427)<br>:open_mouth:oral:star:[code](https://github.com/jiawei-ren/BalancedMSE)<br>:newspaper:[CVPR 2022 (Oral) | 回归标签不平衡? 试试Balanced MSE](https://mp.weixin.qq.com/s/pNnAUJCpJMDocPm_ZUFJWg)
* [Leveraging Unlabeled Data for Sketch-based Understanding](https://arxiv.org/abs/2204.12522)
* [Fixing Malfunctional Objects With Learned Physical Simulation and Functional Prediction](https://arxiv.org/abs/2205.02834)<br>:star:[code](https://github.com/evelinehong/FixIt):house:[project](http://fixing-malfunctional.csail.mit.edu/)
* [Scaling Up Your Kernels to 31x31: Revisiting Large Kernel Design in CNNs](https://arxiv.org/abs/2203.06717)<br>:star:[code](https://github.com/megvii-research/RepLKNet):newspaper:[解读](https://mp.weixin.qq.com/s/kZue3ds348UXQI86xrwudQ)
* [RepMLPNet: Hierarchical Vision MLP with Re-parameterized Locality](https://arxiv.org/abs/2112.11081)<br>:star:[code](https://github.com/DingXiaoH/RepMLP):newspaper:[解读](https://mp.weixin.qq.com/s/kZue3ds348UXQI86xrwudQ)
* [An Image Patch is a Wave: Quantum Inspired Vision MLP](https://arxiv.org/abs/2111.12294)<br>:open_mouth:oral:star:[code](https://github.com/huawei-noah/CV-Backbones/tree/master/wavemlp_pytorch)
* [A ConvNet for the 2020s](https://arxiv.org/abs/2201.03545)<br>:star:[code](https://github.com/facebookresearch/ConvNeXt)
* [NeuralHDHair: Automatic High-fidelity Hair Modeling from a Single Image Using Implicit Neural Representations](https://arxiv.org/abs/2205.04175)<br>头发建模：仅用一幅图像，构建高保真度的头发模型，使用隐式神经表示的方法。出自浙大CAD&CG组、ETH Zurich、香港城市大学。
* [A Unified Framework for Implicit Sinkhorn Differentiation](https://arxiv.org/abs/2205.06688)<br>:star:[code](https://github.com/marvin-eisenberger/implicit-sinkhorn)<br>:newspaper:[解读](https://zhuanlan.zhihu.com/p/515190727)
* [Towards Better Understanding Attribution Methods](https://arxiv.org/abs/2205.10435)<br>:star:[code](https://github.com/sukrutrao/Attribution-Evaluation)
* [Universal Photometric Stereo Network using Global Lighting Contexts](https://arxiv.org/abs/2206.02452)<br>:star:[code](https://github.com/satoshi-ikehata/Universal-PS-CVPR2022):house:[project](https://satoshi-ikehata.github.io/cvpr2022/univps_cvpr2022.html):tv:[video](https://www.youtube.com/watch?v=XSUgqgTSlZM):newspaper:[解读](https://zhuanlan.zhihu.com/p/525331776)
* [Estimating Example Difficulty Using Variance of Gradients](https://arxiv.org/abs/2008.11600)
* [One Loss for Quantization: Deep Hashing with Discrete Wasserstein Distributional Matching](https://arxiv.org/abs/2205.15721)
* [Holocurtains: Programming Light Curtains via Binary Holography](https://openaccess.thecvf.com/content/CVPR2022/papers/Chan_Holocurtains_Programming_Light_Curtains_via_Binary_Holography_CVPR_2022_paper.pdf)
* [Do Learned Representations Respect Causal Relationships?](https://arxiv.org/abs/2204.00762)
* [CAPRI-Net: Learning Compact CAD Shapes With Adaptive Primitive Assembly](https://openaccess.thecvf.com/content/CVPR2022/papers/Yu_CAPRI-Net_Learning_Compact_CAD_Shapes_With_Adaptive_Primitive_Assembly_CVPR_2022_paper.pdf)
* [Mixed Differential Privacy in Computer Vision](https://arxiv.org/abs/2203.11481)
* [Which Model To Transfer? Finding the Needle in the Growing Haystack](https://arxiv.org/abs/2010.06402)
* [Learning Soft Estimator of Keypoint Scale and Orientation With Probabilistic Covariant Loss](https://openaccess.thecvf.com/content/CVPR2022/papers/Yan_Learning_Soft_Estimator_of_Keypoint_Scale_and_Orientation_With_Probabilistic_CVPR_2022_paper.pdf)<br>:star:[code](https://github.com/elvintanhust/S3Esti)
* [RAGO: Recurrent Graph Optimizer For Multiple Rotation Averaging](https://openaccess.thecvf.com/content/CVPR2022/papers/Li_RAGO_Recurrent_Graph_Optimizer_for_Multiple_Rotation_Averaging_CVPR_2022_paper.pdf)<br>:star:[code](https://github.com/sfu-gruvi-3dv/RAGO)
* [Virtual Elastic Objects](https://arxiv.org/abs/2201.04623)<br>:house:[project](https://hsiaoyu.github.io/VEO/)
* [Bayesian Invariant Risk Minimization](https://openaccess.thecvf.com/content/CVPR2022/papers/Lin_Bayesian_Invariant_Risk_Minimization_CVPR_2022_paper.pdf)<br>:star:[code](https://github.com/linyongver/Bayesian-Invariant-Risk-Minmization)
* [Shape From Polarization for Complex Scenes in the Wild](https://arxiv.org/abs/2112.11377)<br>:star:[code](https://github.com/ChenyangLEI/sfp-wild)
* [Non-Iterative Recovery from Nonlinear Observations using Generative Models](https://arxiv.org/abs/2205.15749)
* [Moving Window Regression: A Novel Approach to Ordinal Regression](https://arxiv.org/abs/2203.13122)<br>:star:[code](https://github.com/nhshin-mcl/MWR)
* [Generative Flows With Invertible Attentions](https://arxiv.org/abs/2106.03959)
* [Clipped Hyperbolic Classifiers Are Super-Hyperbolic Classifiers](https://arxiv.org/abs/2107.11472)
* [The Flag Median and FlagIRLS](https://arxiv.org/abs/2203.04437)
* [Implicit Feature Decoupling With Depthwise Quantization](https://arxiv.org/abs/2203.08080)<br>:star:[code](https://github.com/fostiropoulos/Depthwise-Quantization)
* [UNIST: Unpaired Neural Implicit Shape Translation Network](https://arxiv.org/abs/2112.05381)<br>:star:[code](https://github.com/qiminchen/UNIST):house:[project](https://qiminchen.github.io/unist/)
* [Mutual Information-Driven Pan-Sharpening](https://openaccess.thecvf.com/content/CVPR2022/papers/Zhou_Mutual_Information-Driven_Pan-Sharpening_CVPR_2022_paper.pdf)
* [A Framework for Learning Ante-Hoc Explainable Models via Concepts](https://arxiv.org/abs/2108.11761)
* [SeeThroughNet: Resurrection of Auxiliary Loss by Preserving Class Probability Information](https://openaccess.thecvf.com/content/CVPR2022/papers/Han_SeeThroughNet_Resurrection_of_Auxiliary_Loss_by_Preserving_Class_Probability_Information_CVPR_2022_paper.pdf)
* [Learning ABCs: Approximate Bijective Correspondence for Isolating Factors of Variation With Weak Supervision](https://arxiv.org/abs/2103.03240)<br>:star:[code](https://github.com/google-research/google-research/tree/master/isolating_factors)
* [Convolutions for Spatial Interaction Modeling](https://arxiv.org/abs/2104.07182)
* [FastDOG: Fast Discrete Optimization on GPU](https://arxiv.org/abs/2111.10270)<br>:star:[code](https://github.com/LPMP/BDD)
* [Convolution of Convolution: Let Kernels Spatially Collaborate](https://openaccess.thecvf.com/content/CVPR2022/papers/Zhao_Convolution_of_Convolution_Let_Kernels_Spatially_Collaborate_CVPR_2022_paper.pdf)<br>:star:[code](https://github.com/Genera1Z/ConvolutionOfConvolution)
* [Generalized Category Discovery](https://arxiv.org/abs/2201.02609)<br>:star:[code](https://github.com/sgvaze/generalized-category-discovery):house:[project](https://www.robots.ox.ac.uk/~vgg/research/gcd/)
* [Maximum Consensus by Weighted Influences of Monotone Boolean Functions](https://arxiv.org/abs/2112.00953)
* [Divide and Conquer: Compositional Experts for Generalized Novel Class Discovery](https://openaccess.thecvf.com/content/CVPR2022/papers/Yang_Divide_and_Conquer_Compositional_Experts_for_Generalized_Novel_Class_Discovery_CVPR_2022_paper.pdf)<br>:star:[code](https://github.com/muliyangm/ComEx)
* [Fast Algorithm for Low-Rank Tensor Completion in Delay-Embedded Space](https://openaccess.thecvf.com/content/CVPR2022/papers/Yamamoto_Fast_Algorithm_for_Low-Rank_Tensor_Completion_in_Delay-Embedded_Space_CVPR_2022_paper.pdf)
* [Less Is More: Generating Grounded Navigation Instructions From Landmarks](https://arxiv.org/abs/2111.12872)
* [HEAT: Holistic Edge Attention Transformer for Structured Reconstruction](https://arxiv.org/abs/2111.15143)<br>:star:[code](https://github.com/woodfrog/heat):house:[project](https://heat-structured-reconstruction.github.io/)
* [Instance-Dependent Label-Noise Learning With Manifold-Regularized Transition Matrix Estimation](https://openaccess.thecvf.com/content/CVPR2022/papers/Cheng_Instance-Dependent_Label-Noise_Learning_With_Manifold-Regularized_Transition_Matrix_Estimation_CVPR_2022_paper.pdf)
* [Node Representation Learning in Graph via Node-to-Neighbourhood Mutual Information Maximization](https://arxiv.org/abs/2203.12265)<br>:star:[code](https://github.com/dongwei156/n2n)
* [How Well Do Sparse ImageNet Models Transfer?](https://arxiv.org/abs/2111.13445)<br>:star:[code](https://github.com/IST-DASLab/sparse-imagenet-transfer)
* [REX: Reasoning-Aware and Grounded Explanation](https://arxiv.org/abs/2203.06107)<br>:star:[code](https://github.com/szzexpoi/rex)
* [Coherent Point Drift Revisited for Non-Rigid Shape Matching and Registration](https://openaccess.thecvf.com/content/CVPR2022/papers/Fan_Coherent_Point_Drift_Revisited_for_Non-Rigid_Shape_Matching_and_Registration_CVPR_2022_paper.pdf)
* [Hire-MLP: Vision MLP via Hierarchical Rearrangement](https://openaccess.thecvf.com/content/CVPR2022/papers/Guo_Hire-MLP_Vision_MLP_via_Hierarchical_Rearrangement_CVPR_2022_paper.pdf)<br>:star:[code](https://gitee.com/mindspore/models/tree/master/research/cv/HireMLP)
* [One-Bit Active Query With Contrastive Pairs](https://openaccess.thecvf.com/content/CVPR2022/papers/Zhang_One-Bit_Active_Query_With_Contrastive_Pairs_CVPR_2022_paper.pdf)
* [Sparse Non-Local CRF](https://openaccess.thecvf.com/content/CVPR2022/papers/Veksler_Sparse_Non-Local_CRF_CVPR_2022_paper.pdf)
* [Dataset Distillation by Matching Training Trajectories](https://arxiv.org/abs/2203.11932)<br>:star:[code](https://github.com/GeorgeCazenavette/mtt-distillation):house:[project](https://georgecazenavette.github.io/mtt-distillation/)
* [Deep Decomposition for Stochastic Normal-Abnormal Transport](https://arxiv.org/abs/2111.14777)<br>:open_mouth:oral
* [Parametric Scattering Networks](https://openaccess.thecvf.com/content/CVPR2022/papers/Gauthier_Parametric_Scattering_Networks_CVPR_2022_paper.pdf)<br>:star:[code](https://github.com/bentherien/parametricScatteringNetworks)
* [ScaleNet: A Shallow Architecture for Scale Estimation](https://openaccess.thecvf.com/content/CVPR2022/papers/Barroso-Laguna_ScaleNet_A_Shallow_Architecture_for_Scale_Estimation_CVPR_2022_paper.pdf)<br>:star:[code](https://github.com/axelBarroso/ScaleNet)
* [Learning To Solve Hard Minimal Problems](https://arxiv.org/abs/2112.03424)
* [Learning Canonical F-Correlation Projection for Compact Multiview Representation](https://openaccess.thecvf.com/content/CVPR2022/papers/Yuan_Learning_Canonical_F-Correlation_Projection_for_Compact_Multiview_Representation_CVPR_2022_paper.pdf)
* [CellTypeGraph: A New Geometric Computer Vision Benchmark](https://arxiv.org/abs/2205.08166)<br>:star:[code](https://github.com/hci-unihd/celltype-graph-benchmark)
* [RIDDLE: Lidar Data Compression With Range Image Deep Delta Encoding](https://openaccess.thecvf.com/content/CVPR2022/papers/Zhou_RIDDLE_Lidar_Data_Compression_With_Range_Image_Deep_Delta_Encoding_CVPR_2022_paper.pdf)
* [HODEC: Towards Efficient High-Order DEcomposed Convolutional Neural Networks](https://openaccess.thecvf.com/content/CVPR2022/papers/Yin_HODEC_Towards_Efficient_High-Order_DEcomposed_Convolutional_Neural_Networks_CVPR_2022_paper.pdf)
* [Smooth Maximum Unit: Smooth Activation Function for Deep Networks Using Smoothing Maximum Technique](https://openaccess.thecvf.com/content/CVPR2022/papers/Biswas_Smooth_Maximum_Unit_Smooth_Activation_Function_for_Deep_Networks_Using_CVPR_2022_paper.pdf)
* [Learning Invisible Markers for Hidden Codes in Offline-to-Online Photography](https://openaccess.thecvf.com/content/CVPR2022/papers/Jia_Learning_Invisible_Markers_for_Hidden_Codes_in_Offline-to-Online_Photography_CVPR_2022_paper.pdf)
* [Task2Sim: Towards Effective Pre-Training and Transfer From Synthetic Data](https://arxiv.org/abs/2112.00054)<br>:star:[code](https://github.com/samarth4149/task2sim):house:[project](https://samarth4149.github.io/projects/task2sim.html)
* [Neural Prior for Trajectory Estimation](https://openaccess.thecvf.com/content/CVPR2022/papers/Wang_Neural_Prior_for_Trajectory_Estimation_CVPR_2022_paper.pdf)
* [ActiveZero: Mixed Domain Learning for Active Stereovision with Zero Annotation](https://arxiv.org/abs/2112.02772)<br>:star:[code](https://github.com/haosulab/active_zero)
* [Global-Aware Registration of Less-Overlap RGB-D Scans](https://openaccess.thecvf.com/content/CVPR2022/papers/Sun_Global-Aware_Registration_of_Less-Overlap_RGB-D_Scans_CVPR_2022_paper.pdf)
* [Efficient Deep Embedded Subspace Clustering](https://openaccess.thecvf.com/content/CVPR2022/papers/Cai_Efficient_Deep_Embedded_Subspace_Clustering_CVPR_2022_paper.pdf)
* [Rep-Net: Efficient On-Device Learning via Feature Reprogramming](https://openaccess.thecvf.com/content/CVPR2022/papers/Yang_Rep-Net_Efficient_On-Device_Learning_via_Feature_Reprogramming_CVPR_2022_paper.pdf)<br>:star:[code](https://github.com/ASU-ESIC-FAN-Lab/RepNet)
* [WALT: Watch and Learn 2D Amodal Representation From Time-Lapse Imagery](https://openaccess.thecvf.com/content/CVPR2022/papers/Reddy_WALT_Watch_and_Learn_2D_Amodal_Representation_From_Time-Lapse_Imagery_CVPR_2022_paper.pdf)
* [FLAVA: A Foundational Language and Vision Alignment Model](https://arxiv.org/abs/2112.04482)<br>:star:[code](https://github.com/facebookresearch/multimodal/tree/main/examples/flava):house:[project](https://flava-model.github.io/)
* [Scanline Homographies for Rolling-Shutter Plane Absolute Pose](https://openaccess.thecvf.com/content/CVPR2022/papers/Bai_Scanline_Homographies_for_Rolling-Shutter_Plane_Absolute_Pose_CVPR_2022_paper.pdf)<br>:star:[code](https://bitbucket.org/clermontferrand/planarscanlinehomography/src/master/)
* [Exemplar-based Pattern Synthesis with Implicit Periodic Field Network](https://arxiv.org/abs/2204.01671)
* [Understanding Uncertainty Maps in Vision With Statistical Testing](https://openaccess.thecvf.com/content/CVPR2022/papers/Nazarovs_Understanding_Uncertainty_Maps_in_Vision_With_Statistical_Testing_CVPR_2022_paper.pdf)<br>:star:[code](https://github.com/vsingh-group/uncertainty_with_rf)
* [B-Cos Networks: Alignment Is All We Need for Interpretability](https://openaccess.thecvf.com/content/CVPR2022/papers/Bohle_B-Cos_Networks_Alignment_Is_All_We_Need_for_Interpretability_CVPR_2022_paper.pdf)
* [Learning to Collaborate in Decentralized Learning of Personalized Models](https://openaccess.thecvf.com/content/CVPR2022/papers/Li_Learning_To_Collaborate_in_Decentralized_Learning_of_Personalized_Models_CVPR_2022_paper.pdf)<br>:newspaper:[解读](https://mp.weixin.qq.com/s/lSY1is6Fmm6A0Db0Jxo4qg)
* [360-Attack: Distortion-Aware Perturbations From Perspective-Views](https://openaccess.thecvf.com/content/CVPR2022/papers/Zhang_360-Attack_Distortion-Aware_Perturbations_From_Perspective-Views_CVPR_2022_paper.pdf)
* [A Unified Model for Line Projections in Catadioptric Cameras With Rotationally Symmetric Mirrors](https://openaccess.thecvf.com/content/CVPR2022/papers/Miraldo_A_Unified_Model_for_Line_Projections_in_Catadioptric_Cameras_With_CVPR_2022_paper.pdf)
* [A Hybrid Quantum-Classical Algorithm for Robust Fitting](https://arxiv.org/abs/2201.10110)<br>:star:[code](https://openaccess.thecvf.com/CVPR2022?day=all#)
* [Topology Preserving Local Road Network Estimation From Single Onboard Camera Image](https://arxiv.org/abs/2112.10155)<br>:star:[code](https://github.com/ybarancan/TopologicalLaneGraph)
* [RendNet: Unified 2D/3D Recognizer With Latent Space Rendering](https://openaccess.thecvf.com/content/CVPR2022/papers/Shi_RendNet_Unified_2D3D_Recognizer_With_Latent_Space_Rendering_CVPR_2022_paper.pdf)
* [Towards Real-World Navigation With Deep Differentiable Planners](https://arxiv.org/abs/2108.05713)<br>:star:[code](https://github.com/shuishida/calvin)
* [An Iterative Quantum Approach for Transformation Estimation From Point Sets](https://openaccess.thecvf.com/content/CVPR2022/papers/Meli_An_Iterative_Quantum_Approach_for_Transformation_Estimation_From_Point_Sets_CVPR_2022_paper.pdf)
* [UnweaveNet: Unweaving Activity Stories](https://arxiv.org/abs/2112.10194)<br>:star:[code](https://github.com/willprice/activity-stories)
* [Faithful Extreme Rescaling via Generative Prior Reciprocated Invertible Representations](https://openaccess.thecvf.com/content/CVPR2022/papers/Zhong_Faithful_Extreme_Rescaling_via_Generative_Prior_Reciprocated_Invertible_Representations_CVPR_2022_paper.pdf)<br>:star:[code](https://github.com/cszzx/GRAIN)
* [Learning Video Representations of Human Motion From Synthetic Data](https://openaccess.thecvf.com/content/CVPR2022/papers/Guo_Learning_Video_Representations_of_Human_Motion_From_Synthetic_Data_CVPR_2022_paper.pdf)
* [TVConv: Efficient Translation Variant Convolution for Layout-Aware Visual Processing](https://arxiv.org/abs/2203.10489)<br>:star:[code](https://github.com/JierunChen/TVConv)
* [The Probabilistic Normal Epipolar Constraint for Frame-to-Frame Rotation Optimization Under Uncertain Feature Positions](https://arxiv.org/abs/2204.02256)<br>:star:[code](https://github.com/tum-vision/pnec):house:[project](https://vision.in.tum.de/research/vslam/pnec)
* [Simple but Effective: CLIP Embeddings for Embodied AI](https://arxiv.org/abs/2111.09888)<br>:star:[code](https://github.com/allenai/embodied-clip)
* [Interactive Disentanglement: Learning Concepts by Interacting with their Prototype Representations](https://arxiv.org/abs/2112.02290)<br>:star:[code](https://github.com/ml-research/XIConceptLearning)
* [Recall@k Surrogate Loss With Large Batches and Similarity Mixup](https://openaccess.thecvf.com/content/CVPR2022/papers/Patel_Recallk_Surrogate_Loss_With_Large_Batches_and_Similarity_Mixup_CVPR_2022_paper.pdf)<br>:star:[code](https://github.com/yash0307/RecallatK_surrogate)
* [Bending Graphs: Hierarchical Shape Matching Using Gated Optimal Transport](https://arxiv.org/abs/2202.01537)<br>:star:[code](https://github.com/mahdi-slh/BendingGraphs)
* [Nested Hyperbolic Spaces for Dimensionality Reduction and Hyperbolic NN Design](https://arxiv.org/abs/2112.03402)
* [HeadNeRF: A Real-Time NeRF-Based Parametric Head Model](https://arxiv.org/abs/2112.05637)<br>:star:[code](https://github.com/CrisHY1995/headnerf)
* [Replacing Labeled Real-Image Datasets With Auto-Generated Contours](https://openaccess.thecvf.com/content/CVPR2022/papers/Kataoka_Replacing_Labeled_Real-Image_Datasets_With_Auto-Generated_Contours_CVPR_2022_paper.pdf)<br>:star:[code](https://github.com/masora1030/CVPR2022-Pretrained-ViT-PyTorch):house:[project](https://hirokatsukataoka16.github.io/Replacing-Labeled-Real-Image-Datasets/)
* [Pushing the Envelope of Gradient Boosting Forests via Globally-Optimized Oblique Trees](https://openaccess.thecvf.com/content/CVPR2022/papers/Gabidolla_Pushing_the_Envelope_of_Gradient_Boosting_Forests_via_Globally-Optimized_Oblique_CVPR_2022_paper.pdf)
* [Omnivore: A Single Model for Many Visual Modalities](https://arxiv.org/abs/2201.08377)<br>:house:[project](https://facebookresearch.github.io/omnivore/)
* [Leveling Down in Computer Vision: Pareto Inefficiencies in Fair Deep Classifiers](https://arxiv.org/abs/2203.04913)
* [Open-Domain, Content-Based, Multi-Modal Fact-Checking of Out-of-Context Images via Online Resources](https://arxiv.org/abs/2112.00061)<br>:star:[code](https://github.com/S-Abdelnabi/OoC-multi-modal-fc):house:[project](https://s-abdelnabi.github.io/OoC-multi-modal-fc/)
* [Memory-Augmented Deep Conditional Unfolding Network for Pan-Sharpening](https://openaccess.thecvf.com/content/CVPR2022/papers/Yang_Memory-Augmented_Deep_Conditional_Unfolding_Network_for_Pan-Sharpening_CVPR_2022_paper.pdf)<br>:star:[code](https://github.com/yggame/MDCUN)
* [HVH: Learning a Hybrid Neural Volumetric Representation for Dynamic Hair Performance Capture](https://arxiv.org/abs/2112.06904)<br>:house:[project](https://ziyanw1.github.io/hvh/)
* [Deep Image-based Illumination Harmonization](https://arxiv.org/abs/2108.00150)<br>:star:[code](https://github.com/zhongyunbao/Dataset)
* [Ditto: Building Digital Twins of Articulated Objects From Interaction](https://arxiv.org/abs/2202.08227)<br>:open_mouth:oral:star:[code](https://github.com/UT-Austin-RPL/Ditto):house:[project](https://ut-austin-rpl.github.io/Ditto/)
* [TO-FLOW: Efficient Continuous Normalizing Flows With Temporal Optimization Adjoint With Moving Speed](https://openaccess.thecvf.com/content/CVPR2022/papers/Du_TO-FLOW_Efficient_Continuous_Normalizing_Flows_With_Temporal_Optimization_Adjoint_With_CVPR_2022_paper.pdf)
* [Masked Autoencoders Are Scalable Vision Learners](https://arxiv.org/abs/2111.06377)
* [Neural Inertial Localization](https://arxiv.org/abs/2203.15851)<br>:star:[code](https://github.com/Sachini/niloc):house:[project](https://sachini.github.io/niloc)
* [Neural Recognition of Dashed Curves With Gestalt Law of Continuity](https://openaccess.thecvf.com/content/CVPR2022/papers/Liu_Neural_Recognition_of_Dashed_Curves_With_Gestalt_Law_of_Continuity_CVPR_2022_paper.pdf)
* [BACON: Band-Limited Coordinate Networks for Multiscale Scene Representation](https://arxiv.org/abs/2112.04645)<br>:house:[project](https://www.computationalimaging.org/publications/bacon/)
* [Merry Go Round: Rotate a Frame and Fool a DNN](https://openaccess.thecvf.com/content/CVPR2022/papers/Thapar_Merry_Go_Round_Rotate_a_Frame_and_Fool_a_DNN_CVPR_2022_paper.pdf)
* [Modeling sRGB Camera Noise With Normalizing Flows](https://arxiv.org/abs/2206.00812)<br>:house:[project](https://yorkucvil.github.io/sRGBNoise/)
* [Co-Advise: Cross Inductive Bias Distillation](https://openaccess.thecvf.com/content/CVPR2022/papers/Ren_Co-Advise_Cross_Inductive_Bias_Distillation_CVPR_2022_paper.pdf)<br>:star:[code](https://github.com/OliverRensu/co-advise)
* [Automatic Relation-Aware Graph Network Proliferation](https://arxiv.org/abs/2205.15678)<br>:star:[code](https://github.com/phython96/ARGNP)
* [Stereo Magnification With Multi-Layer Images](https://arxiv.org/abs/2201.05023)<br>:star:[code](https://github.com/SamsungLabs/StereoLayers):house:[project](https://samsunglabs.github.io/StereoLayers/)
* [CO-SNE: Dimensionality Reduction and Visualization for Hyperbolic Data](https://openaccess.thecvf.com/content/CVPR2022/papers/Guo_CO-SNE_Dimensionality_Reduction_and_Visualization_for_Hyperbolic_Data_CVPR_2022_paper.pdf)
* [Rethinking Controllable Variational Autoencoders](https://openaccess.thecvf.com/content/CVPR2022/papers/Shao_Rethinking_Controllable_Variational_Autoencoders_CVPR_2022_paper.pdf)
* [BigDL 2.0: Seamless Scaling of AI Pipelines From Laptops to Distributed Cluster](https://openaccess.thecvf.com/content/CVPR2022/papers/Dai_BigDL_2.0_Seamless_Scaling_of_AI_Pipelines_From_Laptops_to_CVPR_2022_paper.pdf)
* [HARA: A Hierarchical Approach for Robust Rotation Averaging](https://arxiv.org/abs/2111.08831)<br>:star:[code](https://github.com/sunghoon031/HARA)
* [Diffusion Autoencoders: Toward a Meaningful and Decodable Representation](https://arxiv.org/abs/2111.15640)<br>:open_mouth:oral:star:[code](https://github.com/phizaz/diffae):house:[project](https://diff-ae.github.io/)
* [Learning Fair Classifiers with Partially Annotated Group Labels](https://arxiv.org/abs/2111.14581)<br>:star:[code](https://github.com/naver-ai/cgl_fairness)
* [Come-Closer-Diffuse-Faster: Accelerating Conditional Diffusion Models for Inverse Problems Through Stochastic Contraction](https://openaccess.thecvf.com/content/CVPR2022/papers/Chung_Come-Closer-Diffuse-Faster_Accelerating_Conditional_Diffusion_Models_for_Inverse_Problems_Through_Stochastic_CVPR_2022_paper.pdf)
* [High-Fidelity Human Avatars From a Single RGB Camera](https://openaccess.thecvf.com/content/CVPR2022/papers/Zhao_High-Fidelity_Human_Avatars_From_a_Single_RGB_Camera_CVPR_2022_paper.pdf)<br>:star:[code](https://github.com/hzhao1997/HF-Avatar):house:[project](http://cic.tju.edu.cn/faculty/likun/projects/HF-Avatar/)
* [RIO: Rotation-Equivariance Supervised Learning of Robust Inertial Odometry](https://arxiv.org/abs/2111.11676)
* [How Good Is Aesthetic Ability of a Fashion Model?](https://openaccess.thecvf.com/content/CVPR2022/papers/Zou_How_Good_Is_Aesthetic_Ability_of_a_Fashion_Model_CVPR_2022_paper.pdf)<br>:star:[code](https://github.com/AemikaChow/AiDLab-fAshIon-Data)
* [Learning With Neighbor Consistency for Noisy Labels](https://arxiv.org/abs/2202.02200)
* [GeoEngine: A Platform for Production-Ready Geospatial Research](https://openaccess.thecvf.com/content/CVPR2022/papers/Verma_GeoEngine_A_Platform_for_Production-Ready_Geospatial_Research_CVPR_2022_paper.pdf)
* [Using 3D Topological Connectivity for Ghost Particle Reduction in Flow Reconstruction](https://openaccess.thecvf.com/content/CVPR2022/papers/Tsalicoglou_Using_3D_Topological_Connectivity_for_Ghost_Particle_Reduction_in_Flow_CVPR_2022_paper.pdf)
* [On the Integration of Self-Attention and Convolution](https://arxiv.org/abs/2111.14556)<br>:star:[code](https://github.com/LeapLabTHU/ACmix)
* [Towards Better Plasticity-Stability Trade-Off in Incremental Learning: A Simple Linear Connector](https://arxiv.org/abs/2110.07905)<br>:star:[code](https://github.com/lingl1024/Connector)
* [MAXIM: Multi-Axis MLP for Image Processing](https://arxiv.org/abs/2201.02973)<br>:open_mouth:oral:star:[code](https://github.com/google-research/maxim)
* [Delving Into the Estimation Shift of Batch Normalization in a Network](https://arxiv.org/abs/2203.10778)<br>:star:[code](https://github.com/huangleiBuaa/XBNBlock)
* [Learning Object Context for Novel-View Scene Layout Generation](https://openaccess.thecvf.com/content/CVPR2022/papers/Qiao_Learning_Object_Context_for_Novel-View_Scene_Layout_Generation_CVPR_2022_paper.pdf)
* [Dist-PU: Positive-Unlabeled Learning From a Label Distribution Perspective](https://openaccess.thecvf.com/content/CVPR2022/papers/Zhao_Dist-PU_Positive-Unlabeled_Learning_From_a_Label_Distribution_Perspective_CVPR_2022_paper.pdf)
* [Relative Pose From a Calibrated and an Uncalibrated Smartphone Image](https://openaccess.thecvf.com/content/CVPR2022/papers/Ding_Relative_Pose_From_a_Calibrated_and_an_Uncalibrated_Smartphone_Image_CVPR_2022_paper.pdf)
* [The Devil Is in the Margin: Margin-Based Label Smoothing for Network Calibration](https://arxiv.org/abs/2111.15430)<br>:star:[code](https://github.com/by-liu/MbLS)

### 扫码CV君微信（注明：CVPR）入微信交流群：
![9475fa20fd5e95235d9fa23ae9587a2](https://user-images.githubusercontent.com/62801906/156720309-de92964f-a6da-464a-b21f-cfb270c13e27.png)

